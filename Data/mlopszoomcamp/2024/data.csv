project_url,project_title,Deployment Type,Reason,Cloud
https://github.com/Ju-an/U073U5KPQDT-MLOps-WeldSpot,Welding Defect Detection System,"Batch, Web Service",The project uses Prefect 2 for workflow orchestration (batch processing) and includes a Flask-based modeling service with Docker containerization for web service deployment. The modeling service exposes APIs and uses TensorBoard for monitoring.,Other
https://github.com/victornemenike/fuel-price-prediction-mlops,LSTM Fuel Price Model,Web Service,"The project serves ML predictions via a Flask API (predict.py) and includes a Dockerfile exposing port 9696 for the web service. The main entrypoint is ""gunicorn --bind=0.0.0.0:9696 predict:app"" which is a web service deployment pattern.",Unknown
https://github.com/divakaivan/insurance-fraud-mlops-pipeline/,Fraud Detection MLOps System,"Batch, Web Service","The repository contains Prefect flows for batch processing (data upload, preprocessing, model training, batch predictions) and FastAPI for web service deployment. The batch flows are orchestrated using Prefect Cloud, while FastAPI provides a web service interface.",GCP
https://github.com/Dakini/MLops_project,Real-Time Diabetes Monitoring System,Streaming,The project uses AWS Kinesis streams for real-time data processing and a Lambda function to continuously process streaming data from input to output streams. The infrastructure includes Kinesis modules and Lambda functions designed for streaming inference.,AWS
https://github.com/SergeiOssokine/droughtwatch_capstone,Kenya Drought Detection Pipeline,Batch,"The repository contains a comprehensive batch processing pipeline using Apache Airflow for training orchestration and AWS Step Functions/Lambda for batch inference. Key evidence includes: 1) Airflow Docker setup with DAGs for training workflows, 2) AWS Step Functions/Lambda-based batch inference pipeline, 3) Scheduled data processing jobs, 4) Training infrastructure setup with workflow orchestrators, 5) Batch inference pipeline that processes data uploaded to S3. The project is designed for periodic batch processing rather than real-time streaming or web service serving.",AWS
https://github.com/chrisdamba/talent-flow-predictor,LinkedIn Job Market Analytics,Batch,"The repository contains a Mage AI pipeline (talent_flow_predictor_pipeline.py) that orchestrates ETL jobs including data loading, model training, prediction generation, and saving results. The pipeline is designed to run as scheduled/batch jobs using Mage's workflow orchestration capabilities, not as a web service API or streaming system.",AWS
https://github.com/aturevich/zoomcamp_mlops_project,Earthquake Prediction System,"Batch, Web Service","The project uses Prefect for workflow orchestration (data preparation, model training, monitoring) which is a batch processing pattern. It also serves ML models via FastAPI endpoint and Streamlit dashboard for ML inference, which is a web service pattern.",Other
https://github.com/TheDataDudeDE/hate_speech_detector,Hate Speech Detection System,"Batch, Web Service","The repository contains Mage workflows for batch ETL/ML pipeline orchestration (Dockerfile.mage, Mage configuration) and multiple web service components: FastAPI for model serving (Dockerfile.flask), Gradio for UI (Dockerfile.gradio), and MLflow tracking server. These are orchestrated together via docker-compose.yml.",Azure
https://github.com/TimovNiedek/genre-classifier-msd,Million Song Genre Classifier,Batch,"The repository uses Prefect as a workflow orchestrator to run scheduled ETL jobs for data ingestion, preprocessing, model training, and inference. The flows are defined as Prefect workflows that run periodically rather than as web services or streaming applications.",AWS
https://github.com/darasiemi/mental_health_mlops_project,Real-Time Stress Prediction Pipeline,"Batch, Streaming, Web Service","The repository contains three distinct deployment modes: Batch processing (using Mage orchestrator for scheduled ETL jobs), Web Service (Flask API for model serving), and Streaming (Kinesis-based real-time processing with Lambda functions).",AWS
https://github.com/edumunozsala/online-gaming-mlops-project,AWS Gaming Data Integration,Batch,"The repository uses Mage AI for orchestrating ETL/ELT pipelines with scheduled workflows. The Dockerfile and docker-compose.yml show Mage AI setup for running batch data pipelines, not real-time streaming or web service deployment. The README shows a batch ML pipeline workflow with scheduled training, inference, and model updates.",AWS
https://github.com/lvsuno/Temperature-Prediction.git,Weather Data Orchestration,"Batch, Web Service","The repository contains both a Mage.ai workflow orchestration system (Dockerfile and docker-compose.yml) for batch ETL pipelines, and a separate model_web_service directory with its own Dockerfile, Flask/Gunicorn setup, and API endpoint (predict.py) for serving ML model predictions via HTTP on port 9696.",AWS
https://github.com/aleksandr-dzhumurat/brazilan-ecom-delivery-time-prediction,Brazilian E-commerce Delivery Prediction,"Batch, Web Service","The repository contains both batch prediction components (batch_prediction_backfill.py, scheduled data preparation pipeline) and a real-time FastAPI web service for model deployment. The docker-compose shows batch processing services alongside a FastAPI container for serving predictions.",Other
https://github.com/kiramishima/anemia-detection-mlops,Anemia Detection MLOps Pipeline,"Batch, Web Service","The project uses Mage AI for batch ETL workflows (scheduled data processing) and deploys ML models via AWS Lambda API Gateway for real-time inference, with additional web services for MLflow and Evidently monitoring dashboards.",AWS
https://github.com/AlmudenaZhou/mlops-student-performance,Student Performance Prediction System,"Streaming, Web Service","The repository contains both a Flask web service for model serving (deployment/web_service/predict.py) and a streaming Lambda implementation (deployment/streaming/Dockerfile, deployment/streaming/lambda_function.py) that processes data continuously via API Gateway. The Makefile shows targets for both run_flask and build_lambda_image/run_streaming_lambda.",AWS
https://github.com/beotavalo/loan-elegibility-prediction,Loan Eligibility Prediction System,Web Service,"The repository contains a Flask application deployed via Docker and Terraform to an AWS EC2 instance, exposing HTTP/HTTPS endpoints for serving the application. The Dockerfile uses Gunicorn to run the Flask app, and the Terraform configuration sets up security groups for HTTP/HTTPS traffic and runs the Docker container on port 80.",AWS
https://github.com/yorgos-ai/pet-adoption,Pet Adoption Prediction System,Batch,"The project uses Prefect workflow orchestrator with scheduled flows (model training and batch prediction) that run periodically (weekly). The code shows Prefect flows for training and batch prediction, MLflow for model tracking, and S3 for data storage - all characteristic of batch processing pipelines.",Other
https://github.com/kachiann/project-mlops,MLOps Model Deployment Service,Web Service,The repository contains a Flask-based web service for model deployment (web_service/deploy.py) and Docker configuration for serving ML models via API. The main deployment target is a web service that exposes model predictions through HTTP endpoints.,Other
https://github.com/AnujPanthri/mlops-zoomcamp-final-project,MLflow Model Tracking System,"Batch, Web Service",The project uses Prefect for workflow orchestration to manage training pipelines (batch component) and includes model deployment services via Docker (web service component for serving ML models).,AWS
https://github.com/sanghvirajit/parkinson_disease_prediction_MLOps/tree/main?tab=readme-ov-file,Parkinson's Disease Prediction Pipeline,"Batch, Streaming, Web Service","The repository contains a comprehensive MLOps pipeline with multiple deployment types: 1) Batch processing via Mage (workflow orchestrator) for ETL jobs, 2) Web Service via Flask API for model serving on port 9696, 3) Streaming via Kinesis for real-time data processing with producers/consumers. The docker-compose.yaml shows all three components running together.",AWS
https://github.com/SofyanAkbar94/mlops-project-2024,Gold Price Prediction Pipeline,"Batch, Web Service","The project uses MageAI for batch workflow orchestration (ETL, training, inference) and Dockerized Streamlit app for serving ML model predictions via web interface. Mage handles scheduled/batch jobs while Streamlit serves the model as a web service.",Azure
https://github.com/batxes/Drought-predictor-project,US Drought Prediction Pipeline,"Batch, Web Service",The project deploys a Flask/Gunicorn API for drought prediction (Web Service) and uses Prefect for workflow orchestration of training pipelines (Batch). Docker Compose runs both the prediction service and MLflow tracking server.,Other
https://github.com/voduyquoc/cyberbullying_detection,Cyberbullying Detection Pipeline,"Batch, Streaming","The repository implements a streaming ML pipeline using AWS Kinesis for real-time tweet classification (Lambda function processes streaming data from Kinesis), and also includes batch training components using Prefect for orchestrating ML model training workflows. The README explicitly states ""stream Machine Learning Operations (MLOps) pipeline"" and the Terraform configuration shows Kinesis streams for streaming data, while the training directory uses Prefect for batch model training.",AWS
https://github.com/mpierrau/flight-price-prediction/,Indian Flight Price Prediction,"Batch, Web Service","The repository contains Prefect workflows for batch data preprocessing and hyperparameter optimization (training/batch jobs), and also includes FastAPI for serving ML models as a web service. The infrastructure includes Sagemaker endpoints for model serving.",AWS
https://github.com/melhamamsy/traffic-volume-prediction-ent-to-end,Traffic Volume Prediction Pipeline,"Batch, Web Service","The repository contains both batch and web service components. The orchestration directory uses Mage (a workflow orchestrator) for batch ETL jobs, while the deployment/online-deployment-flask directory contains a Flask-based web service for serving ML model predictions via API.",AWS
https://github.com/ZiedTrikiDataScience/Credit-Card-Approval-MLOps-Project,Unknown,Unknown,No files fetched,Unknown
https://github.com/caiomiyashiro/mlops_zoomcamp_finalproject,Automated MLOps Infrastructure Pipeline,Web Service,"The repository contains a Flask/FastAPI-based prediction service that exposes an HTTP API on port 9696 for making wine quality predictions. The service is containerized and deployed as part of a Docker Compose setup, serving ML models via REST endpoints.",Azure
https://github.com/chrisdamba/million-song-mlops,Million Song Popularity Predictor,Batch,"The repository implements a batch-oriented MLOps pipeline orchestrated by Mage AI, which schedules and runs data preprocessing, model training, and prediction jobs periodically. The architecture diagram and code show scheduled workflows rather than real-time streaming or web service deployment.",AWS
https://github.com/danilson33/mlops-zoomcamp-project,Diamond Attributes ML Model,"Batch, Web Service","The repository contains both batch and web service components. The batch component is evident from the use of Prefect for workflow automation (deploy-prefect target in Makefile, train_flow.py, and Prefect deployment configuration). The web service component is evident from the FastAPI application deployed in a Docker container (app/Dockerfile, FastAPI dependencies, uvicorn server configuration).",AWS
https://github.com/Josesx506/mlops_zoomcamp_24/tree/main/project,NYC Motor Vehicle Collision Prediction,Web Service,"The repository contains a Flask-based backend API (appserver service) that serves ML model predictions via HTTP endpoints. The frontend (webpack service) is a web application that interacts with this API. There are no streaming components (Kafka, Flink, etc.) or batch workflow orchestrators (Airflow, Prefect flows, etc.) - the orchestration is done via GitHub Actions for CI/CD, not for data processing.",AWS
https://github.com/Kolpashnikova/mlops-project,MLflow-Integrated Survey Data Model,"Batch, Web Service",The project uses Prefect for workflow orchestration (batch processing) and Flask/gunicorn for serving the model as a web service. The Dockerfile and docker-compose.yml show both a web service container and MLflow UI container.,Other
https://github.com/Agnes4Him/online-course-completion-prediction.git,E-learning Progress Prediction System,Web Service,"The repository contains a Flask/FastAPI web service for ML model prediction (predict.py) deployed via Docker container on port 9696 using gunicorn. The Dockerfile and docker-compose.yaml show a web service architecture with API endpoints for model inference, connected to a PostgreSQL database for logging predictions. No streaming components (Kafka, Flink) or batch orchestrators (Airflow, dbt) are present.",AWS
https://github.com/Mannerow/doordash-duration-prediction,Doordash Delivery Time Prediction,Batch,"The project uses Prefect for workflow orchestration (evident from run_flow.py and Prefect Cloud login in docker-compose.yaml), with scheduled pipeline execution for model training and prediction. The Docker Compose setup runs a Python flow script that orchestrates ML pipeline steps, which is characteristic of batch processing rather than real-time streaming or web service deployment.",AWS
https://github.com/jelambrar96-datatalks/mlops-zoomcamp-project,MLOps Pipeline for Ride Duration,"Batch, Web Service","The repository contains both batch processing components (Airflow DAGs for ML pipeline orchestration) and web service components (Flask API for serving ML model predictions). The Airflow DAGs handle batch ML training and data processing workflows, while the Flask application serves predictions via HTTP endpoints.",AWS
https://github.com/bradentam/Customer-Churn-Prediction/tree/main,Customer Churn Prediction Pipeline,Batch,"The project uses Airflow with DAGs for workflow orchestration (quarterly_retrain.py, monthly_prediction.py, monitor.py) and Docker containers for deployment. This is a batch processing system where jobs run on schedules rather than real-time streaming.",GCP
https://github.com/maledias/mlops_zoomcamp_project,Unknown,Unknown,No files fetched,Unknown
https://github.com/adelhassen/flight-delays,Flight Delay Prediction Pipeline,"Batch, Web Service","The repository contains a Prefect workflow orchestrator (Batch) and a Flask API for model serving (Web Service). The Makefile shows 'quality_check' and 'test' targets indicating batch processing, while the Dockerfile exposes port 9696 and runs a gunicorn server for the predict.py Flask application.",Other
https://github.com/CALewis1955/doordash_eta_predictor,Restaurant Delivery ML Pipeline,"Batch, Web Service","The repository contains a Mage workflow orchestrator (evidently_report_07-27-2024.py, evidently_report_07-28-2024.py) for batch processing and an MLflow tracking server for experiment management. It also includes a Flask web service (port 9696) for serving ML predictions, as shown in the Makefile's run target and README description.",AWS
https://github.com/IBPhilippov/basketball_results_prediction/,Google Cloud Basketball Model Deployment,"Batch, Web Service","The project uses Mage.AI as a workflow orchestrator to run scheduled ETL jobs (get_data_from_bq, model training, etc.) which is characteristic of batch processing. It also serves ML predictions via a Flask API on port 9696, indicating a web service component for model inference.",GCP
https://github.com/bluemusk24/mlops-final-project/tree/main,Landmine Detection ML Pipeline,Web Service,"The repository contains a Flask/FastAPI-based prediction service (predict.py) with Dockerfile and docker-compose configurations exposing port 9696, indicating a web service deployment pattern for serving ML model predictions via API endpoints.",Other
https://github.com/Kaustbh/Mlops-ZoomCamp-Project1,Crab Age Prediction Pipeline,"Batch, Web Service","The repository contains both a training pipeline (Batch) and a model deployment service (Web Service). The training pipeline uses Prefect for orchestration and runs scheduled experiments with MLflow tracking. The deployment component includes a Flask/Gunicorn web service that serves the trained model via API endpoints (app.py, model_service.py) and is containerized with Docker.",Other
https://github.com/GameRuiner/Housing-Prices-Prediction,Ames Housing Price Prediction,"Batch, Web Service","The project includes an Airflow DAG (train_and_select_best_model_dag.py) that orchestrates batch model training and selection workflows, and a web service component (web service container in docker-compose.yml) that serves predictions via API on port 9696.",GCP
https://github.com/arunv22/zoomcamp_mlops_project/tree/main,UCI Housing ML Pipeline,Web Service,"The repository contains a Flask web service for house price prediction with Dockerfile, requirements.txt, and predict.py. The README explicitly states ""Flask Application"" and shows how to serve ML models via API. No streaming components (Kafka, Flink) or batch orchestrators (Airflow, Prefect) are present.",Other
https://github.com/artemji/mlops-zoomcamp-2024-project,Unknown,Unknown,No files fetched,Unknown
https://github.com/babaksit/mlops-e2e,Unknown,Unknown,No files fetched,Unknown
https://github.com/FrancescaBellucci/mlops-zoomcamp/tree/main/final_project,Bank Customer Churn Prediction Pipeline,Web Service,"The project uses Flask and Docker to serve ML models via API (predict.py with Flask app, Dockerfile exposing port 9696, gunicorn entrypoint). The Makefile shows commands for building and running Docker containers for the prediction service.",AWS
https://github.com/jankes2000/Cybersecurity-Attack-Detection,Cybersecurity Attack Detection Pipeline,Batch,"The repository uses Mage AI pipelines for ETL/orchestration (evidenced by docker-compose.yml running Mage, Dockerfile with MageAI base image, and infrastructure_setup.py for Terraform). The architecture is designed for batch processing of cybersecurity data rather than real-time streaming or web service deployment.",AWS
https://github.com/el-grudge/bank-marketing,Marketing Campaign Performance Tracker,"Batch, Web Service","The repository contains a Mage workflow orchestrator (magic service) for batch ETL jobs and scheduled model training, plus a Flask-based prediction service (predict.py) and Streamlit dashboard for web service deployment. The dockerfile template shows a Flask API serving ML models on port 9696, while Mage handles batch processing workflows.",Other
https://github.com/svetavasileva/dog-breed-classifier,Dog Breed Classification App,Web Service,"The repository contains a Flask web application (app.py) that serves a dog breed classifier model via HTTP endpoints. The application allows users to upload images and receive breed predictions through a web interface. The Dockerfile and deployment setup are configured for serving the model as a web service, not for batch processing or streaming.",AWS
https://github.com/suwarath/StockAnalysis,Technical Indicators Trading System,"Batch, Web Service",The project includes a Prefect workflow for training and deploying models (batch processing) and a Dockerized web service that serves the trained model via API on port 9696. The Makefile shows both 'deploy' (batch) and 'service' (web service) targets.,Other
https://github.com/JorgeAbrego/hotel-booking-mlops-project,Hotel Booking Demand Forecasting,Batch,"The repository contains an Airflow DAG (dags/etl.py) that orchestrates scheduled ETL jobs for data extraction, transformation, and loading to a PostgreSQL database. The infrastructure includes EC2 instances and RDS, with user_data.sh scripts for setup. The project uses workflow orchestrators (Airflow) to run scheduled ETL jobs, which is characteristic of batch processing.",AWS
https://github.com/anicolas91/CRScanada_MLOps,Immigration Ranking Data Pipeline,Web Service,"The project includes a Flask-based web service (predict.py) that serves ML model predictions via API endpoints, containerized with Docker and deployed using gunicorn. The Dockerfile and docker-compose.yml show a web service architecture with API endpoints exposed on ports 5000 and 9696.",AWS
https://github.com/nfescamillas/Cement-Fineness-Prediction,Cement Fineness Monitoring System,Web Service,"The project serves ML models via Flask API (predict.py) and uses Docker containers for deployment. The Dockerfile shows a Flask app exposed on port 9696 using gunicorn, which is a web service deployment pattern. The architecture diagram and README confirm this is deployed via AWS ElasticBeanstalk for serving predictions.",AWS
https://github.com/martin503/ll3m,MLOps Workflow Orchestration System,"Batch, Web Service","The repository contains a Prefect workflow orchestrator for batch ETL jobs (deployment service) and FastAPI/Streamlit services for serving ML models and dashboards (api and app services). The docker-compose shows both batch processing (Prefect flows) and web services (FastAPI API, Streamlit app).",AWS
https://github.com/Fustincho/datatalks-mlops-zoomcamp/tree/main/07-project/airquality,Nearby Sensor Air Quality Model,"Batch, Web Service","The repository contains MAGE pipelines for ETL (openaq_data_etl, model_training, monitoring) which are batch-oriented workflows, and also includes a FastAPI-based inference API (inference_api) for serving ML model predictions as a web service.",AWS
https://github.com/Sanket-Kathrotiya/Electricity-Demand-Prediction,MLflow Experiment Tracking System,"Batch, Web Service",The project uses Prefect for workflow orchestration (batch processing) and Flask/Docker for serving ML predictions via API (web service). The Makefile shows both orchestrate.py (batch) and deploy targets (web service).,Other
https://github.com/dmytrovoytko/MLOps-churn-prediction,AWS S3 Model Storage System,Web Service,"The repository contains a Flask-based prediction service (app.py) with a Dockerfile and docker-compose.yaml that builds and runs a web service on port 5555 using Gunicorn. The service serves ML models via API endpoints for churn prediction, which is characteristic of a Web Service deployment pattern.",Other
https://github.com/lennardong/yoshi_optionstrading,Options Trading Model Deployment,"Batch, Web Service","The repository contains both batch processing components (Prefect workflows for model training/retraining, MLflow for experiment tracking) and web service components (FastAPI backend for serving predictions). The Prefect flows handle scheduled model training and optimization, while the FastAPI application serves the prediction model via API endpoints.",Other
https://github.com/ta-brook/Credit-Card-Fraud-Prediction,Credit Card Fraud Detection Pipeline,"Batch, Web Service",The project uses Mage.ai for workflow orchestration (batch processing) and Flask for model serving via API (web service). The docker-compose shows both Mage (batch orchestrator) and a Flask deployment service.,GCP
https://github.com/manova01/insured_project,Insurance Data Monitoring Dashboard,"Batch, Web Service","The repository contains a Prefect workflow orchestration script (`model_training_flow.py`) for batch model training pipelines, and a Flask application (`app.py`) with Dockerfile for serving ML model predictions via REST API. The presence of both scheduled/orchestrated training jobs and API-based model serving indicates both batch and web service deployment patterns.",Other
https://github.com/rohmats/mlops-project-customer-churn,Customer Retention Data Pipeline,Web Service,"The repository contains a Flask web application for serving ML model predictions (Flask listed in requirements.txt, Flask mentioned in README for creating web app, Makefile shows data processing pipeline but primary deployment is via Flask API)",Other
https://github.com/Nester4u/MLOPS-Project-Price-Prediction-for-Real-Estate,Real Estate Price Prediction Pipeline,"Batch, Web Service","The project includes batch processing workflows orchestrated by Prefect for training and prediction pipelines, and also provides an API service for real-time predictions based on request parameters. The batch component handles scheduled model training and batch predictions on CSV files, while the web service component serves predictions via API endpoints.",AWS
https://github.com/nrx33/taxi_chicago_prediction_mlops,Chicago Taxi Ride Analytics,Batch,"The project uses Mage for workflow orchestration with scheduled ETL jobs, and includes a train_batch.py script that runs in a container. The Docker Compose setup includes a train-batch service that is profiled for deployment, indicating batch processing rather than real-time streaming or web service serving.",Other
https://github.com/arismendyl97/classifying-gender-by-name,ML-Powered Name Classification Service,Web Service,"The project uses Docker Compose to run a Flask API service (flask_app) that exposes an endpoint on port 4500, along with monitoring services (Prometheus and Grafana). The Dockerfile shows the application runs ""python flow.py"" which serves the model via API. This is a classic web service deployment pattern with containerized API serving.",Other
https://github.com/pedrochitarra/indicators-of-heart-disease,ML Model Deployment for Healthcare,Web Service,"The repository contains a FastAPI application (app/main.py) served via Docker container using uvicorn-gunicorn base image. The Dockerfile shows it's running a FastAPI server on port 80, and the docker-compose.yaml includes infrastructure for database and monitoring. This is clearly a web service deployment pattern for serving ML model predictions.",Other
https://github.com/Hokfu/campaign_success_prediction_with_mlops,Campaign Performance Monitoring System,"Batch, Web Service","The repository contains both batch deployment (orchestration/mlops/etl.py, orchestration/mlops/pipelines/etl) and web service deployment (deployment/web_service_deployment/Dockerfile with FastAPI/uvicorn, deployment/batch_deployment/Dockerfile for batch processing). The batch component uses Mage workflow orchestrator for ETL pipelines, while the web service component serves ML models via API endpoints.",Other
https://github.com/Isaac-Ndirangu-Muturi-749/Car-Price-Prediction-End-to-End-MLOps-Pipeline,Car Price Prediction Pipeline,Web Service,"The repository contains a Flask web service for serving ML model predictions (app.py, test_predict.py) and Docker configurations for containerizing this service. The deployment uses MLflow for model serving and includes Dockerfiles specifically for web service deployment.",AWS
https://github.com/Jaykold/dry-bean-predictor,Dry Bean Classification System,Web Service,"The project serves ML models via a Flask API (app.py) and includes a predict.py component for model inference. The Dockerfile exposes port 9696 and uses gunicorn to serve the Flask application, confirming it's a web service deployment.",Other
https://github.com/jingxp/nasa-neo-hazard-prediction.git,NEO Hazard Prediction Pipeline,"Batch, Web Service",The repository contains both a Prefect workflow orchestrator (training_flow.py) for batch processing and a Flask API (app.py) for serving ML model predictions. The docker-compose shows both services running together.,Other
https://github.com/btalha23/Personalized_Recipe_Recommender,LSTM Recipe Model Pipeline,"Batch, Web Service","The repository contains a Prefect workflow orchestration (training_orchestration.py) that runs scheduled ETL jobs for model training and experiment tracking, which is characteristic of Batch deployment. It also includes a Dockerfile for containerizing the model as a web service (gunicorn server on port 9696) and deployment to AWS EC2, indicating Web Service deployment.",AWS
https://github.com/blackBagel/NHTSA-FARS-MLOps-Project,Car Accident Severity Prediction,"Batch, Web Service","The project uses Prefect for orchestrating batch data processing workflows (datasets_updater) and ML model training (model_trainer), while also serving predictions via a Flask-based web service (accident-injury-prediction-service on port 9696).",GCP
https://github.com/gokuld/product-category-prediction/tree/dev,Flask API Product Categorization,Web Service,"The repository README states it's a ""Machine learning model as a service to classify product description text into categories."" The codebase includes Terraform infrastructure for AWS deployment with VPC, subnets, and networking components, indicating a web service architecture for serving ML model predictions via API endpoints.",AWS
https://github.com/ReveredEye/DnDClassClassification,Character Data Classification Pipeline,"Batch, Web Service","The project uses Airflow (DAGs) for batch processing of data and model training, and Flask for serving ML predictions as a web service. The Airflow DAG orchestrates ETL and model training jobs, while the Flask app serves the trained model via API endpoints.",Other
https://github.com/AbdallaAbker/MLOps_Canadian_Forest_Fire_Prediction,Canadian Forest Fire Risk Prediction,Web Service,"The repository contains a FastAPI application (app.py) and Docker configuration for serving ML model predictions via API endpoints. The Dockerfile exposes port 8000 and runs Uvicorn to serve the FastAPI app. Additionally, there's a Streamlit dashboard for model deployment and monitoring. The project is focused on serving ML model predictions rather than batch processing or streaming data.",Azure
https://github.com/ruqianq/aws-xgboost-mlops-project/tree/main,AWS Serverless ML Model Deployment,Batch,"The repository uses Mage AI as a workflow orchestrator to automate ML training pipelines. The orchestration directory contains Docker Compose and Dockerfile configurations for Mage, along with requirements for hyperopt and MLflow, indicating scheduled/batch execution of training jobs rather than real-time streaming or web service deployment.",AWS
https://github.com/celik-muhammed/MLOps-Zoomcamp-M7-Project-Attempt-1-Mushroom-Classification,Mushroom Classification Pipeline,Batch,"The repository contains Mage workflow files (mage_0_ingest_data.py, mage_1_train_model.py, mage_2_batch_prediction.py) which are batch ETL/orchestration jobs. The project uses Docker Compose for local orchestration and includes batch prediction scripts (predict_batch_s3.py) with scheduled execution patterns. No streaming components (Kafka, Flink) or web service APIs are present.",Other
https://github.com/mj-ml/spain-energy-forecasting,Unknown,Unknown,No files fetched,Unknown
https://github.com/ruqianq/aws-xgboost-mlops-project,Mage AI MLOps Restaurant Solution,"Batch, Web Service","The project uses Mage AI for batch orchestration of ML training pipelines (evidenced by Dockerfile and docker-compose.yml in orchestration/), and deploys models as AWS Lambda functions for web service inference (evidenced by Flask/MLflow in deployment/requirements.txt and Lambda deployment in CDK code).",AWS
https://github.com/amitkooner/mlopszoomcamp_final_project/tree/main/final_project,Bike Rental Demand Forecast,Web Service,"The project serves a trained machine learning model via a Flask web service (app.py) that exposes a /predict API endpoint for receiving predictions. The Dockerfile is configured to run app.py, indicating this is deployed as a web service for model inference.",GCP
https://github.com/cjvandijk/predict_machine_failure,Machine Failure Prediction Pipeline,"Batch, Web Service","The repository contains a Mage AI pipeline for batch data processing and model training, and a Flask/Gunicorn prediction webservice for serving ML models via API. The docker-compose shows both components running as separate services.",Other
https://github.com/aalvan/mlops-diabetes,Diabetes Prediction Pipeline,"Batch, Web Service","The repository contains both a Mage orchestration pipeline (src/mlops) for batch processing of data preprocessing, model training, and evaluation, and a web service component (src/web-service-mlflow) that serves the diabetes prediction model via API on port 9696 using Docker.",Other
https://github.com/the-horhe/mlops-zoomcamp-project,Meteorological Data Pipeline,"Batch, Web Service","The repository contains a training pipeline orchestrated by Dagster (batch processing) and a Flask-based web service for model inference. The training pipeline runs scheduled jobs to train and register models, while the web service exposes a REST API for predictions.",AWS
https://github.com/sushant0709/sarcasm-detection,ML-Powered Text Analysis Pipeline,Web Service,"The repository contains a FastAPI application (main.py) deployed as an AWS Lambda function with API Gateway integration. The code shows a REST API endpoint (/predict) that serves ML model predictions for sarcasm detection. The Dockerfile and deployment configuration are specifically for serving the model via HTTP API, not for batch processing or streaming data pipelines.",AWS
https://github.com/jdaguilar/mlops_zoomcamp_project,Bank Churn Prediction Platform,"Batch, Web Service","The repository contains a Mage AI workflow orchestrator (mlops_zoomcamp_project_ml_pipeline) for batch ML pipeline execution, and a Flask web service (web_service/Dockerfile) for serving ML model predictions via API. The Mage pipeline handles ETL/data processing steps, while the Flask app provides real-time inference endpoints.",Other
https://github.com/QuentinElGuay/belo-horizonte-pricing,Belo Horizonte Real Estate Pricing,Batch,"The repository contains an Airflow DAG (auto_training.py) that schedules model training jobs monthly, indicating a batch processing workflow for MLOps. The project uses DockerOperator to run training tasks in containers on a schedule.",AWS
https://github.com/vucongtuanduong/heart-disease-prediction-mlops,Heart Disease Prediction Pipeline,"Batch, Web Service","The repository contains both batch deployment scripts (using Docker for scheduled predictions) and web service deployment components (Flask, Gunicorn for real-time API serving). The orchestration directory uses Mage for workflow automation, which is a batch-oriented orchestrator.",Other
https://github.com/zhelanov/diabetes-prediction-model,Diabetes Health Indicators Analytics,Batch,"The project implements a batch ML pipeline using Prefect for workflow orchestration, with scheduled model training and validation jobs. The code shows batch processing patterns with periodic data extraction, model training, and experiment tracking rather than real-time streaming or web service deployment.",AWS
https://github.com/PaulinaLP/cancer-detection,Skin Cancer Detection Pipeline,"Batch, Web Service","The project uses Airflow DAGs for batch processing (data ingestion, preprocessing, training) and Flask-based web service for model deployment with Docker. The Airflow DAGs handle scheduled ETL/training workflows, while the Flask service serves predictions via API.",Other
https://github.com/hr-02/Email-spam-detection,Email Spam Detection Pipeline,"Batch, Web Service","The project uses Prefect for workflow orchestration (batch processing) and includes Docker-based model serving with a predict.py entrypoint (web service). The Dockerfile shows a containerized prediction service, while prefect.yaml and deployment orchestration indicate batch pipeline execution.",AWS
https://github.com/RAGHUBIHARI/car-price-prediction.git,Used Car Price Prediction,"Batch, Web Service","The repository contains a Prefect workflow for batch processing (ETL, hyperparameter tuning, model registration) and a Flask application for serving predictions via API. The Dockerfile exposes port 4545 and runs predict.py, indicating a web service deployment for model inference.",Other
https://github.com/Muhongfan/MLops-zoomcamp-2024/tree/mlops-project,Electric Vehicle Consumption Analytics,"Batch, Web Service","The repository contains Mage (an orchestration tool) for batch ETL workflows and a web service component (Flask/FastAPI) for serving ML models via API. The Mage setup indicates batch processing, while the Dockerfile and predict.py suggest a web service deployment.",AWS
https://github.com/hadrrb/predict-online-game-behaviour-mlops,Flask API Game Model,"Batch, Web Service","The project uses Mage AI for batch orchestration (training pipeline) and Flask for web service deployment (model API). The Mage pipelines handle scheduled ETL/training jobs, while the Flask app serves the ML model via API endpoints.",Other
https://github.com/jesusoviedo/paraguay-public-servant-salary-calculato,Paraguay Government Data ML Pipeline,"Batch, Web Service","The project contains Mage AI pipelines for batch data processing and MLflow for model tracking, plus deployment configurations for web services (Flask API, Streamlit dashboard). The pipelines handle scheduled ETL jobs while the deployment folder contains web service components.",AWS
https://github.com/MCAyd/mlops-webservice-streaming-insurance-premium-prediction,MLOps Insurance Premium Pipeline,"Streaming, Web Service",The project serves ML models via a Flask web service (web_service/routes.py) and uses AWS Kinesis for real-time data streaming between the web app and Lambda function. Lambda processes streaming data from Kinesis input stream and returns predictions via Kinesis output stream.,AWS
https://github.com/nathadriele/mlops-zoomcamp-project-paris-price-house,Paris Real Estate ML Pipeline,"Batch, Web Service","The project contains both batch and web service components. The batch component is evidenced by the Prefect flow (paris_housing_prefect_flow.py) that orchestrates scheduled model training and deployment tasks. The web service component is evidenced by the Flask API (paris_housing_api.py) that serves the trained model for predictions, along with Docker deployment configurations.",Other
https://github.com/victorfxz/Apartment-Rental-Predictor/tree/main,SÃ£o Paulo Apartment Price Predictor,"Batch, Web Service","The project includes a Flask web service (front-end/app.py) for serving ML predictions and a Prefect workflow (prefect_flow.py) for training/prediction orchestration, indicating both web service and batch components.",Other
https://github.com/kabiromohd/Ride-Duration-Prediction,Divvy Bikeshare Duration Prediction,"Batch, Web Service","The repository contains both batch processing components (Mage workflow orchestrator for ETL jobs, data preparation, and model training) and web service components (Flask-based model deployment with Docker, deployment to Render cloud service, and API endpoints for model inference). The batch component handles scheduled data processing and model training, while the web service component serves the trained model via API endpoints.",AWS
https://github.com/ipekguler/Weather-Prediction-Web-Service,Weather Forecast Web Service,Web Service,"The repository contains a Flask-based web service (predict.py) that serves ML model predictions via HTTP API on port 9696, with Docker containerization for deployment. The web service is explicitly described as a ""web service integrated with MLFlow using Flask and gunicorn"" for weather prediction.",Other
https://github.com/Alexander-Heinz/autoscout-price-prediction,German Used Car Price Prediction,"Batch, Web Service","The project uses Mage for batch data processing and model training pipelines, and includes a Dockerized web service for serving ML model predictions via REST API. The docker-compose shows both Mage (batch orchestrator) and a webservice component.",Other
https://github.com/dmytrovoytko/mlops-spacy-sentiment-analysis,Amazon Reviews Sentiment Analysis,Web Service,"The repository contains a Flask-based prediction service (app.py, predict.py) with a Dockerfile exposing port 5000 and using Gunicorn as the WSGI server. The service loads a trained SpaCy model and serves predictions via API endpoints. There are no workflow orchestrators, message brokers, or streaming components present.",Other
https://github.com/atishgautam07/max-reutrns-mlops,Technical Indicators ML Pipeline,Batch,"The repository contains a Kubeflow/Vertex AI pipeline with containerized components (data ingestion, transformation, model training, evaluation, prediction, monitoring, simulation) orchestrated as batch jobs. The Dockerfiles and entrypoints indicate scheduled/triggered execution rather than continuous streaming or web service deployment.",GCP
https://github.com/francobach47/MusicGenreClassification,Music Genre Classification System,"Batch, Web Service","The repository contains both batch processing components (Airflow DAG for orchestration, preprocessing and training scripts) and web service components (Flask dependency in pyproject.toml, inference.py for model serving). The Airflow DAG indicates scheduled/batch ETL workflows, while Flask and inference capabilities suggest a web service for model serving.",Other
https://github.com/WayneHsiao0225/mlops-zoomcamp_project,Financial Services Retention Analytics,Batch,"The repository uses Prefect for workflow orchestration with scheduled ETL jobs (training.py, evidently_metrics_calculation.py) and Docker Compose for deployment. The ML model is trained in batch mode using MLflow for experiment tracking, and there are no streaming components like Kafka or real-time APIs for ML inference.",Other
https://github.com/Gjloz009/mlops_proyect_jaf/,Federal Telecom Data Pipeline,Batch,"The repository contains Airflow DAGs (etl_ift.py, etl_ift_toy_model.py) that orchestrate scheduled ETL jobs. The docker-compose.yaml sets up Airflow with CeleryExecutor for batch processing. The workflow downloads files, transforms data, and loads to S3 - all batch-oriented operations.",AWS
https://github.com/AkanimohOD19A/submission_MLOPs24-cohort,Taxi Fleet Optimization Pipeline,"Batch, Web Service","The repository contains a ZenML training pipeline with steps for data ingestion, cleaning, and model training, indicating batch processing. It also includes Streamlit for model serving/visualization, indicating a web service component. The presence of MLflow model deployment further supports the web service aspect.",Unknown
https://github.com/HemalathaRamanujam2022/mlops_temperature_prediction,Weather ML Model Deployment,"Batch, Web Service","The repository contains both batch and web service components. The batch component is orchestrated using Prefect for data preparation, feature engineering, model training, hyperparameter tuning, and model tracking. The web service component is implemented using Flask to serve the trained model as a REST API for making temperature predictions.",AWS
https://github.com/Tobai24/In-Hospital-Mortality-Prediction,Clinical Data Monitoring Pipeline,"Batch, Web Service","The repository contains both batch processing components (Mage pipeline for ETL/orchestration, MLflow for experiment tracking) and web service components (Flask API for model serving, Streamlit dashboard for visualization, Grafana dashboards for monitoring). The Mage pipeline handles scheduled data processing while the Flask application serves the ML model predictions via API.",GCP
https://github.com/alex-kolmakov/divelog-autoreport/tree/main,SCUBA Dive Analytics Pipeline,Batch,"The repository uses MageAI (0.9.73) as an orchestrator with a docker-compose setup that runs scheduled ETL jobs. The code includes dlt (data load tool) pipelines for REST API extraction and LanceDB vector storage, along with MLflow for model tracking. The structure shows batch-oriented workflows rather than real-time streaming or web service deployment.",Other
https://github.com/ArturGR3/MLOps-project,Flask ML Service Template,Web Service,"The repository contains a Flask application (predict.py) with Dockerfile and Makefile for containerization and orchestration. The Makefile shows commands to build and start a Flask app container, simulate requests, and includes Prometheus monitoring. This is a classic web service deployment pattern serving ML models via API endpoints.",GCP
https://github.com/baidlowi/prediction-student-performance.git,Student Performance Prediction Service,"Batch, Web Service","The project includes a Flask-based prediction service (studentpred_service) that serves ML models via API, and uses Prefect for orchestrating data pipelines and workflows. The docker-compose.yml shows both web service components (Flask apps on ports 9696, 8085) and mentions Prefect for pipeline automation in the README.",Other
https://github.com/TommyBark/LitigAItor-mini/,Legal RAG Model Deployment,"Batch, Web Service","The project has a Prefect workflow orchestrator (batch processing) for RAG flows and model training, plus a Gradio web interface (web service) for the chatbot interface. The Dockerfile shows the main application runs on port 7860 with Gradio.",AWS
https://github.com/Tiamz01/Drinking-water-safety-classification,ML-Powered Water Quality Monitoring,Web Service,"The repository contains a Flask application (predict.py) and Docker configuration for serving ML model predictions via HTTP API on port 9696. The Dockerfile uses gunicorn to serve the Flask app, and the testing/README.md explicitly states ""Deploying a model as a web-service"" with instructions for running the Docker container.",GCP
https://github.com/maks36-dev/loan-approval-prediction,Loan Approval Prediction Pipeline,"Batch, Web Service","The project includes batch processing components (data preprocessing, model training, hyperparameter optimization via init.py) and a web service component (Flask app.py for serving predictions via API). The docker-compose.yml also shows database and monitoring services typical of batch ML pipelines.",Other
https://github.com/nilarte/mlops-zoomcamp-project-cohort-2024.git,Employee Salary Prediction Pipeline,"Batch, Web Service","The repository contains both batch processing components (Mage pipeline for model training with ingest, train, and save stages) and web service components (Gradio-based model deployment on HuggingFace Spaces). The batch component uses Mage for pipeline orchestration, while the web service component serves the ML model via Gradio interface.",Other
https://github.com/serg123e/packing-bags-forecast,E-commerce Bag Allocation Model,Batch,"The project uses scheduled ETL jobs (upload_csv.py, download.py, ingest.py, transform.py) and batch training/prediction scripts. The architecture shows data is pulled periodically, transformed, and loaded to a warehouse for batch processing rather than real-time streaming or continuous API serving.",AWS
https://github.com/krmdel/mlops_zoomcamp24_admission_prediction_project,Admission Likelihood Forecasting Service,Web Service,"The repository contains a Flask API (predict.py) that serves ML models via HTTP endpoints (port 9696), a Streamlit web application for user interaction, and Docker configurations for containerized deployment. The code shows a web service architecture with API endpoints for prediction requests.",AWS
https://github.com/kev-wes/2024_heart_attack_mlops,Heart Attack Risk Prediction,"Batch, Web Service","The repository contains both batch processing components (Prefect workflows for model training and hyperparameter tuning orchestrated via Docker containers) and web service components (Flask-based prediction service exposed on port 8000, MLflow server on port 5000). The docker-compose.yaml shows services for both batch orchestration (Prefect) and web services (MLflow, Flask app).",Other
https://github.com/oelghareeb/mlops-zoomcamp-project/tree/main,CNN Model Deployment Platform,Web Service,"The repository contains a Flask web application (deploy.py) that serves a fashion image classifier model via HTTP endpoints. The Dockerfile exposes port 80 and runs ""flask run"", and the README shows a web interface for uploading images and getting predictions. This is a classic web service deployment pattern for ML model serving.",AWS
https://github.com/bilozorov/mlops-capstone,Heart Attack Risk Prediction,"Batch, Web Service","The project includes Mage workflow orchestration for batch ETL/model training pipelines (load_data, transform_data, train_models, register_best_model) and a Flask API containerized with Docker for serving predictions via HTTP endpoint on port 9696.",AWS
https://github.com/ldebele/End-to-End-Airbus-Ship-Detection,Satellite Ship Detection System,"Batch, Streaming, Web Service","The repository contains a multi-component system: (1) Batch pipeline for training orchestrated by Airflow with data ingestion, preprocessing, model training, and evaluation components; (2) Web Service for model inference via FastAPI endpoint; (3) Streaming pipeline using Kafka for event-driven inference with producer/consumer architecture.",Other
https://github.com/remitoudic/forcast_energy_consumption,France Energy Forecast Pipeline,"Batch, Web Service","The project uses Mage AI for pipeline orchestration (ETL jobs) which is a batch processing workflow, and FastAPI for serving ML models via API endpoints. The docker-compose shows both mage (batch orchestrator) and fastapi (web service) services running.",Other
https://github.com/a-t-em/Asthma-Prediction,Asthma Prediction Model Pipeline,Web Service,"The repository contains a Flask app (scoring_script.py) that serves ML model predictions via a REST API endpoint. The Makefile shows 'launch_app' runs 'python scoring_script.py' which starts the Flask web service. While build_model.py trains the model, the primary deployment mechanism is the web service for serving predictions.",Other
https://github.com/sam23121/crop-recomendation/tree/master,Crop Recommendation ML System,Web Service,"The repository contains a FastAPI backend deployed on AWS Lambda and a Streamlit frontend deployed on Streamlit Cloud, both serving as web services for crop recommendation. The FastAPI backend provides API endpoints for ML inference, and Streamlit serves as a web-based frontend interface.",AWS
https://github.com/XiaoLirui/MLops-project,Mobile Price Prediction Pipeline,"Batch, Web Service",The project includes a web service component (Dockerfile with gunicorn serving a Flask API on port 9696) and batch components (Prefect workflows for ML training and deployment using mobile_ml.py and mobile_ml_prefect_deploy.py).,Other
https://github.com/kahramanmurat/NYC-Bike-Share-Project,Bike Trip Time Estimation Platform,Web Service,"The project includes a Flask app (app.py) and Dockerfile for serving ML predictions via API. The README shows ""http://localhost:8080"" endpoint for usage. While there are ML pipeline components, the primary deployment mechanism is a web service for real-time predictions.",Other
https://github.com/atheeralattar/mlops-project,Flight Price Prediction Pipeline,"Batch, Web Service","The project uses Mage for batch pipeline orchestration (data prep, modeling) and Streamlit for web service model serving/visualization",Other
https://github.com/AMaldu/fraud_detection,Mobile Money Fraud Detection,"Batch, Web Service",The project uses Prefect for batch orchestration (scheduled flows in prefect.yaml) and includes a Dockerized Flask/FastAPI service (src/Dockerfile exposes port 9696) for serving predictions. The Prefect flows handle ETL/data processing while the web service provides API endpoints.,Other
https://github.com/Avyukth/mlops-Zoomcamp/tree/main/notebooks/project-01,Heart Disease Prediction Pipeline,"Batch, Web Service","The repository contains both batch processing components (Docker Compose setup with MLflow for model training, orchestrated via Docker containers) and web service components (FastAPI API for model serving and Streamlit dashboard for visualization). The MLflow training setup represents batch model training, while the FastAPI and Streamlit applications provide web service interfaces.",Other
https://github.com/alex-s-888/Travel-Insurance-Prediction,Unknown,Unknown,No files fetched,Unknown
https://github.com/ppatrzyk/rss-store,Text Classification Model Training,"Batch, Web Service",The project uses Prefect for batch orchestration to continuously update RSS feeds and retrain ML models on a schedule (cron-based). It also serves ML predictions via a web service using MLflow's model serving API on port 5000.,Other
https://github.com/kachiann/mlops-project,Unknown,Unknown,No files fetched,Unknown
https://github.com/tiznobaik/mlops-zoomcamp-project/tree/main,Bank Customer Churn Prediction,Batch,"The repository contains Python scripts for data loading and preprocessing that are executed as standalone programs, indicating a batch processing workflow. The project uses Google Cloud Storage for data and includes configuration for scheduled or periodic data processing, which is characteristic of batch ETL jobs.",GCP
https://github.com/quzanh1130/chat_moderation_mlops,Chat Moderation ML Pipeline,"Batch, Web Service","The repository contains both batch and web service components. The orchestration directory uses Mage (a workflow orchestrator) for batch processing with pipelines for data processing, model training, and deployment. The app directory contains a Flask-based web service with a Dockerfile exposing port 9696 and using gunicorn, indicating a web service deployment for model inference.",Other
https://github.com/lillianphyo/mlopsprj022024.git,Myanmar Rice Price Forecasting,Web Service,"The repository contains a Flask application (run.py) served via Docker and docker-compose, with a gunicorn WSGI server on port 5000. This is a classic web service deployment pattern for serving ML predictions via API endpoints. The MLflow service is also exposed on port 5001, which is a web-based model registry and tracking server.",Other
https://github.com/shimkoji/footballer_prediction.git,Unknown,Unknown,No files fetched,Unknown
https://github.com/omjirapat/mlops-zoomcamp/tree/main/project,Bike Rental Demand Prediction System,Batch,"The project uses Prefect for workflow orchestration with batch prediction scripts (batch_predict.py) that run on scheduled inputs (year, season). The architecture includes MLflow for model tracking and batch deployment, with no streaming components or web service APIs for real-time inference.",Other
https://github.com/begoechavarren/sermadrid,SER Zone Time Series Forecasting,Web Service,"The repository contains a FastAPI backend application that serves ML models for parking availability predictions via API endpoints, and a Vue.js frontend served by nginx. The backend loads models at startup and responds to user requests, which is characteristic of a web service deployment pattern.",Other
https://github.com/SebastianSydlik/MRI-based-dementia-prediction,MRI Brain Scan Dementia Predictor,Web Service,The repository contains a Flask-based prediction service (predict.py) with a Dockerfile exposing port 9696 and using gunicorn to serve the model. The predict.py file implements a Flask app with an /predict endpoint that accepts POST requests with MRI scan data and returns dementia probability predictions. This is a classic web service deployment pattern for ML models.,Other
https://github.com/hwting1/MLOps-Zoomcamp-Project,London Bicycle Rental Predictions,Batch,"The repository uses Prefect for workflow orchestration to run scheduled ETL jobs (batch_predict.py with parameters), MLflow for model tracking, and performs batch predictions on data files rather than serving real-time API requests or streaming data.",Other
https://github.com/oelghareeb/mlops-zoomcamp-project,Image Upload Prediction Service,Web Service,"The repository contains a Flask web application (deploy.py) that serves a CNN model for fashion item classification via HTTP endpoints. Users can upload images and receive predictions through a web interface. The Dockerfile and Makefile are configured to run this Flask app, confirming it's a web service deployment.",AWS
https://github.com/alex-s-888/Travel-Insurance-Prediction-V1,Unknown,Unknown,No files fetched,Unknown
https://github.com/AbdelrahmanElmasry/mlops-bike-share-prediction,Bike Share Membership Prediction,Web Service,"The repository contains a Dockerfile that builds a Flask-based prediction service using gunicorn to serve an ML model (linear_reg.bin) via API on port 9696. The predict.py file (referenced in the Dockerfile) implements a Flask app for model inference, which is characteristic of a Web Service deployment pattern.",AWS
https://github.com/Bt-PplusK/Mlops-project-1/tree/main,Unknown,Unknown,No files fetched,Unknown
https://github.com/AP0202/ml-Data_Science_Salary_Estimator,Data Science Salary Predictor,Web Service,"The repository contains a Flask web service (predict.py) that exposes a REST API endpoint at port 9696 for salary prediction. The Dockerfile and docker-compose.yml are configured to run this web service, and the README describes containerizing the project for deployment.",Other
https://github.com/KevinPericart/MLOps-Project/tree/main,AWS MLOps Infrastructure Pipeline,Batch,"The project uses Prefect for workflow orchestration with tasks for data preparation, preprocessing, embedding, and model training. The code shows a Prefect flow with multiple tasks that run sequentially to train and evaluate a CatBoost model, which is characteristic of batch processing rather than real-time streaming or web service deployment.",AWS
https://github.com/victornemenike/fuel-price-prediction,Unknown,Unknown,No files fetched,Unknown
https://github.com/mleiwe/F1_PredictQualifying,Formula 1 Lap Time Forecast,Unknown,"The repository contains data access and analysis code but no clear deployment infrastructure, orchestration workflows, API services, or streaming components are visible in the provided files.",Unknown
https://github.com/thiagoribeiro00/MLOps-Azure-Recomendation/tree/main,Flask Web App Serving,Web Service,"The repository contains a Dockerfile that runs ""app.py"" and uses Flask in requirements.txt, indicating this is a web service application. The README also mentions serving ML models via API using Flask/FastAPI/BentoML.",Azure
https://github.com/gsenseless/mlOps_wrap,Vehicle MPG Prediction System,Batch,The repository contains a Docker-based ML training script (train_model.py) and prediction script (predict.py) that are executed as discrete command-line jobs via docker compose run. The workflow is triggered manually/scheduled rather than serving continuous API requests or real-time streaming.,AWS
https://github.com/lillianphyo/mlohttps://github.com/lillianphyo/mlopsprj012024.gitpsprj012024.git,Unknown,Unknown,No files fetched,Unknown
https://github.com/ParthRajauria/PredictingStudentGrade_EndToEndMLOPs/tree/main,Education Data ML Pipeline,Batch,"The repository uses Prefect for model orchestration, which is a workflow orchestrator for scheduled/batch processing. The project follows MLOps practices with experiment tracking (MLflow) and batch model training pipelines, but shows no evidence of streaming or web service deployment.",Other
https://github.com/haghighi2697/mlopsproject,Machine Learning Artifact Repository,Unknown,"The repository contains only MLflow model artifacts and requirements files, with no actual code for deployment, orchestration, or serving. No evidence of batch workflows, web services, or streaming components can be found.",Unknown
https://github.com/quickSilverShanks/ReviewSentinel,Machine Learning Review Scanner,Batch,"The repository uses Mage AI, a workflow orchestrator for ETL pipelines, with scheduled data processing jobs (data_preparation, data_profiling, model_training). The architecture shows batch-oriented components like scheduled pipelines, data versioning, and periodic model training rather than real-time streaming or continuous API serving.",Other
https://github.com/gsenseless/mlOps_bikesharing,ML-Powered Bike Sharing System,"Batch, Web Service","The project uses Apache Airflow (DAGs) for batch data processing and model training, and also provides a Flask/FastAPI-based prediction API service for real-time inference.",Other
https://github.com/toema/mlops-mage,Taxi Tip ML Model Deployment,Batch,"The repository uses Mage.ai, a workflow orchestrator, to run scheduled ML pipelines. The docker-compose.yml shows Mage starting with ""mage start"" command, and the project structure contains multiple pipelines for data processing and model training. This is a batch-oriented ML pipeline system rather than a real-time streaming or web service deployment.",AWS
https://github.com/alexeygrigorev/build-your-own-search-engine,Zoomcamp FAQ Search Engine,Unknown,"The repository contains educational code for building a search engine with no actual deployment infrastructure, orchestration, or service components. It's a workshop with Jupyter notebooks and Python scripts for learning purposes.",Unknown
https://github.com/brnakin/MLOps-Zoomcamp/tree/main/project,Hyperopt Hyperparameter Optimization Lab,Unknown,"The repository contains only MLflow model artifacts, requirements files, and a Jupyter notebook for model training and experimentation. There is no code for deployment, serving, or orchestration that would indicate batch processing, web service APIs, or streaming pipelines.",Unknown
https://github.com/ellacharmed/mlops-zoomcamp/tree/main/07-project,ML Infrastructure Deployment Pipeline,"Batch, Web Service","The repository contains a course project template with instructions for building end-to-end ML projects. The README describes three deployment options: batch scoring, web service (Flask/FastAPI), and streaming (Kafka/Kinesis). The project structure includes modules for both batch deployment (Module 4 covers batch scoring) and web service deployment (Flask-based model serving). No actual streaming implementation code is present in the repository.",Unknown
https://github.com/ReemJbily/MLOPS/blob/main/01-intro/homework.ipynb,ML Model Performance Monitoring,Unknown,"The repository contains only a Jupyter notebook with a simple machine learning homework exercise (training a linear regression model on the California housing dataset). There is no code for deployment, orchestration, API serving, or streaming infrastructure.",Unknown
https://github.com/s100rab/s100rab_MLOps_DataTalks_zoomcamp/tree/main/project_1_main,Automated Wine Model Training,"Batch, Web Service",The project includes both batch orchestration (Prefect workflows for model training/registration) and web service deployment (FastAPI/gunicorn service on port 9696 for model inference),Other
https://github.com/Yasser314/Proyecto_1,Unknown,Unknown,No files fetched,Unknown

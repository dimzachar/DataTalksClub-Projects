project_url,project_title,Deployment Type,Reason,Cloud
https://github.com/yokalyan/mlops-churn,ML Pipeline for Churn Detection,"Batch, Web Service","The repository contains a Prefect workflow orchestrator (prefect.yaml, prefect-worker) for batch processing and pipeline orchestration, plus a FastAPI-based inference API (src.api.customer_churn_prediction_web) for real-time predictions. The docker-compose shows both Prefect services (batch) and FastAPI deployment.",AWS
https://github.com/AnnaGanisheva/churn-prediction,Automated Churn Monitoring Dashboard,"Batch, Web Service","The project uses Prefect for batch pipeline orchestration (data ingestion, split, training) and Streamlit for a web service interface for model inference. Prefect handles scheduled/batch workflows while Streamlit serves the model predictions via a web UI.",AWS
https://github.com/juditkisistok/eurovision-vote-predictor,Eurovision Song Performance Predictor,"Batch, Web Service","The project includes a FastAPI service (api.py) for real-time predictions and a Prefect workflow (eurovision_point_predictor.py) for scheduled model training and data processing. The FastAPI serves ML model predictions via REST endpoints, while Prefect orchestrates batch training jobs.",GCP
https://github.com/rayluo88/diabetes-prediction-mlops,Diabetes Prediction Pipeline,"Batch, Web Service","The project implements both batch processing (Prefect workflows for data pipeline, training, and monitoring) and web service (FastAPI model serving with Docker containerization). The Makefile shows commands like 'run-data-pipeline' and 'run-training' indicating batch workflows, while Dockerfile.api and FastAPI dependencies indicate web service deployment.",Other
https://github.com/hsviscarra/StockPrediction,Automated Stock Model Training,"Batch, Web Service","The project uses Apache Airflow for batch processing (data ingestion, model training DAGs) and FastAPI/Streamlit for serving predictions via web APIs and dashboards.",Azure
https://github.com/paultongyoo/churn-model-evaluation-platform,Scalable Model Deployment Platform,"Batch, Web Service","The repository contains a Prefect workflow orchestrator (churn_model_training.py, churn_prediction_pipeline.py) for batch processing of model training and inference, plus multiple web services including MLflow tracking server, Optuna dashboard, Prefect UI, Evidently UI, and Grafana UI accessible via ALB. The S3-to-Lambda trigger also processes files in batch when uploaded.",AWS
https://github.com/MuhammadShifa/store-sales-forecasting-mlops.git,Store Sales Forecasting Pipeline,"Batch, Web Service","The repository contains both batch processing components (Prefect workflow orchestration for ML training pipelines) and web service deployment components (Flask-based API served via Docker containers). The README explicitly mentions support for ""web service deployment"", ""batch scoring"", and ""streaming predictions"" but the actual code shows only batch (Prefect) and web service (Flask/Docker) implementations - no streaming components like Kafka or real-time message brokers are present in the codebase.",AWS
https://github.com/LamelK/solar-prediction-mlops_zoomcamp,Solar Energy Prediction Pipeline,"Batch, Web Service","The project uses Prefect for batch workflow orchestration (data ingestion, processing, training) and FastAPI for serving ML models via API endpoints. The prefect_deployment.py shows scheduled pipeline deployments, while the docker-compose.yml shows an API service exposing model predictions.",AWS
https://github.com/aaarl/mlopszoomcamp_25/tree/main/project,Breast Cancer Detection Pipeline,Web Service,"The repository contains a FastAPI application (src/inference/app.py) with Docker configuration and docker-compose setup to serve ML model inference via REST API on port 8000. The Dockerfile builds a containerized FastAPI service, and the Makefile includes commands for building and running the Docker container.",AWS
https://github.com/aletbm/Cardiovascular-Diseases_E2E_MLPipeline,Cardiovascular Disease Risk Prediction,"Batch, Web Service",The repository contains both a Prefect training pipeline (pipelines/training_flow.py) for batch model training and a FastAPI-based REST API (deployment/serve.py) for serving predictions. The Makefile shows commands for both run-training (batch) and run-api (web service).,GCP
https://github.com/dayujdy98/live-fraud-detector,Real-Time Fraud Detection System,"Streaming, Web Service","The repository contains both streaming and web service components. Streaming: Kafka (MSK) topics setup, Flink job for real-time processing, and streaming dependencies (kafka-python, apache-flink). Web Service: FastAPI-based model serving API with ECS Fargate deployment, Docker containerization, and health endpoints.",AWS
https://github.com/RuiFSP/mlops-2025-final_project,Premier League Match Prediction System,"Batch, Web Service","The repository contains a FastAPI web service (src/api/main.py) for real-time predictions and a Prefect orchestration system (src/pipeline/orchestration.py) for batch training and monitoring workflows. The Dockerfile exposes port 8000 for the API, and the Makefile shows commands for both API and Prefect server management.",Other
https://github.com/DAMILARE1012/MLOps---Real-Time-Blockchain-Data-Show.git,Real-Time Bitcoin Anomaly Detection,"Streaming, Web Service","The repository implements a real-time blockchain data pipeline using WebSocket streaming from Blockchain.info with async processing, and includes a FastAPI web service for serving ML models. The Dockerfile shows uvicorn as the entrypoint for the API server, and the architecture diagram shows real-time data flow from WebSocket through the pipeline.",Other
https://github.com/i-azztec/mlops-volatility-news-predictor/,Stock Market Volatility Predictor,"Batch, Web Service","The project implements both batch processing via Prefect workflows (preprocess_flow, training_flow, scoring_flow) for scheduled ETL jobs and model training, plus a real-time web service via FastAPI for serving predictions. The architecture diagram shows dual deployment with both batch scoring and web service components.",Other
https://github.com/aletbm/Estimation_Obesity_Levels_E2E_Pipeline_Attempt2,Nutrition-Based Health Monitoring,"Batch, Web Service","The repository contains both batch processing components (Prefect workflows for training and batch inference) and web service components (FastAPI deployment with Docker and Cloud Run). The Makefile shows commands for both batch inference (`run-inference`) and API serving (`run-api`), and the Terraform configuration deploys a Cloud Run service for API access.",GCP
https://github.com/siddharthps/airwatch-mlops,Air Quality Prediction Pipeline,Batch,"The repository implements a batch-oriented MLOps pipeline using Prefect for workflow orchestration. The Dockerfile runs data preparation and inference flows sequentially, and the Makefile shows batch commands for running flows. The architecture processes EPA AQS data in scheduled batches rather than real-time streaming or web service deployment.",AWS
https://github.com/aphdinh/mlops-zoomcamp-project,Bike Sharing Demand Monitoring,"Batch, Web Service",The project implements both batch processing (Prefect workflow orchestrator for scheduled model training) and web service (FastAPI for model serving). The Prefect orchestrator handles batch training pipelines while FastAPI provides REST API endpoints for model inference.,AWS
https://github.com/Cadarn/mlopszoomcamp-student-grade-prediction/tree/main,ML Model Training Automation,Web Service,"The repository contains a FastAPI application for model serving (src/api/main.py), Docker configurations for API deployment, and Lambda handler for API Gateway integration. The primary deployment pattern is serving ML models via REST API endpoints.",AWS
https://github.com/rajat116/github-anomaly-project/,User Behavior Anomaly Detection,"Batch, Web Service","The project uses Apache Airflow with DAGs for batch processing (daily monitoring and inference pipelines) and FastAPI for serving real-time inference via API endpoints. Streamlit is used for visualization, not streaming.",AWS
https://github.com/Shrikant-Ganji/ecommerce-return-mlops,E-commerce Return Prediction Pipeline,"Batch, Web Service","The repository contains a Prefect 2.0 workflow (prefect_flow.py) that orchestrates scheduled ETL jobs for model training and data processing, indicating a Batch deployment. It also includes a FastAPI app (Dockerfile, deployment folder) for serving the trained model via API, indicating a Web Service deployment.",Other
https://github.com/TheItalianDataGuy/exoplanet-detection-mlops,Diabetes Prediction MLOps Pipeline,"Batch, Web Service","The project includes both batch processing (Airflow DAGs for ML pipeline orchestration) and web service components (FastAPI for model serving). The Makefile shows commands for both batch operations (trigger-ml-pipeline, backfill-dag) and web service operations (run, run-prod for FastAPI).",Other
https://github.com/Deathslayer89/DTC_MLOPS/tree/main/final_project/smart-energy-prediction,Smart Energy Consumption Prediction,Web Service,"The project includes a FastAPI application (src/api/app.py) with a Dockerfile and Docker Compose configuration for containerized deployment. The API serves ML model predictions and includes health checks, indicating it's designed as a web service for real-time inference.",AWS
https://github.com/Kamagatey/Mlops-project,MLOps Pipeline for Accident Analysis,"Batch, Web Service",The repository contains both batch orchestration (Prefect workflow for automated ML pipeline with scheduled runs) and web service deployment (FastAPI + Docker for serving ML models). The 02-Orchestration uses Prefect for batch processing while 03-Deployment uses FastAPI for web service.,Other
https://github.com/nickrussell2025/MLOps-churn-prediction,Bank Churn Prediction Pipeline,"Batch, Web Service","The repository contains both a Prefect workflow orchestration system for batch training pipelines (churn_prediction_pipeline) and a Flask API service for serving model predictions. The Dockerfile.model-api shows a web service deployment, while the Prefect configuration and training pipeline indicate batch processing for model training.",GCP
https://github.com/m-levytskyi/co2-emission-forecast,German Electricity CO2 Forecast,Web Service,"The repository contains a FastAPI web service (deployment/api.py) with Docker containerization and docker-compose for serving ML models. The Makefile shows commands like 'serve', 'test-api', and 'docker-run' indicating API deployment. While there are some pipeline orchestration elements, the primary deployment mechanism is the FastAPI web service.",Other
https://github.com/lorenzocesconetto/zoomcamp-mlops-2025-final-project,OHLCV Data Processing System,"Batch, Web Service","The project includes a batch ML pipeline orchestrated by AWS SageMaker Pipelines for training and monitoring (scheduled jobs), and uses SageMaker Endpoints for real-time model inference (web service). The README explicitly states ""We use `AWS SageMaker Pipelines` to orchestrate our pipelines"" and ""We use Amazon SageMaker to deploy a real-time model.""",AWS
https://github.com/Maxkaizo/milk_price_prediction,Mexico Milk Price Forecast,Web Service,"The project deploys a Lambda function via API Gateway (Terraform modules for lambda and api_gw) and includes orchestration code for model serving. The README mentions deploying to a production-ready API, and the Makefile shows MLflow/Prefect orchestration for model training, but the actual serving is via API Gateway/Lambda.",AWS
https://github.com/BalajiMittapalli/modis-landcover-mlops-pipeline,Satellite Data Processing Pipeline,"Batch, Web Service","The repository contains both a Prefect workflow orchestration system (flows/, Dockerfile with ml_pipeline.py) for batch processing of satellite data, and a Flask-based model server (src/deployment/model_server.py) for serving predictions via REST API. The Dockerfile shows a web service deployment with Flask/Gunicorn, while the Prefect flows handle batch data processing and model training.",Other
https://github.com/romanaumov/MLOps-Project,FastAPI Bike Rental Prediction Service,"Batch, Web Service","The project implements a batch processing pipeline using Apache Airflow with DAGs for ETL workflows, and also provides a web service via FastAPI for model predictions. The docker-compose shows Airflow services (webserver, scheduler) for batch orchestration, while the Makefile and pyproject.toml show FastAPI/uvicorn for serving predictions.",Other
https://github.com/JoshPola96/Company-Bankrupt-Prediction-MLOps,Company Bankruptcy Prediction Pipeline,"Batch, Web Service","The project uses Apache Airflow (docker-compose.yaml) for batch workflow orchestration of ML pipelines (DAGs for training, monitoring, deployment) and Streamlit (Dockerfile, streamlit_app volume) for serving real-time bankruptcy predictions via a web interface.",AWS
https://github.com/rahmaha/ship_fuel_co2,Ship Fuel Consumption Analytics,"Batch, Web Service",The project uses Prefect for workflow orchestration (scheduled training and monitoring flows) and FastAPI for serving ML model predictions via API endpoints.,Other
https://github.com/DavidILLX/diabetes_mlops,Diabetes Prediction MLOps Pipeline,"Batch, Web Service",The project uses Apache Airflow for batch orchestration (scheduled ETL jobs) and Flask API for serving ML predictions as a web service. The docker-compose includes both Airflow and App services.,AWS
https://github.com/sndryn/weather-forecast-mlops,ML Ops Weather Monitoring Platform,"Batch, Web Service","The project uses Dagster for batch data ingestion, model training, and batch forecasting pipelines that run on daily/monthly schedules. It also provides web services via MLflow (port 5000), Grafana (port 3001), and Dagster webserver (port 3000) for model serving, monitoring, and orchestration UI.",AWS
https://github.com/xmuruaga/mlops-amazon-recommender,Unknown,Unknown,No files fetched,Unknown
https://github.com/a920604a/stock-mlops,Real-Time Stock Price Prediction Platform,"Streaming, Web Service","The repository contains a streaming component using Kafka for real-time data processing (Dockerfile.ws_monitor, metrics_publisher, kafka-compose) and a Web Service component using FastAPI for serving ML models and metrics endpoints (Dockerfile.backend, FastAPI app). The system serves both real-time streaming data and provides API endpoints for model inference and monitoring.",Other
https://github.com/jyphotography/Bank-Customer-Churn-Prediction---MLOps-End-to-End-Project,Bank Customer Churn Prediction,"Batch, Web Service","The repository contains both batch processing (Prefect workflow orchestrator for ETL jobs, batch.py script) and web service components (FastAPI model serving with Dockerfile.fastapi, AWS Lambda handler for API deployment)",AWS
https://github.com/OnurKerimoglu/stocks_forecasting_live,Unknown,Unknown,No files fetched,Unknown
https://github.com/Abdelrahman-Adnan/customer-churn-prediction,ML Model Monitoring System,"Batch, Web Service","The repository contains both batch and web service components. For batch: Prefect workflows for automated training pipelines, Docker Compose services for training and monitoring that run scheduled jobs. For web service: FastAPI for model serving endpoints, Streamlit dashboard for interactive visualization, both exposed via HTTP APIs.",AWS
https://github.com/EbEmad/MLOPS-ZOOMCAMP-PROJECT,Predictive Maintenance Pipeline,"Batch, Web Service","The repository contains a Prefect workflow orchestration pipeline (Prefect deployment files, MLflow tracking) for batch training and model management, plus a FastAPI service (src.api:app, uvicorn command) for serving predictions. The Jenkins CI/CD pipeline and Docker build/run commands further confirm the web service deployment pattern.",AWS
https://github.com/Hrithik-Kumar/mlops-project#,Unknown,Unknown,No files fetched,Unknown
https://github.com/Foluwa/sentiment_youtube_mlops,Sentiment Analysis Pipeline for YouTube,"Batch, Web Service","The repository contains Airflow DAGs for scheduled batch processing (ingest, train, monitor) and a FastAPI service for serving sentiment predictions. The Streamlit dashboard is a visualization layer, not streaming.",Other
https://github.com/Mateocontrerass/credit-default-mlops/tree/main,Financial Risk Model API,Web Service,"The repository contains a FastAPI application with Dockerfile for containerization, API endpoints (/predict, /gradio), and is deployed as a web service on Google Cloud Run. The code shows ML model serving via REST API and Gradio UI, which are web service patterns.",GCP
https://github.com/Selbl/mlops-zoomcamp-project-2025,Grade Classification Model Serving,Web Service,"The repository contains a Flask API (flask_predict.py) served via Gunicorn in the Dockerfile on port 9696, which is a classic Web Service deployment pattern for ML model inference. The Dockerfile also includes Prefect and MLflow servers, but the primary serving mechanism is the Flask API.",Other
https://github.com/mamadyonline/mlops-project,Cardiovascular Disease Risk Prediction,"Batch, Web Service","The repository contains Apache Airflow DAGs (ml_training_dag.py, ml_batch_dag.py) for batch processing and orchestration, plus a FastAPI web service (webservice/app.py) for model serving. The code shows both scheduled batch workflows and a REST API endpoint for predictions.",AWS
https://github.com/aletbm/Intrusion_Detection_CIC-IDS2017_Attemp1,Network Intrusion Detection System,"Batch, Web Service","The repository contains both batch processing components (Prefect workflows for training and batch inference, Makefile targets for run-training and run-inference) and web service components (FastAPI application served via Docker container on Cloud Run, with uvicorn server configuration)",GCP
https://github.com/soham-chitnis10/credit-card-fraud-detection,Credit Card Fraud Detection System,"Batch, Web Service","The repository contains both batch processing components (MLflow server for model training with Prefect workflow orchestration) and web service components (FastAPI-based model serving). The training pipeline uses MLflow for batch model training and experiment tracking, while the web service uses FastAPI to serve the trained model via API endpoints.",AWS
https://github.com/FL1NTY24/patient-readmission-prediction,Clinical Decision Support ML Pipeline,Web Service,"The repository contains a FastAPI application (app.py) deployed as a web service using Docker and ECS. The infrastructure provisions ECS for hosting the FastAPI service, and the Dockerfile shows the app is containerized for API deployment. While there are orchestration components (pipeline.py with Prefect), the primary deployment mechanism is the web service for model inference.",AWS
https://github.com/Mahdi-Moalla/isic-skin-cancer-classification,ISIC Skin Cancer Classification Pipeline,"Batch, Web Service","The repository contains Airflow DAGs for batch processing (data download, preprocessing, training) and a web service component for model inference (inference-webservice, webserver Docker image). The batch component is evident from airflow_dags/monitoring_dag.py and training_pipeline DAGs, while the web service is shown by inference_webserver deployment and webserver Docker image.",Other
https://github.com/arsonor/music-genre-classification-with-deep-learning,Automated Model Training Workflow,"Batch, Web Service","The project includes a Flask API with Nginx for serving ML model predictions (Web Service), and uses Prefect for orchestrating training pipelines and workflows (Batch). The docker-compose shows both API services and Prefect workers for batch processing.",Other
https://github.com/norahosny66/Predictive-Maintenance-MLOps/,Rotating Machinery Health Monitoring,"Batch, Web Service","The repository contains a Prefect workflow orchestrator for batch training pipelines (src/prefect_workflow.py) and a FastAPI service for model serving (src/api.py). The Makefile shows both 'train' (Prefect deployment) and 'deploy' (FastAPI) targets, indicating both batch processing and web service deployment.",AWS
https://github.com/AndhikaWB/pawpularity-contest-dtc,Pet Adoption Optimization System,"Batch, Web Service",The project uses Prefect for workflow orchestration (batch processing) and includes FastAPI and Streamlit servers for serving ML models and web interfaces (web service). The Makefile shows both workflow deployment and server run commands.,Other
https://github.com/Dcwind/customer-churn-mlops,Customer Churn Prediction Pipeline,"Batch, Web Service","The repository contains a Prefect 2 workflow orchestration system (orchestration/flows.py) that runs scheduled training pipelines, which is a batch processing pattern. It also includes a FastAPI inference service (deployment/app.py) that serves ML models via REST API, which is a web service deployment pattern. The Makefile shows both orchestrate and serve commands, confirming both batch and web service components.",AWS
https://github.com/capac/higher-education-students-performance-evaluation,Student Performance Prediction Pipeline,Web Service,"The repository contains Flask deployment code (predict.py, Dockerfile for Flask, Flask app structure) and model serving artifacts. The README shows testing a Flask web service for predictions. No streaming components (Kafka, Flink) or batch orchestrators (Airflow, Prefect workflows) are used for deployment - Prefect appears to be for orchestration/tracking, not batch ETL.",Other
https://github.com/bhavaniravi/emoji-suggester-mlops-zoomcamp-2025,MLflow Emoji Model Pipeline,"Batch, Web Service","The project uses Prefect for workflow orchestration to train models (batch processing) and Streamlit for serving predictions via a web interface. The Prefect pipeline handles data preprocessing and model training, while the Streamlit app provides a web service for making predictions.",Other
https://github.com/Danselem/weather_health,Predictive Health Weather Pipeline,"Batch, Web Service","The repository contains both a Prefect workflow orchestrator (prefect.yaml, src/train.py) for batch ML model training and a FastAPI-based web service (Dockerfile, app/main.py) for serving predictions. The Makefile shows commands for both pipeline execution (pipeline, train) and serving (serve, serve_local).",AWS
https://github.com/behrouzm/Medical_Insurance_mlops,Demographic Health Data Insights,"Batch, Web Service","The repository contains both batch processing components (Prefect workflow orchestrator for ETL jobs, Airflow support, scheduled training pipelines) and web service components (FastAPI deployment for serving ML models, Docker containerization for API service). The test_prefect_workflow.py shows batch ETL orchestration, while the FastAPI deployment and Docker configuration indicate a web service for model serving.",Other
https://github.com/clementlwm94/dental-caries-mlops,Google Cloud ML Pipeline,"Batch, Web Service","The project contains both batch and web service components. For batch: Airflow DAGs (local-airflow/dags/) orchestrate ML pipeline jobs including data processing, model training, and evaluation. For web service: Flask-based prediction service (deploy_service/service_test.py, predict_function.py) exposes ML model via REST API on port 9696, deployed on Google Cloud Run.",GCP
https://github.com/mohammadimathstar/m5_forecasting/tree/main,Retail Sales Forecasting Pipeline,Batch,"The repository contains a production pipeline for daily batch inference and monitoring using Prefect orchestration, with scheduled daily data ingestion and prediction jobs. The deployment uses workflow orchestrators (Prefect) to run scheduled ETL jobs for batch forecasting, not real-time streaming or web service APIs.",AWS
https://github.com/coo1y/mlops_condo_rent_price_prediction,Condo Rental Market Analyzer,"Batch, Web Service","The project contains a Prefect model training pipeline (Batch) for training ML models and an AWS Lambda function (Web Service) for serving predictions via API. The Makefile shows both training pipeline and local inference pipeline, indicating batch training and web service deployment.",AWS
https://github.com/Danselem/brain_mri,Brain Tumor MRI Classification,Web Service,"The project serves ML models via API endpoints using FastAPI (Dockerfile, main.py) and includes serve commands (serve, runc). It also has batch components (Prefect pipeline, MLflow tracking) but primary deployment is web service for model inference.",AWS
https://github.com/f-kuzey-edes-huyal/steam-sale-optimizer,Game Sales Optimization Platform,"Batch, Web Service","The repository contains an Apache Airflow DAG (scripts/steam_sale_optimizer_dag.py) that orchestrates scheduled ETL jobs including data scraping, cleaning, model training, and deployment, which is characteristic of Batch processing. It also includes a FastAPI service (main.py) that serves the trained ML model via REST API endpoints, indicating Web Service deployment.",Azure
https://github.com/owhonda-moses/iot-sensor-anomaly-mlops,Anomaly Detection MLOps Pipeline,"Batch, Web Service","The repository contains both batch and web service components. The batch component is evident from the Prefect workflows (iot_training_pipeline, model_monitoring_flow) that orchestrate scheduled ML training and monitoring jobs. The web service component is shown by the Flask API (iot_anomaly.predict:app) deployed via Cloud Run for real-time anomaly detection predictions, along with Docker configuration for serving the model.",GCP
https://github.com/oleitao/mlops-project-zoomcamp,Telecom Churn Prediction System,Web Service,"The repository contains a Flask-based prediction service (churn_prediction_service) that exposes an API endpoint for making predictions. The docker-compose.yml shows services like 'churn_prediction_service' running on port 9696, which is a web service pattern. There are no workflow orchestrators, message brokers, or streaming components present.",Other
https://github.com/alex-sokolov2011/delivery-time-prediction-mlops,Delivery Time Prediction Pipeline,"Batch, Web Service","The project includes batch processing components (Prefect flows for data preparation, MLflow for model training/tracking) and web service components (FastAPI for serving predictions, Grafana dashboards for monitoring). The Docker Compose setup shows both batch-oriented services (MLflow, MinIO, Postgres) and web services (FastAPI, Grafana, Jupyter).",Other
https://github.com/Foluwa/youtube_engagement_predictor,YouTube Engagement Prediction System,"Batch, Web Service","The project has both batch and web service components. The batch component is evidenced by Airflow DAGs (retrain_dag.py, monitor_dag.py) that run scheduled model training and monitoring jobs. The web service component is evidenced by FastAPI REST API endpoints for real-time predictions and a Streamlit web UI for interactive predictions.",Other
https://github.com/ju-arroyom/mlops-zoomcamp-project-2025,Coronary Artery Disease Prediction Pipeline,"Batch, Web Service",The project uses Prefect for workflow orchestration (training pipeline) and FastAPI/Streamlit for serving ML models and dashboards. Docker Compose orchestrates both batch training jobs and web services.,Other
https://github.com/venukarnati92/rental-prediction,Rental Price Prediction Pipeline,"Batch, Web Service","The project includes Prefect workflows for batch ML pipeline orchestration (training, validation, deployment) and AWS Lambda functions serving real-time predictions via API Gateway, plus a Prefect server for workflow management.",AWS
https://github.com/iravkr/mlops,Audio Sentiment Analysis Pipeline,"Batch, Web Service",The project includes both batch processing (Prefect pipeline for training/monitoring) and web service components (FastAPI for model serving),AWS
https://github.com/mmotl/dtc_persona_analysis,DTC Customer Persona Analysis,"Batch, Web Service",The project uses Mage for batch data pipeline orchestration (ETL jobs) and Gunicorn/Flask for serving ML model predictions via API (web service). The Docker Compose files show both Mage pipeline and MLflow server components.,GCP
https://github.com/KevinEsh/mercanova,AI-Powered Retail Operations Suite,Batch,"The repository uses Apache Airflow as a workflow orchestrator to run scheduled ETL jobs and data pipelines. The code includes DAGs (dags.py), Docker Compose for Airflow deployment, and Metaflow for ML pipeline orchestration, all characteristic of batch processing workflows.",Other
https://github.com/YannPhamVan/MLOps-ETF-PEA,ETF Performance Prediction Pipeline,"Batch, Web Service","The project includes a Prefect flow (etf_pipeline) that orchestrates scheduled ETL jobs (feature engineering, dataset rebuild, model training, prediction), which is characteristic of batch processing. Additionally, it serves ML model predictions via a FastAPI endpoint (src/api/main.py), indicating a web service deployment for model inference.",AWS
https://github.com/Nehaharish98/wine-quality-prediction-mlops.git,Wine Quality Prediction Pipeline,"Batch, Web Service",The project includes both batch and web service components. The batch component is evident from the Prefect workflow orchestrator with scheduled training pipelines (prefect.yaml shows daily training at 02:00 UTC). The web service component is evident from the FastAPI model serving setup (Dockerfile shows uvicorn command for API serving on port 8000) and Streamlit UI for interactive inference.,Other
https://github.com/VRabinin/mlops-zoomcamp-project,CRM Sales Opportunity Prediction Platform,"Batch, Web Service","The project implements a batch data pipeline using Prefect 3.x for ETL workflows (data acquisition, ingestion, training, monitoring) with scheduled deployments, and a web service component using Streamlit for model serving and monitoring dashboards. The docker-compose.yml shows both Prefect server for orchestration and Streamlit application components.",Other
https://github.com/onisj/youtube-comment-intelligence,ML-Powered Comment Analysis Pipeline,Web Service,"The repository contains a Flask API (app.py) and Streamlit UI (streamlit_app.py) served via Docker containers, with Docker Compose orchestrating web services on ports 8080 and 8501. The architecture is designed for serving ML models and results via API endpoints and interactive web interfaces, not for batch processing or streaming data flows.",Other
https://github.com/KevinKBui/Prostate-Cancer-Risk-Assessment,Prostate Cancer Risk Assessment,Web Service,"The repository implements a Flask web service (web_service.py) that serves ML model predictions via REST API endpoints (/predict, /predict_api, /health). The Dockerfile and docker-compose.yml show this is deployed as a containerized web service on port 9696. While it includes monitoring components and Prefect orchestration, the primary deployment pattern is a web service for model serving.",Other
https://github.com/hnkovr/MyMLOps2025,Unknown,Unknown,No files fetched,Unknown
https://github.com/tman0004/mlops-zoomcamp-project,Titanic Survival Prediction Pipeline,Web Service,"The project serves ML predictions via a Streamlit web application (prediction_app.py) and uses Prefect for workflow orchestration. Streamlit is a web service framework for ML inference, not a streaming system. The code shows no Kafka/Kinesis/real-time processing components.",AWS
https://github.com/Dkaattae/Monthly-Stock-Return-Prediction,Monthly Stock Return Prediction,"Batch, Web Service","The repository contains both batch processing and web service components. The model_training folder uses Prefect for orchestrating scheduled ETL jobs (download data, transform data, preprocess data, hyperopt training, register model), which is characteristic of batch processing. The model_prediction folder contains a Flask API wrapped with Gunicorn for serving ML model predictions via HTTP endpoints, which is a web service deployment pattern.",AWS
https://github.com/ssmangilev/mlops-zoomcamp-project,Real-Time Fraud Detection Pipeline,"Batch, Streaming","The project contains both batch and streaming components. The orchestration module uses Apache Airflow (a workflow orchestrator) for batch ETL jobs, while the kafka module includes a consumer that processes messages from Kafka topics in real-time, indicating streaming capabilities. The project also includes infrastructure for both batch processing (Airflow) and streaming (Kafka UI).",AWS
https://github.com/naivebird/vancouver-property-tax-prediction,Vancouver Property Tax Prediction,"Batch, Web Service","The project uses Prefect for workflow orchestration to train models (Batch), and FastAPI to serve predictions via a /predict endpoint (Web Service). The README explicitly states ""A FastAPI-based prediction service... serves predictions through the /predict endpoint"" and shows Prefect deployment configuration for training pipelines.",AWS
https://github.com/niting9881/mlops-zoomcamp-project,House Price Prediction Pipeline,Web Service,"The repository contains a FastAPI service (src/api/Dockerfile, docker-compose.yaml) that serves ML models via API endpoints, and a Streamlit application for visualization. Both are web services for ML inference and user interaction, not batch processing or streaming.",Other
https://github.com/DarioDang/Chicago-Taxi-MLOPS,Chicago Taxi Duration Prediction,"Batch, Web Service","The repository contains both batch and web service components. The batch component is evident from the workflow-orchestration folder using Mage for pipeline orchestration, and the web service component is evident from the aws-model-deployment folder containing a Flask API for serving ML predictions via REST endpoints.",AWS
https://github.com/mircohoehne/e2e-taxi-ride-duration-prediction,NYC Taxi Duration Prediction,Web Service,"The repository contains a FastAPI application with a '/predict' endpoint for serving ML predictions, containerized using Docker. The serving layer is the primary deployment method, with CI/CD pipeline publishing the containerized API to GitHub Container Registry. While Prefect is used for orchestration, it's for the training pipeline rather than the serving layer.",AWS
https://github.com/SameulAH/MLops/tree/main/006_Capstone%20Project,ML Pipeline Orchestration,Batch,"The repository contains a single Python script (main.py) that implements a data processing pipeline using the Pandas library. The script reads data from a CSV file, performs data cleaning and feature engineering, trains a machine learning model, and saves the model and metrics. This is a typical batch processing workflow that runs on a scheduled or manual basis, not a continuously running service or streaming system.",Unknown
https://github.com/katjaweb/MLOps-Fake-News-Detection,Fake News Detection Service,Web Service,"The repository contains a Flask-based web service for fake news detection with Docker containers exposing port 9696, using gunicorn to serve the predict.py application. The Dockerfile and docker-compose files show this is deployed as a web service API, not batch processing or streaming.",AWS
https://github.com/Jayadeep19/Mlops_final_project,Iron Ore Quality Prediction Pipeline,"Batch, Web Service","The project uses Prefect for workflow orchestration (batch processing) as shown in prefect.yaml and orchestrate.py, and also includes a web service deployment via Docker container running on port 9696 with Flask/FastAPI for model serving as shown in web_service/Dockerfile and predict.py.",Other
https://github.com/Marcoc51/Match-Video-Detection,Sports Video Analysis System,"Batch, Web Service","The repository contains a Mage.ai workflow orchestrator (indicated by mage-ai dependency in requirements.txt) for batch processing of video analysis, and also includes FastAPI-based API server components (fastapi, uvicorn, pydantic dependencies) for serving ML models and results via web service. The system processes football match videos in batch mode while providing API endpoints for real-time video processing.",Other
https://github.com/galchenm/pet_project_mlops,Stroke Prediction MLOps Pipeline,Web Service,"The project serves an ML model via a FastAPI application (src/serve_model.py) exposed on port 8000, with Docker support for containerized deployment. The Makefile includes 'serve' and 'docker-run' targets, and the Dockerfile runs uvicorn to serve the API.",Other
https://github.com/Dung8229/stock_forecast.git,Stock Price Forecasting Pipeline,Batch,"The repository uses Prefect 2.0 for workflow orchestration with scheduled flows (daily forecasting via cron ""0 7 * * *""), Makefile targets for batch operations (prepare, train, register, inference), and no streaming components or web service APIs for real-time inference.",AWS
https://github.com/MonaHamid/Toxic-Comment-Classifier-MLOps-Project-/tree/main,Toxic Comment Classification Pipeline,Web Service,"The repository contains a FastAPI application (main.py) that serves a toxic comment classification model via REST API endpoints. The code shows a /predict endpoint that accepts text input and returns classification results. The project uses Docker for containerization and includes a Dockerfile for deployment. While there are some orchestration files (Makefile, .prefectignore) and monitoring components (Grafana, Postgres), the core deployment is a web service API for model inference.",Unknown
https://github.com/thobs-10/MLOps-hotel-reservation-prediction-system,Machine Learning Hotel Analytics,Web Service,"The project serves ML predictions via a FastAPI endpoint (src.app:app) exposed on port 8000, with Docker containerization for deployment. The FastAPI service provides real-time prediction capabilities for hotel reservation cancellations.",Other
https://github.com/3d150n-marc3l0/mlops-zoomcamp-2025-capstone-citibike,Bike Availability Optimization System,"Batch, Web Service","The repository contains a batch training pipeline (training.py) and a deployment pipeline (deploy-bentoml.py) that uses BentoML to serve the model via REST API. The Makefile shows both training and deployment targets, indicating both batch processing and web service components.",Other
https://github.com/Shayanix/MLops-Brain-Eye-Detector,Brainwave Eye Monitoring,"Batch, Web Service","The project includes a FastAPI web service for serving ML predictions (src/predict/app.py) and uses Prefect for workflow orchestration with scheduled ML pipeline runs (src/workflows/prefect.yaml). The FastAPI serves ML model inference via REST API, while Prefect handles batch training and deployment workflows.",Other
https://github.com/ileardo/solar-forecasting-mlops,Solar Power Forecasting Pipeline,Batch,"The repository implements a batch prediction system using Prefect for workflow orchestration, with scheduled ETL jobs for solar power forecasting. The system processes historical data in batches to generate 24-hour ahead predictions, as evidenced by the Prefect flows, scheduled training/prediction scripts, and batch-oriented architecture.",Other
https://github.com/facug91/mlops-zoomcamp-final-project,Fruit Image Classification Pipeline,Web Service,"The project serves a fruit image classification model via a Flask-based REST API with a /predict endpoint that accepts uploaded images. The docker-compose shows the service running as a web service on port 8080, and the README explicitly states ""Model serving through a Flask-based API with a /predict endpoint that accepts uploaded images.""",Other
https://github.com/nicoalpis/heart-failure-prediction,Heart Failure Prediction Pipeline,Web Service,"The project serves an ML model via a FastAPI REST API (predict.py) and is containerized with Docker. The docker-compose.yml defines a 'fastapi-app' service exposing port 8000, which is a classic web service deployment pattern for ML inference.",Other
https://github.com/msilaev/air-pollution,Air Pollution ML Model,"Batch, Web Service","The project uses Prefect for batch orchestration of data collection and model training workflows, and provides both a FastAPI REST API and Streamlit dashboard for serving predictions and visualizations.",Other
https://github.com/olufemig/mlops-project,Synthetic Salary Prediction Pipeline,"Batch, Web Service","The repository contains ZenML pipelines for data preprocessing, model training, and evaluation (batch processing), along with FastAPI/Gradio applications for serving model predictions (web service). The presence of Airflow DAGs indicates batch orchestration, while the app/requirements.txt and gradio dependencies confirm web service deployment for model inference.",Other
https://github.com/renzoclaure/imdb-sentiment-mlops.git,IMDb Sentiment Analysis Pipeline,"Batch, Web Service","The project includes both batch processing components (Airflow DAGs for data ingestion, preprocessing, training, and monitoring report generation) and web service components (FastAPI model serving deployed on Cloud Run with Docker)",GCP
https://github.com/BinksWANG/ML_Mental_Health_Project,Mental Health ML Pipeline,"Batch, Web Service","The repository contains both batch processing components (Mage workflow orchestration for ETL jobs) and web service components (Flask API with Docker deployment for model serving). The batch component is evidenced by the Mage orchestration setup, while the web service is shown through Flask predict.py files and Dockerfiles exposing port 9696 for API serving.",GCP
https://github.com/kostas696/women-in-stem-mlops-project,Educational Equity Analytics Pipeline,"Batch, Web Service","The project includes both batch processing (Airflow DAGs for end-to-end ML pipeline orchestration) and web service components (FastAPI app for batch inference served via API). The Airflow DAG orchestrates data preprocessing, model training, and evaluation as scheduled batch jobs, while the FastAPI service provides a web API for model inference.",GCP
https://github.com/mayzt99/crop-recommendation-mlops-zoomcamp,Crop Recommendation System,"Batch, Web Service","The project implements a web service via Flask API (predict.py) on port 9696 for real-time crop recommendations, and batch processing via Prefect workflow (prefect_training_pipeline.py) for scheduled model training and MLflow experiment tracking.",Other
https://github.com/krish-rm/market-master-trading-prediction,Unknown,Unknown,No files fetched,Unknown
https://github.com/renelarsson/taxi-duration-prediction,Ride Duration MLOps Monitoring,Streaming,"The project includes Kinesis streams (stg_taxi_predictions, stg_taxi_trip_events) and AWS Lambda functions for streaming inference, with Docker images built for Lambda handlers that process streaming data.",AWS
https://github.com/tsila-andriantsoa/insurance_premium_prediction.git,Medical Insurance ML Pipeline,"Batch, Web Service","The project includes both batch processing components (Prefect workflow orchestrator for data preparation, training, and prediction pipelines) and web service components (Flask API for serving predictions, Docker containerization for deployment). The Makefile shows commands for both batch operations (make train, make predict) and web service deployment (make docker-run).",AWS
https://github.com/skayikci/tweet-classfication-mlops,Sentiment Classification MLOps System,"Batch, Web Service","The project includes a Prefect workflow (prefect_flow.py) that orchestrates batch ML pipeline tasks (training, model comparison, registration), and a FastAPI service (api.py) that serves the trained model for real-time tweet classification predictions. The Docker Compose setup runs both the API and the orchestration pipeline.",Other
https://github.com/Ramsi-K/mlops-text-summariser,Text Summarization Pipeline,"Batch, Web Service","The repository contains a batch ML pipeline with stages (ingest, transform, train, evaluate) orchestrated via Makefile commands and main.py, plus a FastAPI web service for model serving (app.py, uvicorn server, Docker health checks).",AWS
https://github.com/mk-hassan/EPL-predictions-mlops,Premier League Data Pipeline,"Batch, Web Service","The repository contains Prefect pipelines for batch data ingestion and training workflows, plus FastAPI/uvicorn for serving ML models as a web service. Streamlit is used for visualization but is not streaming.",AWS
https://github.com/SapientSapiens/capstoneproject-2025-mlopsz/tree/main,Bike Sharing Demand Prediction,Web Service,"The repository contains FastAPI and Streamlit applications deployed via Docker containers, serving ML model predictions through REST APIs and interactive web dashboards. The docker-compose.yaml shows FastAPI on port 8010 and Streamlit on port 8501, with FastAPI depending on model serving infrastructure.",AWS
https://github.com/krish-rm/market-master-trading-action-prediction,Market Value Weighted Predictions,"Batch, Web Service","The repository contains both batch processing components (Prefect workflows for ETL/orchestration, pipeline scripts) and web service components (FastAPI for model serving, Streamlit dashboard). The Dockerfile shows FastAPI serving models, while Makefile and flows/ directory show batch orchestration via Prefect.",Other
https://github.com/JDede1/loan_default_prediction,Model Monitoring and Orchestration,"Batch, Web Service","The repository contains Airflow DAGs for batch ETL/orchestration workflows (src/airflow_dag.py) and a model serving component (Dockerfile.serve, entrypoint.sh) that exposes ML predictions via API on port 5001. Streamlit/Gradio dashboards are present for visualization but are not considered streaming.",GCP
https://github.com/razamehar/telco-customer-churn-prediction,Telco Customer Churn Prediction,Web Service,The repository contains a FastAPI application (app.py) that serves ML model predictions via REST endpoints (/predict). The Dockerfile and Makefile show it's containerized and deployed as a web service. No streaming or batch orchestration components are present.,Other
https://github.com/sanchis135/MLOps/tree/main/Project_flight-delay-prediction,Domestic Flight Data Analytics,Web Service,"The repository contains a FastAPI application (main.py) with Docker configuration for serving ML model predictions via API endpoints. The code shows a /predict endpoint that accepts flight data and returns delay predictions, which is characteristic of a web service deployment pattern.",AWS
https://github.com/mohamedhamd32/2025_mlops_project_heart_attack,Patient Data Drift Detection Pipeline,"Batch, Web Service","The project serves ML models via a Flask API (Web Service) exposed on port 8000, and uses Prefect for workflow orchestration including model training and monitoring (Batch). The docker-compose includes both an app service and a Prefect orchestrator.",Other
https://github.com/JuChunHuang/biopay,Biotech Compensation Prediction Pipeline,Batch,"The repository contains Airflow DAGs for workflow orchestration, Docker-based containerization, and a clear ETL pipeline structure with data preprocessing, model training, and prediction steps. The project is designed to run scheduled batch jobs rather than serving real-time APIs or streaming data.",AWS
https://github.com/Gustavo-HA/loan_prediction,Loan Approval Prediction Pipeline,Streaming,"The repository implements a streaming ML inference pipeline using AWS Kinesis for real-time data processing. Key evidence includes: 1) Terraform infrastructure provisioning Kinesis streams (source_kinesis_stream and output_kinesis_stream), 2) Docker container deployment with Kinesis endpoint configuration, 3) Lambda function consuming from Kinesis streams, 4) Integration tests using LocalStack with Kinesis service, and 5) Makefile targets for AWS services setup. While there are some batch components (Prefect workflow orchestration), the primary deployment pattern is streaming inference.",AWS
https://github.com/buzdugan/mlops_zoomcamp,Insurance Claims Risk Pipeline,Batch,"The repository uses Prefect for workflow orchestration with scheduled batch jobs for training, scoring, and monitoring. The prefect.yaml defines multiple deployments with cron schedules (e.g., ""00 06 2 * *"" for monthly training, ""00 06 * * *"" for daily scoring). The code structure shows batch processing patterns with periodic data pulls and transformations rather than real-time streaming or web service APIs.",AWS
https://github.com/gabriellailena/classical-composer-prediction/tree/main,Classical Composer Audio Classifier,"Batch, Web Service","The repository contains both batch and web service components. The batch component is implemented using Mage AI for ETL/ML pipeline orchestration (Dockerfile.mage, Mage project structure). The web service component is a Flask API for serving ML predictions (Dockerfile.api, app.py, gunicorn deployment). The Makefile shows both services are run together via docker-compose.",GCP
https://github.com/mironenkoasja/ny_subway_ridership_prediction,Subway Ridership Prediction System,Batch,"The repository contains an Apache Airflow DAG (turnstile_ml_pipeline.py) that orchestrates a batch ML pipeline for subway ridership prediction. The pipeline includes scheduled tasks for data download, preprocessing, model training, and inference using SequentialExecutor. Docker Compose sets up Airflow webserver and scheduler services for batch job execution.",Other
https://github.com/bonisadar/dhakacity-precipitation-forecast-mlops25,Dhaka City Precipitation Forecast,"Batch, Web Service","The project uses Prefect 2.x for batch orchestration (scheduled flows for data fetching, training, and drift detection) and includes a FastAPI-based web service deployment (Dockerfile with uvicorn) for serving the model. The infrastructure supports both batch processing and web service endpoints.",GCP
https://github.com/nathadriele/mlops-zoomcamp-project-paris-price-house,Paris Housing Price Predictor,"Batch, Web Service","The project includes a Flask API for serving ML model predictions (Web Service) and a Prefect flow for scheduled model training and deployment (Batch). The Prefect flow orchestrates periodic model training, while the API provides real-time prediction endpoints.",Other
https://github.com/freillat/MLOps_CCproject,Credit Card Default Prediction Pipeline,Batch,"The project uses Prefect to orchestrate training and batch prediction pipelines. The README explicitly states ""Deployment (Batch)"" and describes a batch prediction job managed by Prefect Flow that runs on a schedule or triggered by an event. The Dockerfile CMD runs ""flow.py"" which is a Prefect workflow for batch processing.",AWS
https://github.com/habeeb3579/mlops-production-v2,Unknown,Unknown,No files fetched,Unknown
https://github.com/anhnd16/nda-mlops-zoomcamp-project,Unknown,Unknown,No files fetched,Unknown
https://github.com/Rita-cyber/Predicting-UK-Train-Delays-in-Real-Time-and-Classifying-Route-Risk,UK Train Delay Prediction,"Batch, Streaming",The project uses both streaming and batch components. The Dockerfile and producer code show a real-time streaming pipeline using AWS Kinesis to ingest Darwin Push Port XML data continuously. The Airflow orchestration and Glue jobs handle batch processing of historical data stored in S3 for model training and ETL workflows.,AWS
https://github.com/alperugurca/MLOps_2025_project1,Unknown,Unknown,No files fetched,Unknown
https://github.com/glk08909/mlops-hr-attrition,HR Data ML Deployment Pipeline,Web Service,"The repository contains a FastAPI service (src.inference_service) for serving ML model predictions, Docker configuration for containerizing the API, and a Makefile for managing the containerized web service. The project is structured as an MLOps pipeline with model deployment via a web service endpoint.",Other
https://github.com/Johnyckot/mlops_prj_code_quality,NASA Metrics ML Pipeline,"Batch, Web Service","The repository contains a MLOps pipeline that trains models via batch workflows (Databricks workflows) and deploys models as both a Web-Service and a Spark-UDF for batch loads. The README explicitly states the model should be accessible as a Web-Service for downstream consumers and as a Spark-UDF for batch loads, indicating both deployment types.",Other
https://github.com/MuhammadQasim111/DIABETES_ML_OPS,Diabetes Prediction MLOps Pipeline,"Batch, Web Service","The project includes a Flask-based web service for model deployment (predict_local.py, predict_remote.py) and uses Prefect for workflow orchestration (model_training.py), indicating both web service and batch components.",AWS
https://github.com/HaChan/retail-forecast-mlops,Machine Learning Forecasting for Retail,"Batch, Web Service","The repository contains a Temporal workflow orchestrator (pipeline_workflow.py, worker.py) that runs scheduled/triggered ML pipeline jobs (data processing, training, evaluation, deployment), which is characteristic of batch processing. It also includes a FastAPI service (model_service.py, Dockerfile.api) that serves ML model predictions via REST API, which is a web service deployment.",Other
https://github.com/krutto89/mlops-diamond-pricing,Diamond Price Prediction System,"Batch, Web Service","The repository contains both batch processing and web service components. The Airflow DAGs (batch_prediction.py, training_pipeline.py) orchestrate scheduled batch jobs for data ingestion, model training, and batch predictions. The Flask web application serves real-time predictions through a user interface, as evidenced by the Dockerfile.flask and the README description of the Flask app serving predictions.",AWS
https://github.com/Gujeah/Risk-Flag-Prediction,Credit Risk Prediction Pipeline,Web Service,"The project serves ML models via a Flask API (flask_api/main.py) deployed with Gunicorn, exposing endpoints for predictions. This is a classic Web Service deployment pattern for ML inference.",AWS
https://github.com/fabianjkrueger/gdelt-newsimpact/tree/feature/prefect-orchestration,GDELT Model Deployment Pipeline,"Batch, Web Service","The repository contains both batch processing and web service components. For batch processing, it uses Prefect workflow orchestrator with deployment scripts (prefect/deploy.py) and Docker containers for scheduled ETL jobs. For web service, it serves ML models via Flask API (serve_model.py) exposed on port 5002, with a dedicated deploy_model Docker container.",GCP
https://github.com/adeakinwe/MLOps_credit_default_risk_prediction.git,Real-Time Default Prediction Pipeline,"Streaming, Web Service","The repository contains both a Flask-based web service deployment (04-model-deployment/web_service) with Dockerfile and API endpoint, and a streaming deployment (04-model-deployment/streaming) using AWS Lambda and Kinesis for real-time data processing.",AWS
https://github.com/brem-21/mlops_zoomcamp,Loan Eligibility Prediction Pipeline,"Batch, Web Service","The repository contains an Airflow DAG (loan_eligibility_mlops_pipeline.py) that orchestrates batch data processing and model training tasks, and also includes a Flask-based REST API for model serving (serving/requirements.txt indicates Flask usage).",AWS
https://github.com/sntk-76/AI-weather-predictor,Weather Data Collection Pipeline,Web Service,"The project uses Streamlit as a web application framework to serve ML model predictions via a web interface. The architecture shows user input flowing through a Streamlit web app to model inference and back to the user, indicating a web service deployment pattern. There are no workflow orchestrators, message brokers, or streaming components present.",GCP
https://github.com/gsenseless/mlOps_bikesharing,Bike Sharing Demand Prediction,"Batch, Web Service","The repository contains both batch and web service components. The batch component is orchestrated by Apache Airflow (DAGs in dags/ directory) that runs scheduled ML training jobs, processes data, and registers models in MLflow. The web service component is a prediction API built with Flask/FastAPI (api/Dockerfile, /predict endpoint) that serves the trained model for real-time inference.",AWS
https://github.com/dmitrievdeveloper/mlops_project-california-housing,Unknown,Unknown,No files fetched,Unknown
https://github.com/dedenuola/airoute_mlops,Air Quality Data Pipeline,"Batch, Web Service","The repository contains Airflow DAGs for scheduled batch processing (ETL pipelines for data ingestion and monitoring) and a FastAPI service for serving ML predictions via HTTP API. The batch component is evidenced by Airflow DAGs in the dags/ directory, while the web service component is shown by FastAPI implementation for model serving.",AWS
https://github.com/starlord-31/Garbage-classification-mlops/tree/main,Garbage Classification System,"Batch, Web Service","The repository contains a FastAPI-based web service (app.py) for serving ML models via REST endpoints, and a Prefect workflow (prefect_pipeline.py) for batch processing including data preparation, model training, and evaluation. The Dockerfile and Makefile further support the web service deployment, while the Prefect pipeline orchestrates batch ML operations.",Other
https://github.com/hravat/alphafoldmlops,Chemical Compound ML Pipeline,"Batch, Streaming","The repository contains both batch and streaming components. Batch: Uses Mage and Prefect for workflow orchestration with scheduled ETL jobs, MLflow for model tracking, and PostgreSQL for data storage. Streaming: Uses Kafka with producers/consumers for real-time data processing, with synthetic data generation and streaming prediction jobs.",Other
https://github.com/data-tomic/mlops-zoomcamp.git,Machine Learning Model Registry,"Batch, Web Service","The repository contains modules for batch scoring pipelines (using Mage AI/Prefect for orchestration) and real-time online inference services (using Docker, Flask/FastAPI for deployment). The README explicitly mentions both ""batch scoring and real-time online inference"" as deployment patterns covered.",Other
https://github.com/Chrmorod/Project_Final_Stock_Prices_Prediction,Stock Price Prediction Platform,Batch,"The project uses Mage as a workflow orchestration tool to automate and schedule ML pipelines (data extraction, model training, evaluation, export). The infrastructure includes scheduled ETL jobs and batch processing workflows, with no evidence of real-time streaming components or web service APIs for model serving.",Other
https://github.com/matthew-michal/apartment_rental_nj_2025,Real-Time Rental Price Monitoring,Batch,"The repository contains workflow orchestrators (Prefect flows) that run scheduled ETL jobs. The code shows daily and weekly scheduled tasks for data processing, model training, and predictions. Key indicators include Prefect flows for ""daily-apartment-predictions"" and ""weekly-model-training"", Lambda functions for scheduled execution, and Docker Compose for orchestrating batch services. There are no streaming components (Kafka, Kinesis) or web service APIs for ML inference.",AWS
https://github.com/carolinelile/healthcare_mlops_pipeline,Patient Readmission Risk Pipeline,Batch,"The repository uses Apache Airflow (Cloud Composer) with DAGs to orchestrate scheduled ETL jobs including data generation, ingestion, transformation, model training, and batch predictions. The pipeline follows a batch processing pattern with weekly scheduling and periodic data movement through GCS, BigQuery, and Vertex AI.",GCP
https://github.com/Pselen/Stroke-Prediction-MLOps,Healthcare Risk Monitoring Pipeline,"Batch, Web Service","The project includes a FastAPI service for serving ML model predictions (Web Service) and uses Prefect for orchestrating daily drift monitoring workflows (Batch). The FastAPI app serves model inference endpoints, while Prefect schedules and executes the daily drift check pipeline.",Other
https://github.com/conrad-ch1/edge-iiot.git,Unknown,Unknown,No files fetched,Unknown
https://github.com/AreebAhmad-02/mlops-zoomcamp-project,Model Monitoring and Logging Service,Web Service,"The repository contains a Flask-based prediction service (app.py) with a Dockerfile and docker-compose.yaml for containerized deployment. The service exposes an API endpoint on port 5555 using Gunicorn as the WSGI server. The code includes model loading, preprocessing, and prediction logic designed for serving ML models via HTTP requests.",Other
https://github.com/fonsecagabriella/passcompass/tree/main,Student Risk Prediction Pipeline,"Batch, Web Service","The repository contains both batch processing components (Prefect flows for data extraction, training, and promotion) and a web service component (Flask/FastAPI application for serving ML model predictions). The batch flows handle ETL and model training, while the webapp serves the deployed model via API endpoints.",Other
https://github.com/Facco-Bruno/Bluebikes-trip-mlops,Unknown,Unknown,No files fetched,Unknown
https://github.com/lauraalexandria/ecommerce_demand_forecast,Time Series Sales Modeling,Web Service,"The project uses FastAPI for serving ML models via API endpoints (scr/api_csv.py) and includes Docker containerization for deployment. The Dockerfile and docker-compose.yaml show a web service architecture with FastAPI exposed on port 8000, along with MLflow tracking server. There are no streaming components (Kafka, Flink) or batch workflow orchestrators (Airflow, Prefect) present.",Other
https://github.com/rshiva/datatalks-mlops-2025/tree/main/07-project-01,Introvert-Extrovert Prediction Engine,Batch,"The repository contains batch processing pipelines for training and prediction, orchestrated by Mage. The Dockerfile and docker-compose setup are configured to run batch prediction scripts, and the Makefile targets (train, predict) indicate scheduled/batch execution rather than real-time streaming or web service deployment.",AWS
https://github.com/selsinan/review-rating-prediction/,Semantic Book Analysis Pipeline,Web Service,"The repository contains a FastAPI application (src/api/main.py) with Docker containerization for serving ML model predictions via REST API endpoints. The Dockerfile exposes port 8000 and runs uvicorn to serve the FastAPI app, indicating a web service deployment pattern.",Unknown
https://github.com/parker-sy/mlops-zoomcamp-project-stress-level-prediction,Physiological Data Prediction Pipeline,Web Service,"The project includes a Flask app (app.py) and a Dockerfile for containerizing the model serving application. The Dockerfile copies the app.py file and runs it with pipenv, indicating this is a web service deployment for serving ML model predictions via API.",Unknown
https://github.com/mar1-k/fruit_image_classification,Fruit Image Classification Pipeline,Batch,"The repository contains an Apache Airflow DAG (ingest_extract_data.py) that orchestrates scheduled ETL workflows for data ingestion and model training. The architecture is built around batch processing with Airflow scheduling, MLflow experiment tracking, and MinIO artifact storage. No streaming components (Kafka, Flink) or web service APIs (Flask, FastAPI) are present for real-time inference.",GCP
https://github.com/calzateu/customer-churn-mlops,Customer Churn Prediction Pipeline,"Batch, Web Service","The project uses Prefect for orchestrating batch workflows (ETL, training, inference) and MLflow for serving models via web service. The architecture includes both scheduled/batch processing flows and model serving endpoints.",Other
https://github.com/mcherif/plant-disease-mlops,Gradio Plant Disease Web Interface,Web Service,"The repository contains a Dockerized application that serves a plant disease classification model via Gradio (src/app_gradio.py) and Streamlit (src/streamlit_app.py), with FastAPI components for inference. The Dockerfile exposes port 7860 and the main entry point is a Gradio app, indicating this is a web service deployment for ML model inference.",Other
https://github.com/shanurwan/retirement-readiness,Malaysia Retirement Decision Support,Batch,"The repository contains a Prefect workflow orchestration system (Prefect in requirements.txt, training_pipeline.py) that runs scheduled ML training jobs. The pipeline trains models periodically and stores them, but does not serve real-time API inference. Streamlit is present but only for visualization, not for ML inference. No streaming components like Kafka or real-time processing are found.",AWS
https://github.com/amorsi1/Embedded-spam-MLOps,Flask Spam Classifier Web App,Web Service,"The repository contains a Flask web application (serve_model.py) that serves ML model predictions via REST API endpoints (/predict, /health). The Docker containers are configured to expose port 4242 and run the Flask app, with a demo link showing a web interface for spam classification.",AWS
https://github.com/OluwajobaOluwabori/Retinamnist-ML0ps-Pipeline,Diabetic Retinopathy Prediction Pipeline,Web Service,"The repository contains a Flask API (app.py) for serving model predictions, Dockerfile for containerization, and Docker Compose for deployment. The main serving component is a web service that exposes a /predict endpoint for ML inference.",Other
https://github.com/SalimaMamma/mlops-zoomcamp-project/tree/main,Cryptocurrency Price Forecasting Platform,"Batch, Web Service","The repository contains Apache Airflow DAGs for scheduled batch processing (data ingestion, model training, drift monitoring) and a FastAPI service for real-time model serving. The Airflow DAGs handle batch workflows like hourly data preprocessing and model training, while the FastAPI application provides web service endpoints for predictions.",Other
https://github.com/Xue-Zhiming-Bruce/Netflix-Recommendation-Rating,Netflix Movie Recommendation Engine,Batch,"The repository implements a complete MLOps pipeline using Apache Airflow with DAGs (Directed Acyclic Graphs) for workflow orchestration. The code shows scheduled ETL jobs that process Netflix Prize data through bronze, silver, and gold layers in a batch processing pattern. The pipeline includes data extraction, transformation, feature engineering, model training, and monitoring tasks that run on a daily schedule (schedule_interval=timedelta(days=1)).",AWS
https://github.com/dakn2005/ASAL-Households-Classification-for-Social-Protection,Humanitarian Aid Classification Engine,"Batch, Web Service","The repository contains both MageAI (workflow orchestrator for batch ETL jobs) and a FastAPI web service for serving ML models. The Makefile shows commands for both ""Run MageAI"" (batch) and ""Go Live!"" which starts the web API (web service).",GCP
https://github.com/oktavianidewi/mlops-zc-2025,Diabetes Prediction MLOps Pipeline,"Batch, Web Service","The project uses Apache Airflow with DAGs for batch processing (ETL, model training, drift detection) and includes a Streamlit app for serving diabetes predictions via web interface.",Other
https://github.com/F-U-Njoku/solar-efficiency-forecast,Solar Panel Efficiency Forecast,"Batch, Web Service",The project uses Airflow DAGs for batch ML training workflows and deploys a Streamlit web service for model inference and interactive visualization.,AWS
https://github.com/omkar-thite/stroke-predictor,Stroke Risk Prediction Pipeline,Batch,"The repository implements a batch-oriented MLOps pipeline with Cloud Run Jobs for training and inference, orchestrated via Docker Compose and Terraform. The architecture uses scheduled or triggered jobs rather than continuous streaming or real-time web service deployment.",GCP
https://github.com/almondheng/healthcare-classification,Healthcare ML Classification System,"Batch, Web Service","The repository contains a workflow orchestrator (Prefect) for batch processing and a FastAPI server for serving ML model predictions via API. The Dockerfile and serve.py indicate web service deployment, while workflow.py uses Prefect for batch orchestration.",Other
https://github.com/AnefuIII/MLOps_spine_disease,Spine Disease Classification Pipeline,Web Service,The repository contains a Flask web service (predict.py) with Docker containerization for serving spine disease classification predictions via HTTP endpoints (/predict). The Dockerfile and deployment scripts confirm this is a web service deployment pattern.,Other
https://github.com/wingylui/MLOps_zoomcamp/tree/main/project,Perth Housing Price Predictor,"Batch, Web Service","The project serves ML predictions via a Flask API (Web Service) and uses Prefect for automated orchestration of training/retraining pipelines (Batch). The monitoring stack (Grafana, PostgreSQL) is for observability, not streaming.",AWS
https://github.com/calatre/mlops-pipeline,NYC Taxi Duration Prediction Pipeline,"Batch, Streaming","The repository contains both batch and streaming components. For batch: Airflow DAGs for scheduled ETL jobs, MLflow for model training/tracking, and data processing pipelines. For streaming: Kinesis Data Stream for real-time data streaming, Lambda function for containerized model inference, and frontend dashboard that submits events to Kinesis and monitors stream results.",AWS
https://github.com/conradorg/biological-age-regression,NHANES Health Data Pipeline,"Batch, Web Service","The repository contains Apache Airflow DAGs (bioage_dag.py) that orchestrate batch ML workflows including data preparation and hyperparameter tuning. It also includes model deployment components (Dockerfile.mlflow, MLflow server configuration) indicating a web service for serving the trained model via API.",Other
https://github.com/Optimistix/MLOPs_Pima_Indian_Diabetes_Analysis,Pima Diabetes Prediction Pipeline,"Batch, Web Service","The repository implements both batch and web service deployment types. It uses Prefect for batch workflow orchestration (scheduled ETL jobs and model training pipelines) and provides web services through multiple APIs: MLflow UI (port 5000), Grafana dashboard (port 3000), and a custom metrics API (port 8000). The docker-compose configuration shows these services running as web services, while the Prefect flows handle batch processing.",Other
https://github.com/abandonedmonk/MLOps-Zoomcamp-Project,Heart Disease Prediction Pipeline,"Batch, Web Service","The project includes a Prefect workflow (prefect_flow.py) that orchestrates batch training and registration tasks, and a FastAPI service (api/main.py) with Dockerfile for serving predictions via REST API.",Unknown
https://github.com/nd-serge/mobile-money-fraud-detection,African Mobile Money Fraud Detection,"Batch, Web Service","The project has two main components: (1) A training workflow using Prefect for batch processing (data preprocessing, model training, model registration), and (2) A microservice using FastAPI with a /predict endpoint for real-time fraud detection predictions.",Other
https://github.com/tsbalzhanov/MLOpsZoomcamp_LinguisticsStackExchange,Linguistics Forum Classification Pipeline,"Batch, Web Service","The project uses Prefect for orchestrating batch workflows (download, prepare, train) and MLServer for serving ML models via REST API (ports 8080-8082). The Streamlit/Gradio distinction is noted - this uses MLServer for inference.",Other
https://github.com/jhuff-genomics/mlops-project-llm-safety,Personal Health Data Detection,Web Service,"The repository contains a FastAPI application (ml-deploy/pyproject.toml) and Modal deployment configuration, indicating a web service API for serving ML models. The project description states it will provide a ""web service API"" that takes input and runs LLM genAI followed by ML judge assessment.",Other
https://github.com/JorgeSandon/MLOPS-ZoomCamp-Cohorts-2025/tree/main/Project/restaurant-sales-mlops,Restaurant Sales Prediction Pipeline,"Batch, Web Service",The repository contains a Prefect pipeline for automated data processing and model training (batch component) and a Streamlit application for serving predictions via web interface (web service component). The Dockerfile shows the Streamlit app is containerized and served on port 8501.,Other
https://github.com/Danodia-Rahul/MLops-Project-25,MLOps Model Deployment Workflow,"Batch, Web Service","The repository contains both a Prefect workflow (Prefect/prefect_flow.py) for batch processing and model evaluation, and a Flask-based API service (Deployment/Dockerfile, app.py) for serving predictions via HTTP endpoints.",Other
https://github.com/maxim-eyengue/Machine-RUL-Predictor,Construction Equipment RUL Predictor,Batch,"The repository uses Prefect for workflow orchestration with a prefect.yaml configuration file and orchestrate_prefect.py defining Prefect flows for data processing and model training. This is a batch processing pipeline that runs scheduled ETL jobs to download data, preprocess it, train models, and track experiments.",Other
https://github.com/stefbp066/shill-bidding-prediction,Shill Bidding Detection System,Web Service,"The repository contains a FastAPI application (main_simple.py/main.py) that serves ML predictions via HTTP endpoints (/predict, /health). The Dockerfile and docker-compose.yml show it's containerized and deployed as a web service. While Grafana/Prometheus are included for monitoring, they are visualization/monitoring layers, not streaming components.",AWS
https://github.com/deedeepratiwi/mlops-hotel-cancellation,Hotel Cancellation Prediction System,Web Service,"The repository contains a FastAPI application (deployment/app.py) that serves ML models via REST API endpoints, containerized with Docker and orchestrated via docker-compose. This is a classic web service deployment pattern for ML model serving.",Other
https://github.com/hbinol/IMDB_reviews_sentiment_classifier/tree/main,IMDB Reviews Sentiment Classifier,Batch,"The project is a batch-oriented ML pipeline for sentiment classification. It processes IMDB reviews in scheduled/batch workflows using Prefect for orchestration, performs batch training and batch predictions via command-line scripts, and lacks real-time streaming components or web service APIs for ML inference.",Other
https://github.com/iamparody/faang-mlops,FAANG Stock Price Forecasting,"Batch, Web Service",The repository contains both batch and web service components. The batch component is implemented using Mage orchestrator (docker-compose.yaml shows mage service with faang_pipeline) for scheduled ETL jobs. The web service component is implemented using FastAPI for model serving (docker-compose.yaml shows fastapi service with uvicorn server on port 8000).,Other
https://github.com/DanielIramain/forecast-house-pricing-california/tree/main,California House Price Forecasting,Web Service,"The repository contains a FastAPI application (main.py) that serves ML model predictions via REST API endpoints. The architecture diagram shows a web service deployment pattern with API serving, and the monitoring setup includes Grafana dashboards for tracking the service. The codebase is structured as a web service that provides real-time predictions rather than batch processing or streaming data pipelines.",Other
https://github.com/ravil-gasanov/fraud_detector,Credit Card Fraud Detection Pipeline,Batch,"The repository uses Prefect for workflow orchestration with batch prediction flows. The Docker Compose setup includes a batch_predict service that runs scheduled batch jobs to process data and make predictions, loading results to a PostgreSQL database. No streaming components (Kafka, Flink) or web service APIs are present.",Other
https://github.com/KrishnaG-101/Germany-Used-Cars-Price-Prediction,Used Car Price Prediction,Web Service,"The repository contains a Flask web application (app.py) that serves ML model predictions via API endpoints. The Dockerfile builds a containerized Flask app using Gunicorn, and the prefect.yaml shows deployment configuration for a web service. This is not batch processing (no scheduled ETL jobs) or streaming (no message brokers or real-time processing).",Other
https://github.com/AFARNOOD/SmartMobility-Engine,Montreal Bike Sharing ML Pipeline,"Batch, Web Service","The repository contains a Prefect 3 MLOps pipeline for batch model training and orchestration, plus a FastAPI web service for model deployment and prediction API. The Dockerfile shows containerization of the FastAPI service, while the project architecture and badges indicate Prefect-based batch processing.",Other
https://github.com/bizzaccelerator/corn-yield-prediction.git,Kenya Corn Yield Prediction,"Batch, Web Service","The repository contains both batch processing components (Kestra workflow orchestrator for scheduled ETL jobs) and web service components (Flask API for serving ML model predictions, MLflow UI for model tracking, Evidently UI for monitoring)",GCP
https://github.com/gishoo/RedditTitleSentiment#,Unknown,Unknown,No files fetched,Unknown
https://github.com/adeakinwe/MLOps_credit_risk_default,Credit Default Monitoring Platform,"Batch, Web Service","The repository contains Prefect workflow orchestrator code for batch data processing and model training, plus Flask API code for serving the trained model as a web service. The requirements.txt includes both Prefect and Flask, indicating both batch orchestration and web service deployment.",Other
https://github.com/ARMOD07/MALOPS-ZOOMCAMP/tree/main/final_project,Unknown,Unknown,No files fetched,Unknown
https://github.com/michelangelo89/churn-clv-mlops-project,Customer Churn Prediction Pipeline,Web Service,"The repository contains a Flask API (predict_flask.py) served via Docker container on port 9696, with curl-based testing and Docker build/run commands in the Makefile. This is a classic Web Service deployment pattern for ML model serving.",AWS
https://github.com/cssaritama/Predictive-Maintenance-Mlops,Predictive Maintenance Pipeline for Manufacturing,"Batch, Web Service","The project includes a FastAPI web service for model serving (main.py, Dockerfile exposing port 8000) and a Prefect workflow orchestrator (prefect_pipeline.py) for batch model training and pipeline execution.",GCP
https://github.com/April-hjy217/AUSHousingML,XGBoost House Price Model,"Batch, Web Service","The project uses Prefect 2.0 for workflow orchestration (ETL  train  evaluate  monitor) which is a batch processing pattern, and also includes a Flask API for real-time model serving via /predict endpoint",Other
https://github.com/Sergiosm3GIT/pollution_prediction,Meteorological Data Processing Engine,Batch,"The project uses Prefect flows for scheduled data extraction and preprocessing jobs, with Terraform-deployed Cloud Run Jobs that run on schedules. The architecture shows batch ETL patterns: extract data periodically, preprocess it, and store in GCS. No streaming components (Kafka, Flink) or web service APIs are present.",GCP
https://github.com/elvisiraguha/student-performance-mlops,Education ML Model Serving,Web Service,"The repository contains a FastAPI application (serving/main.py) that serves ML model predictions via HTTP endpoints, which is characteristic of a Web Service deployment for model serving.",Other
https://github.com/hannarud/energy-behavior-prosumers-kaggle-mlops,MLOps Energy Demand Pipeline,Batch,"The repository contains a complete ML pipeline with training, validation, and prediction capabilities, along with MLFlow integration for experiment tracking and model registry. The docker-compose setup shows a batch-oriented MLflow server for model management and tracking, with MySQL backend for storing experiments and metadata. The project includes scripts for running pipeline training jobs and comprehensive configuration management, indicating a batch processing approach for ML model training and deployment rather than real-time streaming or web service serving.",Other
https://github.com/exequiel-santucho/MLOpsProject-StockMarketPricePrediction,XGBoost Stock Price Model,Batch,"The project uses Prefect for workflow orchestration with scheduled/interval-based execution (IntervalSchedule), MLflow for experiment tracking/model registry, and Docker for containerization. The pipeline downloads historical data, trains models, and registers them - all batch operations triggered on a schedule or manually.",Other
https://github.com/gabrielorce/311_predictions,NYC 311 Response Time Model,Web Service,"The repository contains a FastAPI application (app/main.py) that serves ML predictions via HTTP endpoints (/predict, /health). The docker-compose.yml shows the model is deployed as a web service container accessible at http://localhost:8000. The project also includes Grafana dashboards for monitoring, but these are visualization layers, not streaming components.",Other
https://github.com/MohamedMostafa259/LandType-Sentinel2-Segmentation,Sentinel-2 Land Cover Segmentation,Web Service,"The repository contains a Gradio-based web application (app.py) that serves ML model inference via an interactive web interface. The code shows a real-time prediction system where users can upload Sentinel-2 images and get segmentation masks, which is characteristic of a web service deployment rather than batch or streaming processing.",Other
https://github.com/fabianjkrueger/mlops_zoomcamp_project,Batch ML Model for Gaming,"Batch, Web Service","The repository contains both batch processing and web service components. The README explicitly states the goal was to build a model deployable ""as a web service as well as in batch mode."" The Dockerfile shows a Flask-based model serving application (serve_model_simple.py) for web service deployment, while the README mentions using workflow orchestrators for scheduled batch jobs and simulating batch processing with time-based data.",Other
https://github.com/Jayadeep19/powerplant_output_prediction#,Unknown,Unknown,No files fetched,Unknown
https://github.com/SafeNeptune148-privjob/Credit-Risk-Prediction,Credit Risk Prediction Pipeline,Batch,"The repository contains an Airflow implementation (model_airflow.py) that orchestrates scheduled ETL jobs for credit risk prediction, which is characteristic of batch processing. The workflow involves periodic data processing rather than real-time streaming or web service API serving.",Other
https://github.com/raven0205/Written-Assignment-1,Unknown,Unknown,No files fetched,Unknown
https://github.com/Jasmine25005/AgroScanAI-Plant-Disease-Detection-with-Transfer-Learning/tree/main,AgroScanAI Deep Learning Models,Unknown,"The repository contains only a Jupyter notebook with deep learning model training code and no deployment infrastructure, API endpoints, workflow orchestrators, or streaming components. It appears to be a research/analysis project without production deployment code.",Unknown
https://github.com/pats2014/weather-forecast,Weather Forecast Accuracy Monitor,Batch,"The repository contains Airflow DAGs (dags/forecast.py, dags/monitoring.py) that orchestrate scheduled ETL jobs for model training and monitoring. The code shows scheduled tasks that pull data, train models, and update monitoring dashboards periodically rather than real-time streaming or API-based web service deployment.",AWS
https://github.com/Chrmorod/mlops-zoomcamp/tree/main/07-final-project,30-Day Stock Price Prediction,Batch,"The repository uses Mage.ai as a workflow orchestrator to run scheduled ETL/ML pipelines. The architecture shows discrete steps (data ingestion, preprocessing, modeling, visualization) executed as orchestrated workflows, not continuous streaming or web service deployment. The docker-compose and scripts are set up for batch pipeline execution.",Other
https://github.com/profgreatwonder/Adenocarcinoma_CNN_CT_Scan_Classification,CT Scan Adenocarcinoma Detection Pipeline,Web Service,"The repository contains a Flask application (Flask-Cors, Flask in requirements.txt) and a deep learning model serving setup for chest disease detection. The code structure shows a MLOps pipeline designed to serve ML models via API endpoints for medical image classification, which is characteristic of a Web Service deployment pattern.",Other
https://github.com/freillat/MLOpsProject,ML Pipeline Orchestration Framework,Web Service,"The repository contains a FastAPI application (app.py) that serves ML models via REST API endpoints, along with a Streamlit dashboard (dashboard.py) for visualization. These are web service components for serving ML inference and dashboards.",Unknown
https://github.com/Foluwa/yoruba_mlops_alphabet_project,Yoruba ML Model Registry,Web Service,"The repository contains a Streamlit application (app.py) that serves as a web interface for the Yoruba alphabets MLOPS project, allowing users to interact with the model through a web dashboard.",Unknown
https://github.com/nourrezk/finalprojeeect,E-commerce Sales Analysis,Unknown,"The repository contains only a README.md file with no actual code, configuration files, or infrastructure definitions. No deployment type can be determined from the available content.",Unknown
https://github.com/YYHugo/mlops-zoomcamp/tree/project-churn/churn-project,Infrastructure Management for ML,Web Service,"The repository contains a Flask API (`api.py`) that serves ML model predictions via HTTP endpoints, a Dockerfile for containerization, and a `docker-compose.yml` for deployment orchestration. This indicates a web service deployment pattern for serving ML models.",Other

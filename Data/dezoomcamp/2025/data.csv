project_url,project_title,Deployment Type,Reason,Cloud
https://github.com/Maddiezheng/LTA_CarParkAvailability_DE_Project,Urban Parking Data Pipeline,"Batch, Streaming","The project implements both batch and streaming components. The streaming component uses Redpanda (Kafka-compatible) for real-time data flow and Flink for stream processing. The batch component uses Airflow for scheduled ETL jobs, dbt for data transformations, and periodic data extraction from the LTA API.",GCP
https://github.com/KareemAdel10/Aviation-Flights-Real-Time-Analytics,Aviation Flights Real-Time Analytics,"Batch, Streaming","The project uses both batch and streaming components. For batch: Apache NiFi flows for data ingestion, Kestra workflow orchestrator, dbt transformations, and Databricks notebooks for ETL. For streaming: The repository name explicitly mentions ""Real-Time Analytics"" and includes a streaming notebook (Streaming_Notebook.ipynb) that processes data in real-time using Spark Structured Streaming with Azure Event Hubs as the source.",Azure
https://github.com/ketut-garjita/Hospital-Data-Pipeline-Project/tree/main,Flink Hospital Operations Analytics,"Batch, Streaming","The project implements both streaming and batch components. Streaming: Redpanda (Kafka-compatible) for message brokering, Apache Flink for real-time stream processing, Debezium for CDC, Python producers/consumers for continuous data flow. Batch: Kestra workflow orchestrator for scheduled ETL jobs, dbt for batch transformations, Terraform for infrastructure provisioning, and scheduled data pipeline orchestration.",GCP
https://github.com/chenjing2025/de-project,TfL Cycling Count Analytics Pipeline,Batch,"The repository contains Airflow DAGs (data_ingestion.py, data_load_to_bq.py) that orchestrate scheduled ETL jobs. The pipeline pulls data periodically from TfL URLs, transforms it using dbt, and loads it to BigQuery. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/cpwill01/webalytics-data-pipeline,Kafka Music Streaming Data Warehouse,"Batch, Streaming","The repository contains both streaming and batch components. Streaming: Eventsim generates real-time event data published to Kafka topics, with Kafka Connect sink connectors consuming data and writing to GCS. Batch: An hourly scheduled job is run on dbt Cloud to transform data into tables, indicating batch processing for the transformation layer.",GCP
https://github.com/mark-js/bitcoin-pipeline,Bitcoin Market Data Pipeline,"Batch, Streaming","The repository contains both batch and streaming components. Batch: Airflow DAGs orchestrate scheduled ETL jobs for historical data processing, with Spark jobs for transforming and loading data. Streaming: Redpanda (Kafka-compatible) messaging system with Flink for real-time stream processing of tick data into OHLCV format, plus Kafka producers/consumers for continuous data flow.",GCP
https://github.com/Deathslayer89/DTC_dataEngg/tree/main/Stock-market-analytics,Kafka Stock Data Processing Platform,"Batch, Streaming",The repository contains both batch processing components (Dataproc with Spark jobs for historical data) and streaming components (Kafka VM for real-time stock market data streaming). The architecture documentation shows batch processing via Dataproc and real-time streaming via Kafka working together.,GCP
https://github.com/tuanlearning/news_sentiment,Kafka News Data Pipeline,Streaming,"The repository implements a streaming pipeline using Apache Kafka with a producer (producer/producer.py) that fetches news data and performs sentiment analysis, and a consumer (consumer/consumer.py) that reads from Kafka and loads data into BigQuery. The Docker Compose configuration and Makefile commands confirm this is a real-time streaming architecture.",GCP
https://github.com/abda-torey/instacart-ELT-dashboard,Instacart Real-Time Analytics Pipeline,"Batch, Streaming",The repository contains both streaming and batch components. Streaming: Apache Flink processes data continuously with Kafka/Redpanda for real-time data ingestion and processing. Batch: Apache Airflow orchestrates scheduled ETL workflows for moving data from GCS to BigQuery and running dbt transformations.,GCP
https://github.com/schwinger42/movielens-streaming-pipeline,MovieLens Streaming Analytics Pipeline,"Batch, Streaming","The project contains both batch and streaming components. The streaming component is evidenced by Kafka producers/consumers (movie_producer.py, test_consumer.py, verify_consumer.py) that stream MovieLens data to Kafka topics. The batch component is evidenced by workflow orchestrators (Kestra flow files, dbt models) that run scheduled ETL jobs to process data through a medallion architecture with bronze/silver/gold layers.",AWS
https://github.com/SergeiOssokine/airquality_capstone/tree/main,WHO Air Quality Compliance ETL,Batch,"The repository uses Prefect Cloud as a workflow orchestrator to run scheduled ETL jobs. The architecture shows Prefect flows that pull data periodically, transform it, and load it to BigQuery. The code includes Prefect task/flow definitions, deployment configurations for CloudRun, and dbt transformations - all characteristic of batch processing rather than continuous streaming.",GCP
https://github.com/DrUkachi/streaming-ecom-analytics/,E-Commerce User Journey Analytics,"Batch, Streaming","The project uses both batch and streaming components. The Airflow DAG orchestrates batch processing tasks (extract_monthly_data, validate_snowflake_table, materialize_views) that run on a schedule. Simultaneously, there's a streaming component where data is sent to Kafka via send_to_kafka() and consumed by a Snowflake Snowpipe connector for real-time ingestion.",Other
https://github.com/armanruet/DE-2025-Air-quality-monitoring-analysis,Air Quality Data Integration Platform,Batch,"The project uses Apache Airflow with DAGs (airflow/dags/air_quality_dag.py) that run scheduled ETL jobs. The pipeline extracts data from APIs, transforms it, and loads it to BigQuery on a daily schedule. The dbt models also indicate batch transformation workflows.",GCP
https://github.com/envoamr/crypto-transactions,Crypto Market Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs (docker-compose.yml shows Kestra server, and README describes batch processing with daily PySpark jobs). The pipeline processes cryptocurrency data in batches rather than streaming.",GCP
https://github.com/jorczyk/HistoricalFootballResultsInsights,International Football Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs (data_ingestion_scores.py) for scheduled ETL jobs, and dbt for data transformation. The architecture follows a batch processing pattern where data is pulled periodically, transformed, and loaded to BigQuery. The Airflow DAGs show scheduled data ingestion from CSV files, transformation via dbt, and loading to BigQuery.",GCP
https://github.com/kantundpeterpan/bluesky_ddd_influenza,Digital Disease Detection ETL,Batch,"The repository uses Kestra workflow orchestrator with Docker containers to run scheduled ETL jobs. The code shows Kestra flows for backfill and triggered data collection, dlt pipelines for data extraction/loading, and dbt models for transformation. The architecture diagram and TODOs indicate batch processing pipelines that run once per day at midnight for the preceding day, using incremental loads and scheduled triggers rather than continuous streaming.",GCP
https://github.com/rupesh-biswas/Indian-Accident-Analysis-Data-Pipeline,Accident Data Warehouse Platform,Batch,"The project uses Apache Airflow DAGs to orchestrate scheduled ETL jobs that process data periodically. The DAGs run monthly, transform data using Python operators, and submit PySpark jobs to Dataproc for batch processing. Data is loaded to BigQuery for analysis, indicating a batch-oriented pipeline rather than continuous streaming.",GCP
https://github.com/petertrung8/phs_data_engineering,COVID-19 Scotland Monitoring ETL,Batch,"The repository uses Kestra workflow orchestrator with scheduled ETL jobs (docker-compose.yml, phs-dataset-pipeline.yml) that run weekly on Mondays at 1am. The pipeline fetches data from APIs, transforms it using dbt, and loads it to BigQuery - all characteristic of batch processing with scheduled workflows rather than continuous streaming.",GCP
https://github.com/CMPablos/wikimedia-pageviews-analysis.git,Wikimedia Pageviews Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (pageviews-data and dbt-transformation-flow) that run periodically to download, transform, and load data. The architecture shows batch-oriented ETL jobs triggered hourly, with data stored in GCS and BigQuery, and dbt transformations run as discrete jobs rather than continuous streaming.",GCP
https://github.com/kiritsugulyn/hdb-price-etl,Singapore Property Price ETL Pipeline,Batch,"The repository uses Kestra workflow orchestrator with scheduled ETL jobs (docker-compose.yml shows Kestra server), Python scripts for data extraction/transformation, dbt for modeling, and Terraform for infrastructure. This is a classic batch ETL pipeline with scheduled data pulls and transformations, not continuous streaming.",GCP
https://github.com/omerdogan7/citibike-data-engineering-project,Citi Bike Trip Analytics Pipeline,Batch,"The repository contains an Airflow DAG (citibike_batch_processing) that orchestrates scheduled ETL jobs, including data ingestion, Spark batch processing, and dbt transformations. The pipeline downloads historical data periodically and processes it in batches rather than streaming it continuously.",GCP
https://github.com/dnagarajan807/de-zoomcamp-final-project.git,US Housing Market Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs (data_ingestion_gcs_dag.py) to orchestrate scheduled ETL jobs that download TSV files, convert to Parquet, upload to GCS, and load to BigQuery. This is a classic batch processing pattern with periodic data pulls and transformations.",GCP
https://github.com/naivebird/noaa-storms-analysis/tree/main,NOAA Storms BigQuery Platform,Batch,"The project uses Airflow with DAGs (ingest_noaa_data.py) to orchestrate scheduled ETL jobs that download, process, and load data periodically. The architecture diagram shows batch processing with Spark and scheduled workflows, not continuous streaming.",GCP
https://github.com/Tinker0425/PetFinderAPI,Pet Adoption Data Warehouse,Batch,"The repository contains Terraform infrastructure for GCS and BigQuery, and the README describes an end-to-end data pipeline with workflow automation via GitHub Actions. The architecture involves scheduled data extraction from PetFinder API, transformation with dbt, and loading to BigQuery - all characteristics of batch processing. No streaming components (Kafka, Flink, Kinesis) are present in the code.",GCP
https://github.com/keremycdg/NEO_Data-Pipeline_2D-Simulation,NASA NEO Data Pipeline,Batch,"The repository uses Kestra workflow orchestrator with docker-compose.yml configuration for scheduled ETL jobs. The pipeline pulls data periodically from NASA's API, transforms it, and loads it to BigQuery data warehouse. The Terraform configuration shows infrastructure setup for batch processing components (BigQuery, GCS) rather than streaming infrastructure like Kafka or Flink.",GCP
https://github.com/AuraFrizzati/DE-2025-FinalProject-NHS-EmergencyDeptAttendances,NHS England Healthcare Data ETL,Batch,"The project uses Kestra workflow orchestrator with scheduled triggers (cron: ""0 0 15 * *"") to run monthly ETL jobs that scrape NHS data, transform it via Python scripts, and load to GCS/BigQuery. This is batch processing, not streaming.",GCP
https://github.com/Adaora-AA/Synthetic-CMS-Medicare-Claims---Chronic-Kidney-Disease-Analysis.git,CMS Medicare Claims Data Warehouse,Batch,"The project uses Apache Airflow with DAGs for workflow orchestration, dbt for batch transformations, and scheduled ETL jobs. The Dockerfile and docker-compose.yaml show Airflow setup with dbt commands, indicating batch processing rather than streaming.",GCP
https://github.com/scarlett-de/Zoomcamp_hw/tree/main/project,Unknown,Unknown,No files fetched,Unknown
https://github.com/mikitatryzno/caucasus-apartment-price-insights,Caucasus Apartment Price Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs, dbt transformations, and Spark batch processing. The architecture shows data is pulled periodically from CSV files, transformed in batch mode, and loaded to BigQuery.",GCP
https://github.com/valeqm/World-Disaster-Pipeline,Global Disaster Statistics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with scheduled ETL jobs (docker-compose.yml shows Kestra service, flows directory for workflows), Terraform for infrastructure, dbt for data transformation, and BigQuery for data warehouse - all characteristic of batch processing pipeline",GCP
https://github.com/VietNgoDev/sp500-data-pipeline,S&P 500 Analytics Pipeline,Batch,"The repository contains an Airflow DAG (sp500_data_pipeline.py) that orchestrates scheduled ETL jobs to process S&P500 data. The pipeline includes tasks for downloading data, uploading to GCS, processing with PySpark, and loading to BigQuery. The README describes this as a ""batch data pipeline"" and the architecture diagram shows batch-oriented components (Cloud Composer/Airflow, Cloud Storage, BigQuery).",GCP
https://github.com/data-tomic/gcp-open-payments-pipeline,Open Payments Batch Processing Pipeline,Batch,"The pipeline uses Airflow (Cloud Composer) to orchestrate scheduled ETL jobs. Data is ingested from GCS, processed by Spark on Dataproc, loaded to BigQuery, and transformed using SQL queries - all in batch mode. The DAG structure and scheduled execution confirm batch processing.",GCP
https://github.com/intergalacticmule/sales-data-solution/,Retail Sales Analytics Pipeline,Batch,"The repository uses Apache Airflow with multiple DAGs for workflow orchestration, including data upload, dbt transformations, and infrastructure management. The architecture follows a traditional ETL pipeline with scheduled jobs rather than continuous streaming.",GCP
https://github.com/yz-jz/Traffic-incidents-ELT-pipeline,Traffic Incident Analytics Pipeline,Batch,"The repository contains an Apache Airflow DAG (extract_transform_load.py) that orchestrates a batch ETL pipeline. The pipeline extracts traffic incident data from APIs, transforms it using Polars, and loads it to cloud storage and BigQuery. The infrastructure is provisioned using Terraform with a Compute Engine instance running Airflow, and dbt transformations are executed via Cosmos. There are no streaming components like Kafka, Kinesis, or Flink present.",GCP
https://github.com/gabrielfoo/zoomcamp_taxi_availability,Singapore Taxi Coordinate Data Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled DAGs (flows) that run periodically (e.g., every 15 minutes for data ingestion, daily for transformations). The pipeline follows a traditional ETL pattern: scheduled data scraping, batch uploads to BigQuery, and batch transformations using Dataproc/Spark. No streaming technologies like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/zx-99/data-engineering-zoomcamp2025-project.git,Top 50 Charts Audio Analysis,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs, Terraform for infrastructure as code, and dbt for data transformations. The workflow orchestration is configured with Docker Compose and includes scheduled data processing tasks, indicating a batch processing architecture rather than continuous streaming.",GCP
https://github.com/Marinazuzum/World-Happiness-Index,World Happiness Analytics Pipeline,Batch,"The repository contains an Apache Airflow DAG (`02-airflow/dags/happiness_dag.py`) that orchestrates a batch ETL pipeline. The pipeline downloads a static dataset from Kaggle, processes it using Python operators, loads it to BigQuery, and runs dbt transformations. This is a classic batch workflow triggered periodically, not a continuous streaming process.",GCP
https://github.com/alibstill/housing,UK Residential Property Data Warehouse,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL pipelines (flows/housing_local_gc_etlt_price_paid.yml) that run periodically to extract, transform, and load data. The code shows batch processing patterns with scheduled jobs, dbt transformations, and monthly data updates rather than continuous streaming.",GCP
https://github.com/fengyu20/paris-bike-sharing-data-engineering,Bike Usage Patterns ETL Platform,Batch,"The repository contains Airflow DAGs for scheduled ETL jobs (hourly ingestion, daily dbt transformations) and dbt transformations, indicating a batch processing architecture. There are no streaming components like Kafka, Flink, or real-time processing elements.",GCP
https://github.com/francofox/mtl-cityjobstats,City Employment Insights Dashboard,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs (flow.yml files) that pull data periodically from CSV files, transform it, and load to BigQuery. The pipeline is triggered weekly and uses dbt for transformations, which are batch processing patterns.",GCP
https://github.com/Radu-Stroe/spotify_pipeline,Global Music Trends Data Warehouse,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (spotify_ingestion.yaml and spotify_transformation.yaml) that run periodically to download, transform, and load data. The pipeline follows a batch ETL pattern with scheduled tasks rather than continuous streaming.",GCP
https://github.com/dehaoterryzhang/EcoPulse,Economic Data Cloud Pipeline,Batch,"The repository uses Kestra workflow orchestrator with YAML-defined flows (data_load_gcs.yaml) that fetch economic data from FRED API, process it, and load to GCS. The architecture diagram and documentation explicitly describe it as a ""batch data pipeline"" with scheduled ETL jobs.",GCP
https://github.com/WadyOsama/USA-Flights-Pipeline,US Flight Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator to run scheduled ETL jobs. The Kestra flow YAML file shows a batch-oriented pipeline that downloads data, processes it, loads it to BigQuery, and runs dbt transformations. The infrastructure is provisioned via Terraform with a VM running the orchestrator, indicating batch processing rather than continuous streaming.",GCP
https://github.com/xuhui-eagle-ying/de-zoomcamp-yfinance,Nasdaq 100 Stock Analytics Pipeline,Batch,"The project uses Airflow with DAGs to orchestrate scheduled ETL jobs that run periodically to extract data from Wikipedia and Yahoo Finance, transform it with PySpark, and load it to BigQuery. The workflow is batch-oriented with scheduled tasks rather than continuous streaming.",GCP
https://github.com/gatechme99/nfl-datapipeline,NFL Quarterback Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator (docker-compose.yaml) to run scheduled ETL jobs. The architecture shows data ingestion, transformation, and loading to BigQuery in a batch process. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/hilarylui96/de-zoomcamp-project,Weather Severity Insights Pipeline,Batch,"The project uses Apache Airflow with DAGs scheduled daily (@daily) and orchestrated ETL jobs. Data is pulled from the NWS API periodically, transformed using PySpark on Dataproc, and loaded to BigQuery. The architecture follows a classic batch ELT pattern with scheduled workflows.",GCP
https://github.com/franadam/Infrabel-Analytics/tree/main,Infrabel Train Punctuality Analytics Pipeline,Batch,"The repository contains Airflow DAGs (punctuality_data_ingestion_dag, station_data_ingestion_dag) that orchestrate scheduled ETL jobs for downloading, converting, and uploading data. The README explicitly describes using Airflow with PythonOperators/BashOperators for batch processing, and mentions using backfill for historical data loading.",Other
https://github.com/Sharonsyra/beverage-sales-data-engineering-project,Beverage Sales Insights Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (e.g., zoomcamp.gcp_dbt.yml, zoomcamp.gcp_ingest_and_load.yml) to run scheduled ETL jobs. The pipeline ingests CSV data from GCS, transforms it using dbt (which runs batch transformations), and loads to BigQuery. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/IuliiaKameneva/DataEngineering_Project_USA_Births,USA Birth Rate Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with DAGs (gcp_natality_flow.yaml) to run scheduled ETL jobs. The pipeline extracts data from BigQuery, transforms it using dbt, and loads it to partitioned tables. The docker-compose.yml shows Kestra is configured for scheduled execution, not continuous streaming.",GCP
https://github.com/Kadipa/DOB-JOB-DE-Project,DOB Job Applications Data Warehouse,Batch,"The pipeline is orchestrated by Airflow with a daily schedule (schedule_interval=""@daily""), using DLT for batch ingestion from REST API, dbt for batch transformations, and Glue crawlers for batch cataloging. No streaming components like Kafka, Kinesis, or Flink are present.",AWS
https://github.com/celineyayifeng/National-Gallery-Art-Analytics,National Gallery Art Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML-defined flows (extract_identifier.yaml, extract_objects_categorization.yaml, etc.) that run scheduled ETL jobs to extract CSV data, upload to GCS, and load to BigQuery. This is a classic batch processing pattern with periodic data pulls and transformations.",GCP
https://github.com/m-sav/data-careers-pipeline,Global Data Careers Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs for scheduled ETL jobs, includes dbt for data transformation, and follows a batch processing pattern with daily scheduled data ingestion from Kaggle.",GCP
https://github.com/fenniez2334/blockchain-data-pipeline,Bitcoin Transaction Data Pipeline,Batch,"The repository contains Kestra workflow orchestrator configurations (docker-compose.yml) and references to batch processing flows. The README explicitly mentions ""Batch Processing"" as a component, and Kestra is used for scheduling and orchestrating ETL jobs rather than real-time streaming.",GCP
https://github.com/rochanofa/eu-energy-dashboard,EU Energy Consumption Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with YAML flow definitions (01_gcp_kv.yaml, 02_gcp_setup.yaml, 03_ingest_energy.yaml, 04_run_dbt.yml) that define scheduled ETL jobs. The pipeline downloads CSV data, uploads to GCS, loads to BigQuery, and runs dbt transformations - all batch-oriented operations rather than continuous streaming.",GCP
https://github.com/adedeji-rodemade/de_zoomcamp_project,Transport for London Data ETL,Batch,"The repository contains a Prefect workflow orchestrator that runs scheduled ETL jobs to download and process TfL cycling data. The ingestion pipeline uses Prefect flows with cron scheduling (monthly) to check for new CSV files, download them, and upload to GCS. This is a classic batch processing pattern where data is collected periodically and processed in scheduled jobs, not continuously streaming.",GCP
https://github.com/Ccinaza/nyc_citibike_pipeline,Citi Bike Trip Data Warehouse,Batch,"The repository contains an Airflow DAG (etl.py) that orchestrates batch processing of monthly Citi Bike trip data. The pipeline downloads, uploads, and loads data in scheduled batches, with no streaming components like Kafka or Flink present.",GCP
https://github.com/Howard233/global-fashion-retail-sales-insights,Retail E-commerce Sales Insights ETL,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL flows (gcp_upload.yml, gcp_dbt.yml) that run periodically to extract, transform, and load data. The pipeline includes dbt build commands for batch transformations in BigQuery, and there are no streaming components like Kafka, Flink, or real-time processing elements.",GCP
https://github.com/sagar-rsh/Smart-Dublin-Traffic-Insights,Urban Traffic Monitoring ETL Solution,Batch,"The repository contains an Airflow DAG (Dockerfile, scripts folder) for orchestrating ETL jobs, dbt transformations for data modeling, and scheduled data processing from S3 to Redshift. This is a classic batch-oriented data pipeline architecture.",AWS
https://github.com/DEBBY-CODE/stats_canada_retail_food_prices,Retail Price Analytics with Kestra,Batch,"The project uses Kestra workflow orchestrator with scheduled tasks (cron: ""0 9 8 * *"") to run batch ETL jobs. The pipeline extracts data monthly from GCS, loads to BigQuery, and transforms with dbt. No streaming components (Kafka, Flink, Kinesis) are present.",GCP
https://github.com/Virgo-Alpha/LinkedIn_Job_Posts_Insights,LinkedIn Job Market Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs for workflow orchestration, which is a batch processing pattern. The code shows scheduled ETL jobs that run periodically to download, transform, and load data to BigQuery.",GCP
https://github.com/piotr-owczarek/github-data-insights,Bot vs Human Activity Analysis,Batch,"The code uses Apache Airflow with DAGs (gharchive_etl_to_gcs.py) that run on a scheduled basis (hourly at 15 minutes past the hour). The pipeline downloads hourly JSON files from GH Archive, transforms them to Parquet, and loads them to BigQuery using Dataproc/PySpark. This is a classic batch ETL pattern with scheduled workflow orchestration.",GCP
https://github.com/aimlandweb/santander-bike-insights,Santander Bike Analytics Pipeline,Batch,"The repository contains an Airflow DAG (santander_data_pipeline.py) that orchestrates scheduled ETL jobs, including loading data from GCS to BigQuery and running dbt transformations. The pipeline is configured with a daily schedule interval (@daily), indicating batch processing rather than continuous streaming.",GCP
https://github.com/DavidVFitzGerald/trade-flows-analysis,HS Code Trade Analysis Platform,Batch,"The repository contains a batch data pipeline that processes trade data from the BACI database. The code shows Terraform infrastructure for Google Cloud Storage, BigQuery, and Dataproc cluster for running PySpark jobs. The README explicitly states ""this project uses a batch workflow"" and describes steps including downloading annual data, converting CSV to Parquet, and processing with PySpark on Dataproc. No streaming components like Kafka, Kinesis, or real-time processing are present.",GCP
https://github.com/gchoong/SevereStorms,Severe Storms Data Pipeline,Batch,"The project uses Kestra as a workflow orchestrator with parameterized DAGs to run scheduled ETL jobs. The pipeline extracts data from BigQuery public datasets, exports to Cloud Storage, transforms with dbt, and loads to BigQuery. This is a classic batch processing pattern with scheduled data pulls and transformations, not continuous real-time streaming.",GCP
https://github.com/Vasupriya2000/zoomcamp_project.git,Amazon Book Reviews Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL pipelines (evident from Kestra flow definitions and scheduled triggers), Docker containers for processing, and dbt for batch transformations in BigQuery. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/singh3ss/zoomcamp2025project_worldecon/tree/main,Fraser Institute Data Processing Platform,Batch,"The repository uses Kestra, a workflow orchestrator, to run scheduled ETL jobs. The pipeline is defined through YAML files (gcp_econ_data.yaml) that specify tasks for data extraction, transformation, and loading into BigQuery. The code includes backfilling capabilities and scheduled execution patterns typical of batch processing workflows.",GCP
https://github.com/fensals/FAANG-STOCK-DATA-PIPELINE,FAANG Stock Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs for workflow orchestration, scheduled ETL jobs, and dbt for batch transformations. The README explicitly states it's a ""batch-based"" pipeline that executes periodically.",GCP
https://github.com/joseph-higaki/emr_data_pipeline/,EMR Healthcare Analytics Pipeline,Batch,"The repository implements a batch-oriented data pipeline using Apache Airflow for workflow orchestration. The code shows Airflow DAGs for ETL processes, dbt transformations, and scheduled data ingestion from Synthea. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/wingylui/DE_zoomcamp/tree/main/project,Refugee Population Data Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled tasks (data_ingest.yaml, GCP_setup.yaml) to run periodic ETL jobs. The pipeline downloads data yearly, transforms it with PySpark, and loads to BigQuery - all batch operations. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/abhayra12/lending_data_analytics,Lending Data Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (lending-data-flow.yml) that define a sequence of PySpark jobs submitted to Dataproc. Each step (data ingestion, cleaning, transformations) is executed as a batch job with dependencies, not continuously streaming data.",GCP
https://github.com/dryzrlbs/WeatherPipeline,PostgreSQL Weather Transformation Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (kestra/kestra.yml) to run scheduled ETL jobs. The pipeline includes dbt transformations and Python scripts for data ingestion, which are characteristic of batch processing patterns. The Dockerfile and docker-compose setup further indicate scheduled job execution rather than continuous streaming.",GCP
https://github.com/Rickarddev-code/zoomcamp-project,AI Adoption Data Transformation Pipeline,Batch,"The project uses Prefect for workflow orchestration with scheduled ETL jobs, Docker containers for batch processing, and dbt for batch transformations. The pipeline fetches data periodically from SCB's API rather than processing streaming data.",GCP
https://github.com/armoustafa/f1-data-pipeline,Formula 1 Data Analytics Pipeline,Batch,"The repository uses Apache Airflow with DAGs (upload_to_gcs.py, gcs_to_bq_dag.py) to orchestrate scheduled ETL jobs that process data periodically. The pipeline uploads CSV files to GCS, loads them to BigQuery, and uses dbt for transformations - all characteristic of batch processing. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/freillat/DEproject,S&P 500 Historical Analytics Pipeline,Batch,"The project uses Apache Airflow with a DAG (`fetch_sp500_data`) to orchestrate scheduled ETL jobs that pull stock data periodically, transform it, and load it to Google Cloud Storage and BigQuery. The pipeline is triggered manually or on a schedule, not in real-time.",GCP
https://github.com/muralimittireddy/Solar-Crops-Analysis,Andhra Pradesh Solar Agriculture Analytics,Batch,"The project uses Apache Airflow with DAGs for workflow orchestration, scheduled ETL jobs that pull data periodically from APIs, transform it using Spark, and load it to BigQuery. The architecture shows batch processing patterns with scheduled tasks rather than continuous streaming.",GCP
https://github.com/VadimChernik/de-zoomcamp-2025-project,Citi Bike Data Transformation Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (01_create_tables.yml, 02_dbt.yml) that run ETL jobs periodically. Data is loaded from GCS to BigQuery, then transformed using dbt - a classic batch processing pattern. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/SpreadSheetStation/Final-Project_DEZ2025,Unknown,Unknown,No files fetched,Unknown
https://github.com/April-hjy217/FinFlow,Financial Market Data Pipeline,Batch,"The project uses Airflow DAGs to orchestrate scheduled ETL jobs that pull data periodically from Alpha Vantage, transform it with Spark and dbt, and load it to BigQuery. The pipeline is designed for batch processing with scheduled runs rather than continuous real-time streaming.",GCP
https://github.com/kirill-developer/mta-crz-entries-pipeline,NYC Traffic Entry Data Warehouse,Batch,"The repository contains Airflow DAGs (nyc_traffic_congestion_data_daily_update.py, nyc_traffic_congestion_data_full_load.py, dbt_runner.py) that run scheduled ETL jobs. The pipeline uses dlt for batch data extraction from MTA's OData API and dbt for batch transformations. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/AGuanDE/ontario-sunshine-salary-dashboard,Ontario Payroll Data Warehouse,Batch,"The repository uses Apache Airflow with DAGs (run_dbt.py, sunshine_data_pipeline.py) to orchestrate scheduled ETL jobs, including downloading data, uploading to cloud storage, and running dbt transformations. There are no streaming components like Kafka, Kinesis, or Flink.",GCP
https://github.com/YinlongQian/thermal_anomaly_insights/,NASA MODIS Thermal Insights Platform,Batch,"The project uses Apache Airflow with DAGs (bash_extract_dag.py, python_transform_dag.py) to orchestrate scheduled ETL jobs. Data is extracted from MODIS sources, transformed using PySpark, and loaded to BigQuery in batch mode. No streaming components like Kafka, Flink, or Kinesis are present.",GCP
https://github.com/datapopcorn/Coffee-Shop-Sales-Analysis,Streamlit Coffee Shop Insights Dashboard,Batch,"The repository contains a batch data pipeline orchestrated by Apache Airflow, with FastAPI simulating sales data stored in PostgreSQL, then batch-synced to GCS and BigQuery using dlt. The pipeline uses dbt for transformations and Terraform for infrastructure, with no streaming components like Kafka or Flink present.",GCP
https://github.com/buzdugan/weekend_box_office,UK Film Box Office Pipeline,Batch,"The repository contains Airflow DAGs for scheduled ETL jobs, Terraform configuration for Cloud Composer (GCP's managed Airflow), and dbt models for data transformation. The project uses workflow orchestrators to run scheduled jobs that pull data periodically, transform it, and load it to BigQuery. The presence of DAGs and dbt runs indicates batch processing rather than streaming.",GCP
https://github.com/HossamDC/aircrash-data-pipeline-de.git,Airplane Crash Analytics Pipeline,Batch,"The pipeline uses Prefect for orchestration with scheduled ETL jobs, Spark on EMR for batch processing, and dbt for batch data modeling. The architecture shows periodic data pulls, transformations, and loads to Redshift rather than continuous real-time processing.",AWS
https://github.com/ramirovazq/vial-incidents-cdmx-c5,Vial Incidents Data Transformation Pipeline,Batch,"The repository uses Kestra workflow orchestrator with YAML-defined flows for ETL processes (data upload, transformation, loading). The Makefile shows scheduled/triggered execution of flows like ""01_gcp_kv"", ""02_gcp_create_bucket_and_dataset"", and ""03_upload_and_create_external"". Additionally, dbt is used for data transformation in the warehouse, which is a batch-oriented tool. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/baidlowi/Reddit-Streaming-with-Kafka,Technology News Data Streaming System,Streaming,"The repository contains a Kafka producer (producer.py) that streams Reddit data in real-time to Kafka topics, and a Spark consumer (consumer.py) that processes the streaming data. The docker-compose.yaml also sets up a local Kafka cluster for streaming. This is a continuous data flow system, not batch-processed.",GCP
https://github.com/Circhastic/de-zoomcamp-project,Cosmetics E-commerce Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs for workflow orchestration, which is a batch processing approach. The pipeline includes scheduled ETL jobs that pull data periodically from Kaggle, transform it, and load it to BigQuery. There are no streaming components like Kafka, Kinesis, or Flink mentioned.",GCP
https://github.com/mayurpat999/data-engineering-zoomcamp-2025/tree/main/Course%20Project,Unknown,Unknown,No files fetched,Unknown
https://github.com/harry1321/Grocery-Sales-Analysis,Grocery Sales Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs for workflow orchestration, dbt for transformation, and scheduled ETL jobs. The Dockerfile and configuration show Airflow as the orchestrator, with no streaming components like Kafka or Flink present.",GCP
https://github.com/jesusoviedo/spotify-dwh-insights,Music Insights BigQuery Pipeline,Batch,"The project uses workflow orchestrators (Kestra) with scheduled ETL jobs, DLT for batch data loading, and dbt for batch transformations. The code shows scheduled data extraction from Spotify API, batch processing through Parquet files, and batch loading to BigQuery - all characteristic of batch processing rather than streaming.",GCP
https://github.com/nikultcev/ny-parking-violations-data,New York City Parking Data Warehouse,Batch,"The project uses Apache Airflow with DAGs scheduled to run periodically (e.g., yearly on July 1st) to orchestrate ETL jobs. It includes batch processing steps like downloading CSV files, transforming data in batches, and loading to BigQuery, followed by dbt runs for transformations. No streaming components like Kafka, Flink, or Kinesis are present.",GCP
https://github.com/fsjoyti/dataeng-zoomcamp-final-project,Premium Trend ETL Pipeline,Batch,"The project uses Kestra, a workflow orchestrator, to run scheduled ETL jobs. The architecture shows data flowing from source → Kestra → GCS → BigQuery, which is a batch-oriented pipeline. The presence of dbt for transformations further confirms batch processing.",GCP
https://github.com/ipekguler/F1-Telemetry-Dashboard,F1 Race Control Analytics,Streaming,"The project uses Redpanda (Kafka-compatible) for real-time data streaming, PyFlink for continuous stream processing, and multiple Kafka topics (f1-laps, f1-position, f1-drivers, f1-race_control) for live F1 telemetry data. The architecture shows a continuous data pipeline from ingestion through Redpanda to processing with PyFlink and real-time visualization.",GCP
https://github.com/dareq112/db_project,US Airline Performance Analytics Pipeline,Batch,"The repository contains an Airflow DAG (dag.py) that orchestrates a batch ETL pipeline using Spark and dbt. The pipeline downloads data, processes it in Spark, and then runs dbt transformations. There are no streaming components like Kafka, Flink, or Kinesis present.",GCP
https://github.com/Enzoherewj/mta-subway-analytics/tree/main,Unknown,Unknown,No files fetched,Unknown
https://github.com/AmmarrOsama/Divvy-Bikes-Pipeline,Divvy Bikes Analytics Pipeline,Batch,"The repository contains an Airflow DAG (bikes_pipeline.py) that orchestrates scheduled ETL jobs including data download, unzip, upload to GCS, BigQuery table operations, and dbt transformations. The pipeline is scheduled monthly on the 15th with catchup enabled, indicating batch processing rather than continuous streaming.",GCP
https://github.com/cc59chong/End-to-End-Cloud-Data-Pipeline-for-Retail-Sales-Forecasting,BigQuery Retail Forecasting ETL Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs (kestra-etl.yml) that run dbt transformations and Spark data cleaning tasks periodically. The architecture shows batch processing patterns with workflow DAGs, scheduled dbt runs, and containerized Spark jobs rather than continuous streaming.",GCP
https://github.com/zixuansunnydeng/stock-analysis,US Stock Market ETL Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (stock_market_dbt_workflow.yml) that define scheduled ETL tasks including data download, transformation, and dbt runs. The architecture shows batch-oriented components like dbt transformations and scheduled data processing rather than continuous streaming.",GCP
https://github.com/QzQz-2000/Global-fashion-retail-sales-analysis.git,Global Retail Sales Data Pipeline,Batch,"The repository contains an Airflow DAG (fashion_sales_elt.py) that orchestrates scheduled ETL jobs, including data extraction, transformation with dbt, and loading to BigQuery. The architecture diagram shows batch processing components like Cloud Storage and BigQuery, with no streaming components like Kafka or Flink mentioned.",GCP
https://github.com/Edyarich/russia-historical-climate,Russia Climate Data Analytics Pipeline,Batch,"The repository contains a batch-oriented data pipeline using AWS Step Functions to orchestrate Glue ETL jobs that process historical climate data by year (1901-2024). The architecture shows scheduled/periodic processing of data rather than continuous streaming. While Airflow is listed as a dependency, the actual implementation uses Step Functions for workflow orchestration, which is characteristic of batch processing.",AWS
https://github.com/abliskan/capstone-project-dezoomcamp-1,Formula 1 Historical Data Pipeline,Batch,"The project uses Apache Airflow with DAGs (dag-ingest-*.py) to orchestrate scheduled ETL jobs that extract historical F1 data, transform it, and load it to cloud storage and BigQuery. The README explicitly states ""batch (@daily)"" processing and shows Airflow DAGs for data ingestion.",GCP
https://github.com/hypertoast/clean-de-zoomcamp-2025,Weather Data Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML-defined tasks (noaa_weather_ingest.yml) that run scheduled ETL jobs - extracting data from BigQuery, transforming it, and loading to optimized tables. This is a classic batch processing pattern with discrete workflow runs.",GCP
https://github.com/Abdou240/Sales-Analysis,Sales Data Processing Infrastructure,Batch,"The project uses Airflow DAGs for scheduled ETL jobs, Terraform for infrastructure provisioning, and PySpark jobs for data processing. The Airflow DAGs show scheduled batch processing (e.g., cron expressions), and the infrastructure includes Dataproc clusters for batch processing rather than streaming components.",GCP
https://github.com/plpcout/api-data-pipeline,Earthquake Data Processing Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions for scheduled ETL jobs, includes dbt transformations, and has backfill capabilities for historical data processing. The Makefile shows commands for triggering backfills and running dbt jobs, indicating batch-oriented data pipeline operations.",GCP
https://github.com/pizofreude/insightflow-retail-economic-pipeline,Economic Indicators Transformation Pipeline,Batch,"The repository contains a Kestra workflow orchestration file (insightflow_prod_pipeline.yml) that defines a batch pipeline with scheduled tasks: AWS Batch job submission for data ingestion, AWS Glue Crawler execution for cataloging, and dbt model execution for transformation. The dbt configuration files and requirements.txt confirm the use of dbt for batch data transformation. There are no streaming components like Kafka, Kinesis, or Flink present in the codebase.",AWS
https://github.com/IssaAlBawwab/gcp-ecommerce-end-to-end,BigQuery E-commerce Insights Pipeline,Streaming,"The repository implements a streaming data pipeline using Confluent Cloud Kafka with a Python producer that reads CSV data and streams it to a Kafka topic, and a Python consumer that reads from the topic and writes to BigQuery. The architecture diagram and code show continuous data flow through Kafka rather than scheduled batch jobs.",GCP
https://github.com/abhayra12/agri_data_pipeline,Farm Equipment Data Integration,"Batch, Streaming","The repository contains both streaming and batch components. Streaming: Kafka producers/consumers for real-time data ingestion from data sources to GCS (streaming_pipeline/ directory with data_producer.py, data_consumer.py, Kafka DAGs). Batch: Apache Airflow DAGs for scheduled batch processing, Spark jobs for batch transformation, and dbt for batch transformations on BigQuery data.",GCP
https://github.com/SherifOlalekan/Fashion_Retail_Sales_ETL_Analysis,E-commerce Fashion ETL Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (data_etl.yml, revenue.yml) that define scheduled ETL jobs. The pipeline follows a traditional batch pattern: extract data from GCS, transform with Spark, load to BigQuery. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/Rimsha-Bashir/GHG-Emissions-Analytics-Pipeline,Global GHG Emissions Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (gcp_kv.yml, gcp_upload.yml, gcp_spark_bq.yml) that execute scheduled ETL jobs. The pipeline follows a traditional batch pattern: data is ingested periodically, transformed using PySpark/Dataproc, loaded to BigQuery, and then dbt models are applied. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/serny2020/ShawnZoomcamp/tree/main/project,U.S. Traffic Accident Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML-defined tasks (toGCS.yml, toBigquery.yml) that run sequentially to download, transform, and load data. The pipeline follows a batch ETL pattern: download dataset via cURL, upload to GCS, then load into BigQuery using scheduled or triggered workflows. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/YannPhamVan/OptiFund-Data-Driven-Portfolio-Optimization/tree/main,Market Index Data Optimization Platform,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (fetch_indices_data.yaml) that run scheduled ETL jobs. The pipeline fetches historical index data periodically using yfinance, transforms it, and loads it to BigQuery. The presence of dbt transformations and Terraform infrastructure further confirms this is a batch-oriented data pipeline rather than real-time streaming.",GCP
https://github.com/leonardorleon/de-github-events-pipeline,Data Warehouse Development Environment,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (kestra/00_*.yml) to schedule and run ETL jobs. The architecture shows data being pulled periodically from GitHub Archive, stored in GCS, then processed through BigQuery with dbt transformations. The Docker Compose setup includes Kestra for orchestration and dbt for batch transformations, with no streaming components like Kafka or Flink present.",GCP
https://github.com/mananyev/portfolio-performance-tracking,Portfolio Performance Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (e.g., `ppt-project.all_tickers_names.yml`, `ppt-project.bigquery_dbt.yml`) that define scheduled ETL jobs. The architecture pulls financial data periodically from APIs, transforms it using dbt, and loads it to BigQuery. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/FaresAmiar/f1-data-engineering/tree/main,F1 Analytics Pipeline,Batch,"The repository implements a batch data pipeline using Docker Compose to orchestrate sequential ETL jobs (Terraform, ingestion, Spark processing, upload, dbt transformations). The workflow pulls data periodically from the Ergast API, transforms it using Spark, loads it to BigQuery, and runs dbt models - all characteristic of batch processing rather than continuous streaming.",GCP
https://github.com/mfajarandikha/youtube-data-analysis,Content Performance ETL Platform,Batch,"The project uses Apache Airflow with DAGs to orchestrate scheduled ETL jobs, including batch data ingestion from YouTube API, transformation with dbt, and loading to BigQuery. The code shows daily scheduled data fetching and processing workflows.",GCP
https://github.com/nsriram/data-engg-zc-2025-project/,IPL Performance Analytics Platform,Batch,"The project uses Kestra workflow orchestrator with YAML-defined tasks (00-ingest-from-kaggle-to-gcs.yml, 01-cleanse-data-and-upload-to-gcs.yml, 02-copy-gcs-data-to-bigquery.yml) that run sequentially to process data in batches. The architecture shows scheduled ETL jobs moving data from Kaggle → GCS → BigQuery, with dbt for transformation, which is characteristic of batch processing.",GCP
https://github.com/Bamboo-Star/overnight-equity-indices-market-analysis,Unknown,Unknown,No files fetched,Unknown
https://github.com/TejalK14/NYC_BIKESHARE_DATA_PIPELINE,Bike Share BigQuery Analytics Platform,Batch,"The repository contains an Airflow DAG (`nyc_bike_share_data_pipeline.py`) that orchestrates a batch data pipeline. It includes tasks for extracting data, uploading CSV files to GCS, creating BigQuery datasets, and running dbt transformations. The pipeline processes historical data from 2022-2025, which is characteristic of batch processing rather than real-time streaming.",GCP
https://github.com/rmengato/art-gallery-pipeline/,Art Gallery Data Pipeline,Batch,"The repository uses Kestra workflow orchestrator with scheduled ETL flows (from_bucket_to_staging, from_met_api_to_bucket, from_staging_to_transformed) that run periodically to extract, transform, and load data. The docker-compose setup shows Kestra server running in standalone mode with Terraform deploying these batch workflows.",GCP
https://github.com/rodriguesmafalda/de-project-seattle-library/tree/main,Seattle Library Checkout Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator to run scheduled ETL jobs that download data from Seattle Open Data, upload to GCS, and transform with dbt. The architecture shows batch processing patterns with periodic data pulls and transformations rather than continuous streaming.",GCP
https://github.com/Alvaro-Kothe/earthquake-hazzard,Global Earthquake Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs to orchestrate scheduled ETL jobs that run daily. The pipeline includes: 1) Airflow DAGs for data ingestion and processing, 2) dbt for transformation with scheduled runs, 3) Terraform for infrastructure provisioning, and 4) daily scheduled execution on a Google Cloud Compute instance. This is a classic batch processing architecture.",GCP
https://github.com/abdelrahmanyasser2001/Chicago-Crime-Data-Zoomcamp,Chicago Crime Data Pipeline,Batch,"The project uses Apache Airflow with DAGs to orchestrate scheduled ETL jobs that pull data from the Chicago crime API, load it into Snowflake staging tables, and transform it using dbt. This is a classic batch processing pattern with periodic data pulls and transformations.",Other
https://github.com/mohamedelieba/EGX-Analysis,Egyptian Stock Exchange ETL System,Batch,"The project uses Apache Airflow with a DAG (`egx_pipeline.py`) that runs daily (`schedule=""@daily""`) to extract, process, and load EGX stock market data. This is a classic batch ETL pattern with scheduled workflow execution, not continuous real-time processing.",GCP
https://github.com/kkumyk/server-logs-daily-data-pipeline,Server Access Logs Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML-defined flows (00_gcp_kv.yaml, 01_gcp_setup.yaml, 02_logs_2_gcs_2_bq.yaml) that process daily log files. The flows are triggered by date patterns (daily uploads) and include scheduled ETL tasks like extracting from GitHub, uploading to GCS/BigQuery, and dbt transformations. The architecture diagram shows batch processing components rather than streaming.",GCP
https://github.com/fonsecagabriella/carbonlens,World Emissions Data Integration,Batch,"The project uses Airflow DAGs (dbt-transform-dag.py, climate-data-dag.py, historical-spark-processing-dag.py) to orchestrate scheduled ETL jobs. Data is extracted from APIs, stored in GCS, processed with Spark, loaded to BigQuery, and transformed with dbt - all following a batch processing pattern with scheduled workflows rather than continuous streaming.",GCP
https://github.com/AbdelaliEch/Football-Data-Project.git,Kaggle Football Data Pipeline,Batch,"The project uses Apache Airflow with DAGs (project_dag.py) to orchestrate scheduled ETL jobs. The pipeline includes: 1) Weekly scheduled DAG execution (@weekly), 2) Sequential tasks: download data from Kaggle, upload to GCS, submit PySpark job to Dataproc, run dbt Cloud job, 3) Uses Airflow operators for file transfers, job submission, and dbt runs. No streaming components (Kafka, Flink, etc.) are present.",GCP
https://github.com/maberrospi/CancerAnalytics-de-zoomcamp2025/,Global Cancer Data Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (dbt_build_all.yaml) and dbt for ELT transformations. The pipeline follows a batch-oriented pattern: data is extracted from GBD, uploaded to GCS, loaded to BigQuery, then transformed using dbt models. The Kestra flows are manually triggered rather than event-driven, and there are no streaming components like Kafka, Flink, or Kinesis.",GCP
https://github.com/Poshi/dezc-project,Hate Crime Data Processing System,Batch,"The project uses Kestra workflow orchestrator with docker-compose.yml defining a Kestra service for scheduled ETL jobs. The workflow includes multiple steps: querying API, uploading to GCS, creating BigQuery tables, data transformation, and merging with deduplication. This is a batch-oriented pipeline with periodic data pulls and scheduled transformations.",GCP
https://github.com/yvt-ee/data-engineering-zoomcamp/blob/main/Project/README.md,PostgreSQL Data Ingestion Platform,Batch,"The repository contains workflow orchestration files for Airflow, Prefect, Kestra, and Mage, indicating batch-oriented ETL pipelines with scheduled DAGs and flows. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/MartinSchmeisserTrendence/stackoverflow_survey_insights,Stack Overflow Data Transformation Pipeline,Batch,"The project uses Kestra workflow orchestrator to run scheduled ETL jobs that extract, transform, and load data from Stack Overflow surveys. The code shows batch processing workflows with tasks for unzipping files, uploading to GCS, creating BigQuery tables, and dbt transformations. There are no streaming components like Kafka, Kinesis, or Flink.",GCP
https://github.com/TiagoCosta922/DataTalksProject_Commerce,Medallion Architecture Sales Analytics,Batch,"The project uses Prefect for workflow orchestration to run scheduled ETL jobs (daily and monthly), with data being extracted from Kaggle API, loaded into SQL Server, then copied to staging and processed through Microsoft Fabric pipelines. This is a classic batch processing pattern with periodic data pulls and transformations.",Other
https://github.com/hravat/coincap-de-project,Kafka Cryptocurrency Data Pipeline,Streaming,"The project uses Kafka for real-time message streaming and Apache Flink for continuous stream processing of cryptocurrency data from the CoinCap API. The architecture diagram and configuration files show a streaming pipeline where data flows from API → Kafka → Flink → Postgres, with real-time processing and analytics capabilities.",Other
https://github.com/jsrl/aemet-elt,AEMET Weather Data Pipeline,Batch,"The repository uses Kestra workflow orchestrator with scheduled ETL jobs, dlt for batch data loading, and dbt for batch transformations. The README explicitly states ""Kestra is an open-source, highly scalable, and flexible data orchestration platform designed to manage and automate the complex workflows of data pipelines"" and shows a Kestra topology diagram. There are no streaming components like Kafka, Flink, or real-time processing mentioned.",GCP
https://github.com/JoshPola96/heart-attack-data-pipeline,Indonesian Heart Attack Analytics Pipeline,Batch,"The project uses Apache Airflow (Google Cloud Composer) with DAGs for workflow orchestration, Terraform for infrastructure-as-code, and dbt for batch data transformations. The pipeline follows a traditional ELT pattern with scheduled jobs rather than continuous streaming.",GCP
https://github.com/fabianono/Stock_Data_Injestion,Stock Market ETL Pipeline,Batch,"The project uses Airflow DAGs with scheduled batch jobs (daily at midnight) to pull stock data, transform it, and load it to GCS and BigQuery. The code shows BashOperators running Python scripts on a cron schedule, not continuous streaming.",GCP
https://github.com/TimboBl/data_talks_club_de_2025_project?tab=readme-ov-file,Unknown,Unknown,No files fetched,Unknown
https://github.com/kelmanchen/spotify-charts-pipeline,Spotify Charts Analytics Pipeline,Batch,"The repository uses Apache Airflow with DAGs for workflow orchestration, includes dbt for data transformation, and has a Makefile with commands like ""terraform_deploy"" and ""up"" that build and run scheduled ETL jobs. The architecture diagram shows batch processing components rather than streaming.",AWS
https://github.com/liuchennn1414/DE-Zoomcamp-Project/tree/main,Singapore HDB Carpark Analytics Pipeline,Batch,"The project uses Airflow with DAGs (full_refresh_dag.py, ingest_carpark_info_dag.py, ingest_carpark_availability_dag.py) to orchestrate scheduled ETL jobs. Data is pulled periodically from APIs, transformed using Spark, and loaded to BigQuery. The architecture diagram and README confirm batch processing with daily/monthly updates.",GCP
https://github.com/nik19abramov/Soccer-Games-Analysis/tree/main,International Football Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs (soccer_analytics_pipeline) to orchestrate scheduled ETL jobs. The pipeline downloads data from Kaggle, processes it, loads it to BigQuery, and then runs dbt transformations. This is a classic batch processing pattern with periodic data pulls and scheduled transformations.",GCP
https://github.com/kabiromohd/Divvy-Analytics-Dashboard-Project,Divvy Bikeshare Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (e.g., zoomcamp-09_gcp_capstone1.yml, zoomcamp-09_gcp_scheduled_capstone1.yml) that define scheduled ETL jobs. The flows show batch processing patterns: extract data from CSV files, upload to GCS, then load/transform in BigQuery. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/VMynenko/air-route-analytics,US Airline Route Analytics Pipeline,Batch,"The repository uses Apache Airflow for workflow orchestration, which is a batch processing framework. The pipeline includes scheduled DAGs for data extraction, transformation (dbt), and loading to BigQuery, indicating periodic/batch data processing rather than continuous streaming.",GCP
https://github.com/Anran0716/DE-Project-Bikeshare,Philadelphia Bikeshare Analytics Pipeline,Batch,"The repository uses Apache Airflow with DAGs for scheduled ETL jobs, DBT for transformations, and Docker for containerization. The workflow is orchestrated through Airflow DAGs that run periodically, which is characteristic of batch processing.",GCP
https://github.com/clementlwm94/data-engineering-zoomcamp_final,Unknown,Unknown,No files fetched,Unknown
https://github.com/nozpera/Olist-E-Commerce-Insights,Olist E-Commerce Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML-defined flows (gcp_kv.yml, project-1.yml) that define scheduled ETL tasks including data extraction, transformation, and loading to BigQuery. The dbt project further confirms batch processing with transformation pipelines. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/GabrielArpini/SatelliteDataPipeline,Satellite Orbital Data Analytics Pipeline,Batch,"The repository uses Kestra, a workflow orchestrator, to run scheduled ETL jobs. The pipeline follows an ELT (Extract, Load, Transform) approach with automated batch processing through backfill executions and daily scheduled runs. The README explicitly states ""automated batch processing through backfill executions and daily scheduled runs"" and mentions ""automated data extraction, loading, and transformation"" using dbt.",GCP
https://github.com/NicolasImagawa/gsod-wind-data-project,USA Wind Speed Analytics Pipeline,Batch,"The project uses Airflow DAGs for workflow orchestration with scheduled ETL jobs (01_creating_entry_tables.py, 02_extraction_pipeline.py), dbt for data transformation, and batch processing patterns. The README explicitly states ""Data pipeline type: Batch"".",GCP
https://github.com/jugnuarora/DataTalks-de-zoomcamp-project,French Education Data Integration Pipeline,Batch,"The repository uses Kestra as a workflow orchestrator with Docker Compose, includes dbt for transformations, and has Python scripts for data ingestion using DLT Hub. The architecture shows scheduled ETL jobs with monthly snapshots and batch processing using Apache Spark, indicating a batch-oriented data pipeline.",GCP
https://github.com/greglenane/nyc_ems_calls.git,NYC EMS Incident Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (YAML files) to run periodic ETL jobs. The pipeline pulls data from an API weekly, transforms it with Python/pandas, loads to GCS, then to BigQuery, and runs dbt transformations. This is a classic batch processing pattern with scheduled data pulls and transformations.",GCP
https://github.com/Alisasu3/project-zoomcamp-2025#,Unknown,Unknown,No files fetched,Unknown
https://github.com/shawnanx/natality_zoomcamp,Maternal Age Birth Outcomes ETL,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (01_gcp_setup.yml, 02_gcp_bq.yml, 03_dbt_bq.yml) for batch data processing. The pipeline downloads annual CDC natality CSV files, uploads to GCS, loads to BigQuery, and runs dbt transformations - all batch operations triggered by date ranges rather than continuous streaming.",GCP
https://github.com/Viktorija-Alexeeva/DE-project,Airbnb Data Transformation Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs (YAML files defining tasks for data extraction, transformation, and loading). The pipeline runs monthly on a schedule, not continuously. DBT transformations are also batch-oriented. There are no streaming components like Kafka, Kinesis, or Flink.",GCP
https://github.com/chrisgilbert/de-zoomcamp-project,UK Vehicle Registration Data Pipeline,Batch,"The project uses Prefect for workflow orchestration with scheduled ETL jobs (cron schedule: 0 1 * * *), dbt for batch transformations, and DLT for batch data loading. The architecture follows a traditional ELT pattern with periodic data extraction from gov.uk datasets.",GCP
https://github.com/Maxkaizo/cdmx_ecobici_usage/tree/main,CDMX Ecobici Data Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (cron: ""0 7 1 1 *""), ETL tasks, and dbt transformations. The architecture diagram shows batch processing components like GCS, BigQuery, and Looker Studio. No streaming technologies like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/thedatadudech/data-engineering-zoomcamp/tree/project1/projects/TaxiDataPipeline,Urban Mobility Data Pipeline,Batch,"The repository contains a workflow orchestration setup using Kestra (kestra-flows.yml) that defines scheduled ETL jobs for ingesting taxi data from GCS to BigQuery. The pipeline includes batch data ingestion, transformation, and loading steps that run periodically rather than processing data in real-time streams.",GCP
https://github.com/jiejian925/CMS_Data_Engineer,CMS Research Payments Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs (GCP_ingestion_CMS_RSRCH, GCP_ingestion_CMS) to orchestrate scheduled ETL jobs that download, process, and load CMS Open Payments data. The workflow includes downloading zip files, converting to Parquet, uploading to GCS, and loading into BigQuery - all batch-oriented operations. dbt Cloud is also used for batch transformations.",GCP
https://github.com/Alessine/zurich_air_quality,Air Quality Batch Processing Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs that run daily. The architecture shows batch processing steps including data extraction, transformation, and loading to BigQuery, with no streaming components like Kafka or Flink present.",GCP
https://github.com/nathadriele/data-engineering-zoomcamp/tree/main/project-2025,Global Mental Health Analytics Pipeline,Batch,"The project uses Mage as a workflow orchestrator with scheduled ETL jobs. The pipeline follows a batch processing pattern: data is extracted from a CSV file, transformed using DBT, and loaded to DuckDB warehouse in scheduled runs. The configuration shows scheduled triggers and batch-oriented transformations rather than continuous streaming.",Other
https://github.com/DylanD-H/stock-market-analysis,US Stock Market Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (e.g., Weekly_Price_Data flow runs every Saturday at 8 AM UTC). The pipeline follows a batch processing model with periodic data pulls, transformations, and loads to BigQuery. The code shows scheduled flows, dbt runs, and periodic data updates rather than continuous streaming.",GCP
https://github.com/LilyTao123/financial_transaction_etl_airflow_dbt.git,Customer Behavior Analysis Warehouse,Batch,"The repository contains an Airflow DAG (etl_initialise_table.py) that orchestrates scheduled ETL jobs, including data ingestion, transformation, and loading to BigQuery. The workflow diagram shows batch-oriented steps like ""Ingestion DAG"" and ""DBT Transform DAG"" that run periodically rather than processing data in real-time.",GCP
https://github.com/ting-18/DEProject_StockMarketInsights/tree/main,Investment Insights Data Platform,Batch,"The project uses Apache Airflow with DAGs (daily_schedule_update_dag.py) to orchestrate scheduled ETL jobs. The pipeline extracts stock data daily, transforms it using dbt, and loads it to BigQuery. The architecture diagram and README explicitly mention ""daily batch processing"" and ""batch processing data"".",GCP
https://github.com/jakutyna/de-zoomcamp-2025-project,Met Museum Collection Data Pipeline,Batch,"The project uses Apache Airflow with a DAG (`met_museum_dag.py`) that runs scheduled ETL jobs daily. It orchestrates dlt for extract/load and dbt for transformations, which is a classic batch processing pattern. No streaming components (Kafka, Flink, etc.) are present.",GCP
https://github.com/30cannedpineapple/tate_art_project,Tate Art Collection Analytics Pipeline,Batch,"The project uses Airflow with DAGs (data_ingestion_gcs_dag) to orchestrate scheduled ETL jobs that pull data from GitHub, transform it, and load it to BigQuery. The architecture shows batch-oriented workflow management rather than continuous streaming.",GCP
https://github.com/semissatto2/us-natality-analysis,US Natality Analytics Pipeline,Batch,"The repository contains Apache Airflow DAGs (bigquery_to_gcs.py) that orchestrate scheduled ETL jobs. The pipeline follows a batch pattern: it queries BigQuery periodically, uploads data to GCS, processes with PySpark on Dataproc, and loads to BigQuery. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/HubertWong95/weather-analytics-pipeline,Weather Data Analytics Pipeline,Batch,"The repository contains Airflow DAGs (weather_gcs_to_bq.py) that orchestrate scheduled ETL jobs to move data from GCS to BigQuery and dbt transformations, indicating a batch processing pipeline.",GCP
https://github.com/mtayar25/FlightDelayAnalysis,Flight Delay Analytics Pipeline,Batch,"The project uses Mage for pipeline orchestration with scheduled ETL jobs (loadkaggleexportbq pipeline) and dbt for transformations, both characteristic of batch processing. Data is pulled periodically from Kaggle, transformed, and loaded to BigQuery.",GCP
https://github.com/taylorPat/parking-transactions-analysis.git,Urban Parking Demand Analysis,Batch,"The repository contains Terraform infrastructure for GCS and BigQuery, and the architecture diagram shows batch processing with Spark. No streaming components (Kafka, Flink, Kinesis) are present in the code or configuration files.",GCP
https://github.com/nikolai-neustroev/ab-test-lakehouse,Streaming A/B Test Data Lakehouse,"Batch, Streaming","The project implements both streaming and batch processing. Streaming: Uses Google Cloud Pub/Sub for real-time event streaming, Dataflow for continuous data ingestion from Pub/Sub to GCS. Batch: Uses Dataproc with PySpark for scheduled/batch processing of data files (CSV/JSON) stored in GCS, with data partitioning and transformation jobs. The architecture supports both real-time data flow and periodic batch processing workflows.",GCP
https://github.com/AmanAwadhiya/SoccerETL-Pipeline,International Football Analytics Pipeline,Batch,"The repository implements a batch ETL pipeline using Apache Airflow with Docker Compose for orchestration. The code includes Airflow DAGs (kaggle_toBigquery.py), scheduled tasks, and dbt transformations that run periodically to process historical football data. There are no streaming components like Kafka, Kinesis, or Flink present.",GCP
https://github.com/JOHNFFFEE/DE_Datacamp_Project2025,COVID-19 Vaccination Analytics Pipeline,Batch,"The repository uses Prefect workflow orchestrator with scheduled ETL jobs (ingestion_flow.py, dbt_flow.py) that pull data periodically from APIs, transform it using dbt, and load it to BigQuery. This is a classic batch processing pattern with scheduled DAGs/flows, not continuous real-time streaming.",GCP
https://github.com/Dakini/de-engineering-steam,Steam Game Analytics Pipeline,Batch,"The project uses Prefect for workflow orchestration with scheduled ETL jobs (daily cron schedules at 00:00, 00:10, and 00:30). The code includes three separate Prefect workflows: SteamIngest, SteamClean, and Dbtrun, which pull data from Steam APIs, clean it, and run dbt transformations. This is a classic batch processing pattern with periodic data pulls and transformations.",GCP
https://github.com/dakn2005/Plane_Incidents_Over_the_Century,Plane Incident Data Warehouse,Batch,"The repository uses Kestra workflow orchestration for automated data ingestion, which is a batch processing approach. The code shows scheduled ETL jobs that pull data periodically from the planecrashinfo website, transform it, and load it to BigQuery. The README explicitly mentions ""automated in a Kestra flow"" and describes the data pipeline as involving workflow orchestration rather than continuous streaming.",GCP
https://github.com/Rafiki00/Boston-crime-report/tree/master,Boston Crime Analytics Pipeline,Batch,"The repository uses Airflow DAGs to orchestrate scheduled ETL jobs that run periodically (e.g., dbt_build_mart.py, load_incidents.py, load_offense_codes.py). Data is pulled from GCS, transformed using dbt, and loaded to BigQuery in batch mode. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/andrea-leonel/BigBrotherBrasilAnalysis,Big Brother Historical Data Platform,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (e.g., bbb_gcp_ingest_contestants.yaml, bbb_gcp_setup_nominations.yaml) that run scheduled ETL jobs. The data pipeline follows a batch pattern: webscraping Python scripts collect data periodically, Kestra orchestrates the ingestion into BigQuery, and dbt transforms the data. There are no streaming components like Kafka, Kinesis, or Flink.",GCP
https://github.com/lFahadl/global-trade-analytics,Global Trade Analytics Pipeline,Batch,"The project uses Dagster as a workflow orchestrator with scheduled ETL jobs. The pipeline processes large CSV files in batch mode with a three-step approach (local to GCS, GCS to BigQuery, then transformations). The docker-compose setup runs Dagster webserver and daemon for batch job orchestration, not continuous streaming.",GCP
https://github.com/pyjom/Region-1-Cities-Economic-Index-Pipeline,Ilocos Cities Benchmarking Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (kestra_to_gcp_version4.yml) that define scheduled ETL tasks including data extraction, GCS upload, and BigQuery loading. The architecture follows a batch processing pattern with periodic data ingestion and transformation.",GCP
https://github.com/linhmai96/Global-Energy-Capacity-Dashboard/tree/main,Unknown,Unknown,No files fetched,Unknown
https://github.com/Jingyuan805/Amazon-Sales-DE-Zoomcamp2025.git,Amazon Sales Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML-defined flows (gcp_data.yaml, gcp_setup.yaml) that run scheduled ETL jobs. The pipeline extracts data from a static CSV file, uploads to GCS, loads to BigQuery, and runs dbt transformations - all batch-oriented operations. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/simonecarriero/data-pipeline-airbnb,Rome Airbnb Analytics Pipeline,Batch,"The repository uses Dagster for orchestration with scheduled ETL jobs that materialize assets periodically. The pipeline ingests data quarterly, transforms it through dbt models, and loads to BigQuery - all characteristic of batch processing. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/tasemgt/top-songs-de-pipeline,Billboard Top Songs Analytics Pipeline,Batch,"The repository contains Airflow DAGs (exampledag.py, music.py) that define scheduled ETL workflows. The pipeline ingests data from GCS to BigQuery, transforms it using dbt, and includes data quality checks - all orchestrated as batch jobs. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/ccppyos/house_violence_peru_etl,Peru Violence Data Analytics Pipeline,Batch,"The project uses Airflow with DAGs (proc_0_ingestion_to_s3_dag.py) to orchestrate scheduled ETL jobs. Data is pulled periodically from CSV files, transformed using EMR/Spark, and loaded to Redshift. The architecture diagram shows batch processing with workflow orchestration, not continuous streaming.",AWS
https://github.com/MediumSeaGreen/de-zoomcamp-berlin-bicycle-data,Bicycle Traffic Monitoring Data Pipeline,Batch,"The project uses Prefect as an orchestration tool to run scheduled ETL jobs that download, transform, and load data periodically. The pipeline includes workflow steps like downloading XLSX files, converting to Parquet, uploading to GCS, loading to BigQuery, and running dbt transformations - all characteristic of batch processing.",GCP
https://github.com/a920604a/data-engineering-zoomcamp-2025/tree/main/project,Open Source Trend Analysis,Batch,"The project uses Apache Airflow with DAGs (cloud_gharchive_dag) to orchestrate scheduled ETL jobs that download hourly GitHub Archive data, transform it to Parquet, and load to BigQuery. This is a classic batch processing pattern with periodic data pulls and transformations.",GCP
https://github.com/stefbp066/zoomcamp_capstone/,Weather Data Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with scheduled flows (ke-openmeteo-api_ingest.yml, ke_gcp_dbt.yml) that run daily at midnight. The pipeline follows a batch pattern: API data is pulled periodically, converted to Parquet, uploaded to GCS, then transformed with dbt. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/kostas696/data-engineering-zoomcamp/tree/main/project-okx-streaming-pipeline,Kafka Crypto Data Pipeline,"Batch, Streaming",The project contains both streaming and batch components. Streaming: OKX WebSocket API feeds data to Kafka producers/consumers for real-time processing. Batch: Airflow DAGs orchestrate scheduled Spark jobs for ETL transformations and data validation.,GCP
https://github.com/kevinhongzl/airbnb-de-project,Taipei Airbnb Analytics Pipeline,Batch,"The repository uses Apache Airflow with DAGs (gcp_elt.py, gcp_tear_down.py) for scheduled ETL/ELT workflows, includes dbt transformations, and has no streaming components like Kafka, Flink, or real-time processing.",GCP
https://github.com/SitaraJin/zoomcamp_project_2025/tree/main,Global Music Streaming Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML-defined flows (gcp_kv_setup.yaml, data_load_to_gcp.yaml) that run scheduled ETL jobs to extract data, load it to GCS, then to BigQuery, and transform with dbt. This is a classic batch processing pipeline with periodic data pulls and transformations.",GCP
https://github.com/rain2624/nypd_arrest_data_analysis,NYPD Arrest Data Analytics Pipeline,Batch,"The project uses Airflow with DAGs for scheduled ETL jobs, pulling data periodically from Socrata API and loading to GCS/BigQuery. The architecture diagram and docker-airflow setup confirm batch processing with scheduled monthly data ingestion.",GCP
https://github.com/pgdemiranda/de_capstone2025,ANEEL Electricity Tariff Analytics Pipeline,Batch,"The repository contains an Airflow DAG (`example_astronauts`) and uses Astronomer for orchestration, with dbt for transformations. The README explicitly states ""batch workflow"" and ""ELT pipeline"" with scheduled data processing, indicating batch-oriented data engineering rather than real-time streaming.",GCP
https://github.com/rassel25/Microsoft-Azure-Data-Engineering-Zoomcamp/tree/main,Yelp Business Analytics Pipeline,Batch,"The repository contains Terraform configuration for Azure Data Factory (ADF) with tumbling window triggers for scheduled batch data extraction from Cosmos DB to ADLS Gen2. The architecture diagram and documentation describe batch orchestration using ADF, with data lake storage and periodic extraction jobs. No streaming components like Kafka, Kinesis, or Flink are present.",Azure
https://github.com/neema233/Online-Retail-Data-Pipeline,Online Retail Analytics Pipeline,Batch,The pipeline uses Apache Airflow with a DAG (csv_s3_redshift.py) that runs daily to upload CSV files from local storage to S3 and then load them into Redshift using the COPY command. This is a classic batch processing pattern with scheduled ETL jobs.,AWS
https://github.com/compileandrun/deng25_project,Bitcoin Market Data Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled triggers (cron: ""*/10 * * * *"") to run periodic ETL jobs. The pipeline pulls data from Binance API every 10 minutes, transforms it using dlt and dbt, and loads it to BigQuery. This is a classic batch processing pattern with scheduled workflow execution.",GCP
https://github.com/adriangarciaestrada/hydrocarbons_dashboard,Energy Sector Data Warehouse Platform,Batch,"The repository uses Apache Airflow with DAGs (convert_excel_to_csv_dag.py, upload_gcs_to_gcs_dag.py, load_gcs_to_bigquery_dag.py) for scheduled ETL jobs, dbt for data modeling, and Docker for environment reproducibility. This is a classic batch processing architecture where data is pulled periodically, transformed, and loaded to BigQuery.",GCP
https://github.com/hannz88/world_happiness_project,Happiness Report BigQuery Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled YAML workflows (04_gcp_world_happiness_scheduled.yaml) that run ETL jobs periodically. The workflows include Python scripts for data extraction, GCS uploads, BigQuery queries, and dbt transformations - all characteristic of batch processing. The scheduled workflow runs at ""1 minute past 0000"" daily, confirming batch-oriented data pipeline.",GCP
https://github.com/bielacki/igdb-game-data,IGDB Game Analytics Pipeline,Batch,"The repository contains a batch-oriented data pipeline orchestrated by Apache Airflow with DAGs, uses dbt for batch transformations, and includes ETL jobs that run on schedules (e.g., daily, hourly). There are no streaming components like Kafka, Kinesis, or Flink.",GCP
https://github.com/AlbertPKW/Singapore-Residential-CarPark-Availability,Singapore Residential Parking Data Analytics,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs (YAML workflow files) that run periodically (every 3 hours for carpark availability, monthly for HDB info). Data is pulled, transformed, and loaded to BigQuery in batch mode. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/romanaumov/TradeMe,E-commerce Analytics on GCP,Batch,"The repository contains an Apache Airflow DAG (dags/etl_dag.py) that orchestrates a scheduled ETL workflow. The pipeline extracts data from TradeMe API, loads it to Google Cloud Storage, and then transforms/loads it to BigQuery. This is a classic batch processing pattern with periodic data pulls and scheduled job execution.",GCP
https://github.com/Ndblaze/DE-Zoomcamp/tree/main/Project,Los Angeles Traffic Collision Analytics Pipeline,Batch,"The project uses Airflow DAGs to orchestrate scheduled ETL jobs that download, transform, and load traffic collision data. The pipeline runs periodically (daily at 10 AM) and uses dbt for batch transformations, with no streaming components like Kafka or Flink present.",GCP
https://github.com/FeloXbit/Ethereum-Block-Analytics.git,Ethereum Network Performance Analytics,Batch,"The project uses Apache Airflow DAGs for workflow orchestration, dbt for scheduled transformations, and periodic ETL jobs. The architecture shows scheduled data ingestion, transformation, and loading to BigQuery, which is characteristic of batch processing. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/brukeg/lewis-hamilton-brilliance,Formula 1 Career Data Warehouse,Batch,"The repository uses Kestra as a workflow orchestrator with scheduled ETL jobs, dbt for transformations, and Terraform for infrastructure provisioning. The code shows a batch-oriented pipeline with ingestion scripts, scheduled transformations, and Looker dashboards for visualization. While Kestra can handle streaming, the configuration and structure indicate batch processing with scheduled data pulls and transformations.",GCP
https://github.com/dmytrovoytko/stock-market-data-engineering,Unknown,Unknown,No files fetched,Unknown
https://github.com/Juwon-Ogunseye/bitcoin-etl-pipeline,WBTC Blockchain Analytics Pipeline,Batch,"The repository uses Apache Airflow with DAGs (etl_dags.py, test_dag.py) to orchestrate scheduled ETL jobs. The pipeline runs daily with tasks for data extraction, loading to ClickHouse, and running dbt models. This is a classic batch processing architecture using workflow orchestrators.",AWS
https://github.com/SapientSapiens/capstoneproject-2025-dez,NYC Taxi Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (hourly_air_quality, daily_air_quality) to run periodic ETL jobs. The code shows scheduled data fetching from APIs, loading to GCS/BigQuery, and dbt transformations - all characteristic of batch processing rather than continuous streaming.",GCP
https://github.com/dmitrievdeveloper/de_project/tree/main/air_pollution,Unknown,Unknown,No files fetched,Unknown
https://github.com/3d150n-marc3l0/de-zoomcamp-2025-capstone-baywheels,Bay Wheels Data Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (docker-compose.yml, flows/*.yaml) that run ETL jobs periodically (e.g., monthly scheduled data loading). No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/hbg108/tfl-data-visualization/tree/main,TfL Footfall Data Transformation Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (e.g., 04_station_footfall_scheduled.yaml) to run periodic ETL jobs. Data is pulled from TfL sources, transformed, and loaded to BigQuery. The architecture includes dbt transformations and Looker Studio visualization, all characteristic of batch processing rather than continuous streaming.",GCP
https://github.com/victorbarreiros/brazil_covid_data,Ministry of Health ETL Pipeline,Batch,"The project uses Apache Airflow with DAGs (covid_pipeline.py) to orchestrate scheduled ETL jobs that run daily at midnight. The pipeline follows a batch processing pattern: download data, upload to GCS, load to BigQuery, and transform data in scheduled batches.",GCP
https://github.com/Deepcakex/zoomcamp_gym,Singapore Gym Capacity Pipeline,Batch,"The repository uses Kestra workflow orchestrator with scheduled ETL jobs (kestraflow_gym_scraper.yml and kestraflow_sg_rainfall.yml) that run at specific intervals (every 30 minutes for gym data, daily for rainfall data). The code shows batch processing patterns with Python scripts that extract, transform, and load data to BigQuery at scheduled times.",GCP
https://github.com/Hexagon9099/website_logs,E-commerce Log Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (e.g., weekly batch processing), dbt for transformations, and Spark for ETL jobs. The architecture diagram and README explicitly state ""batch process runs on a weekly schedule."" There are no streaming components like Kafka, Flink, or real-time processing.",GCP
https://github.com/alvarovs89/data-eng-ultra-marathon,Ultra-Running Data Processing Platform,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs (YAML flow definitions) that run periodically to download data from Kaggle, transform it using Spark on Dataproc, and load to BigQuery. This is a classic batch processing pipeline with discrete job runs rather than continuous streaming.",GCP
https://github.com/tomojpin1234/NYC-Bus-Delays-Weather-Analysis,School Bus Performance Data Warehouse,Batch,"The project uses Apache Airflow with DAGs to orchestrate scheduled ETL jobs that run periodically (daily). The pipeline includes: 1) Fetching raw data from APIs, 2) Processing with PySpark, 3) Loading to BigQuery, 4) dbt transformations. This is a classic batch processing architecture with workflow orchestrators, not continuous streaming.",GCP
https://github.com/victorKatemana/Ebola-Data-Engineering-Pipeline,Ebola Outbreak Data Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with scheduled flows (cron: ""0 0 * * *"") to run ETL jobs. The pipeline downloads data, uploads to GCS, loads to BigQuery, and runs dbt transformations - all batch-oriented operations. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/yassersakr88/DE-Zoomcamp-project/tree/main,Citi Bike Data Transformation Pipeline,Batch,"The project uses Apache Airflow with DAGs (dlt_ingestion_dag.py, upload_new_data.py) for scheduled ETL jobs, dbt for batch transformations, and Terraform for infrastructure management. All components follow a batch processing pattern with periodic data pulls and transformations.",GCP
https://github.com/LolloPero/Github-activities-visualization/tree/main,GitHub Activity Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with YAML flow definitions (github_activities_ingestion.yaml) that define scheduled ETL tasks including data extraction, GCS upload, and BigQuery operations. The pipeline processes GitHub archive files in batch mode rather than streaming.",GCP
https://github.com/vareha/data_engineering_zoomcamp_project/,AIS Ship Tracking Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (ais_data_extraction_and_load.yaml, ais_data_transformation.yaml) that define scheduled ETL jobs. The architecture includes dbt for batch transformations and scheduled data loading from monthly CSV files. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/kkhatke/Fuel-n-Fleet/tree/main,Fuel Type Registration Insights,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (scrape_vahan_registrations_scheduled.yml) and dbt for batch transformations. The pipeline scrapes data periodically, stores it in GCS/BigQuery, and runs dbt models for transformation - all characteristic of batch processing rather than continuous streaming.",GCP
https://github.com/lingxin0812/DE-Zoomcamp-Project.git,Unknown,Unknown,No files fetched,Unknown
https://github.com/Delphin1/data-engineering-project,Real-Time Stock Analytics Pipeline,Streaming,"The repository implements a streaming data pipeline using Kafka (Confluent Cloud) with a tick generator producing continuous market data, ksqlDB for real-time processing, and Trino for querying. The architecture shows data flowing from Kafka topics to Iceberg tables with minimal delay for candlestick aggregation.",Other
https://github.com/khushal2911/GD-ETL-Flow-Batch_Data_Pipeline,GDELT Global Events Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with scheduled flows for batch ETL processing of GDELT data, along with dbt for batch transformations. The architecture diagram and documentation explicitly describe it as a ""batch data pipeline"" that processes CSV files periodically rather than streaming data.",GCP
https://github.com/Pro100v/air-quality-pipeline,Air Quality Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (kestra/flows/*.yml) and dbt for batch transformations. The pipeline extracts data periodically from IQAir API using dlt, loads it to BigQuery, and transforms it with dbt - all batch operations. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/1412010/nda-de-zoomcamp-project,PostgreSQL to BigQuery ETL Pipeline,Batch,"The repository contains Airflow DAGs (dag_silver_orders.py, dag_silver_products.py, dag_gold_orders.py) that orchestrate scheduled ETL jobs. The architecture follows a batch processing pattern with Medallion Architecture (Bronze -> Silver -> Gold layers) where data is ingested periodically from GCS, transformed using Dataproc/PySpark, and loaded to BigQuery. No streaming components (Kafka, Flink, Kinesis) are present.",GCP
https://github.com/josh-monto/dob-permits,NYC Construction Permit Analytics Pipeline,Batch,"The project uses Apache Airflow with a Python DAG (data_ingestion_gcs_dag.py) that orchestrates scheduled ETL jobs to pull data daily from the NYC Open Data API, transform it, and load it to BigQuery. The code shows workflow orchestrators (Airflow), scheduled tasks, and batch processing patterns rather than continuous streaming.",GCP
https://github.com/cancinoray/stackoverflow-data-pipeline,Developer Technology Trends Data Warehouse,Batch,"The repository uses Apache Airflow with DAGs (web_to_gs_pipeline.py, main_dags.py) to orchestrate scheduled ETL jobs. The pipeline includes scraping data, processing with PySpark, loading to BigQuery, and dbt transformations - all typical batch processing patterns with scheduled execution.",GCP
https://github.com/aafaf655/DE-Job-Market-Analysis,DE Job Market Insights Platform,Batch,"The repository uses Kestra workflow orchestrator with flow definitions (kestra/flows) and dbt for transformations, indicating a batch ETL pipeline. Data is collected periodically via scheduled workflows, transformed using dbt, and loaded to BigQuery. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/Ruidozo/TFL-Historical-Accidents-05_19,Historical Traffic Incident Data Pipeline,Batch,"The project uses Apache Airflow with DAGs for scheduled ETL jobs, dbt for data transformations, and dlt for batch data loading. The docker-compose shows Airflow webserver and scheduler services, indicating a batch-oriented workflow orchestrator rather than streaming components like Kafka or Flink.",GCP
https://github.com/50kuai/Dataengineerzoomcamp2025/tree/main/stock-market,Daily Portfolio Performance Analytics,Batch,"The project uses Apache Airflow with DAGs (transaction.py) to orchestrate scheduled ETL jobs that run daily to fetch stock data, generate transactions, and load data to BigQuery. The architecture shows a traditional batch processing pipeline with scheduled data ingestion and transformation using dbt.",GCP
https://github.com/bargavpec/Global_Fashion_Retail_Sales_Analysis,Global Fashion Retail Analytics Pipeline,Batch,"The project uses Airflow DAGs to orchestrate scheduled ETL jobs that extract data from Kaggle, load it to Google Cloud Storage, transform via Dataproc/Spark, and load to BigQuery. This is a classic batch processing pattern with periodic data pulls and transformations.",GCP
https://github.com/Qaladid/sales-analytics-pipeline,Customer Behavior Analytics,Batch,"The repository uses Kestra for workflow orchestration with scheduled ETL jobs, dbt for batch transformations, and DLT for batch data loading from SQL Server to Snowflake. The architecture follows a traditional batch ETL pattern with scheduled runs rather than continuous streaming.",Other
https://github.com/OlgasAcc/de-zoomcamp-project,Flight Delay Analytics Pipeline,Batch,"The repository uses Apache Airflow with DAGs (airline_delay_pipeline.py) to orchestrate scheduled ETL jobs. The pipeline includes: 1) Airflow DAGs for workflow orchestration, 2) dbt transformations for data modeling, 3) Spark jobs for batch processing, 4) Periodic data pulls from Kaggle and Postgres. No streaming components (Kafka, Flink, Kinesis) are present.",GCP
https://github.com/krish-rm/plant-health-analytics/,Unknown,Unknown,No files fetched,Unknown
https://github.com/cl3rO3Y/road-traffic-injuries,Road Traffic ETL Pipeline France,Batch,"The project uses Kestra workflow orchestrator to run scheduled ETL jobs that process annual CSV files from data.gouv.fr. The pipeline involves downloading files, uploading to GCS, transforming via dbt, and loading to BigQuery - all batch-oriented operations with no streaming components detected.",GCP
https://github.com/cmurph255/de-zoomcamp-project-25,U.S. County Unemployment Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (e.g., dbt_build.yaml, unemployment_reference_data.yaml) that define scheduled ETL tasks. The flows include data ingestion, transformation, and dbt build operations, which are characteristic of batch processing rather than continuous streaming.",GCP
https://github.com/wangjenn/london-cycling-analytics,London Cycling Analytics Pipeline,Batch,"The repository contains a requirements.txt with dbt-core, dbt-postgres, and dbt-bigquery for data transformation, along with pandas and numpy for data processing. The README describes a methodological approach using strategic sampling of specific time periods rather than continuous data flow. There are no streaming components like Kafka, Kinesis, Flink, or real-time processing frameworks present in the codebase.",GCP
https://github.com/jxu71748/f1_de_pipeline,Formula 1 Race Data Warehouse,Batch,"The project uses Airflow DAGs to orchestrate scheduled ETL jobs (f1_ingestion_dag.py and f1_transform_dag.py) that download data from Kaggle, upload to GCS, and transform using PySpark. The schedule_interval is set to None for manual triggering, indicating batch processing rather than continuous streaming.",GCP
https://github.com/MichaelSalata/compare-my-biometrics,Fitbit Biometric Analytics Pipeline,Batch,"The repository uses Apache Airflow with DAGs to orchestrate scheduled ETL jobs, including data extraction from Fitbit API, transformation to Parquet, upload to GCS, and SQL transformations via dbt. This is a classic batch processing pipeline with periodic data pulls and scheduled transformations.",GCP
https://github.com/ayoubchaoui/data-engineering-zoomcamp/tree/main/Project,Toronto Crime Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (01_gcp_kv, 02_gcp_setup, 03_gcp_bucket, 04_gcp_bigQuery, 05_gcp_dbt) that run sequentially to process data. The pipeline downloads CSV files, uploads to GCS, loads to BigQuery, and runs dbt transformations - all batch operations triggered by the orchestrator.",GCP
https://github.com/DasariRishi/citibike_zoomcamp,Citi Bike Trip Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with YAML files defining scheduled ETL jobs that extract data monthly from CitiBike, load it to Google Cloud Storage and BigQuery, and transform it with dbt. This is a classic batch processing pattern with periodic data pulls and transformations.",GCP
https://github.com/mrvneslihan/Data-Pipeline-for-E-commerce-Customer-Segmentation/tree/main,E-commerce Customer Segmentation Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (gcp_setup.yaml, loading_data.yaml) that define scheduled tasks for data extraction, transformation, and loading. The architecture shows batch-oriented components like BigQuery, dbt transformations, and PowerBI reporting rather than continuous streaming.",GCP
https://github.com/sara-soomro/Project,Walmart Sales Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (gcp_kv.yaml, Spark-gcp.yaml, GCP-BigQuery.yaml) to run scheduled ETL jobs. Data is pulled from Kaggle API periodically, transformed using Spark/DBT, and loaded to BigQuery. The architecture shows batch-oriented components: workflow orchestrators, scheduled tasks, and dbt runs rather than continuous streaming with message brokers.",GCP
https://github.com/klimantje/de_zoomcamp_2025_project,Journey and Station Data Warehouse,Batch,"The repository uses Kestra as a workflow orchestrator to run scheduled ETL jobs, including Python scripts for data ingestion and dbt for transformations. The architecture shows batch processing patterns with scheduled flows and incremental dbt runs, not continuous streaming.",GCP
https://github.com/codac-black/metro-bikeshare.git,Metro Bikeshare Analytics Pipeline,Batch,"The repository contains Apache Airflow DAGs for scheduled ETL workflows, dbt transformations, and Terraform infrastructure for EC2 deployment. All components follow a batch processing pattern with periodic data extraction, transformation, and loading rather than continuous streaming.",AWS
https://github.com/thanhdatnguyen2604/Rhythmic-ETL/,Kafka Music Event Processing System,"Batch, Streaming","The repository contains both streaming and batch components. Streaming: Kafka cluster with Eventsim for real-time event generation, Flink jobs for continuous stream processing of listen_events, page_view_events, and auth_events. Batch: Airflow DAGs for orchestrating ETL and analytics tasks, with dbt for data transformation. The architecture explicitly states it processes both streaming and batch data.",GCP
https://github.com/Guanyi-Lu/Final-Project/tree/main,Mortality Data Analytics Pipeline,Batch,"The repository contains a Cloud Composer (Airflow) environment with scheduled DAGs for ETL processing, indicating batch-oriented data pipelines rather than continuous streaming.",GCP
https://github.com/sntk-76/Retail-Data-Pipeline,Sales Data Processing Infrastructure,Batch,"The repository contains Apache Airflow DAGs (GCS_to_bigquery_clean_data.py, GCS_to_bigquery_raw_data.py) that orchestrate scheduled ETL workflows for batch processing. The pipeline architecture diagram and README confirm this is a batch processing system with workflow orchestration, not streaming.",GCP
https://github.com/tuanlearning/spotify,Track Analytics Batch Pipeline,Batch,"The project uses Airflow with DAGs to orchestrate scheduled ETL jobs that extract data from Spotify API, stage it in GCS, load to BigQuery, and transform with dbt. This is a classic batch processing pipeline with periodic data pulls and scheduled transformations.",GCP
https://github.com/Katrindenek/de-zoomcamp-2025/tree/main/Project,Data Engineering News Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML-defined flows (extract_news.yml, news_transform_dbt.yml) that run scheduled ETL jobs. The architecture shows NewsAPI data being pulled periodically by dlt, orchestrated by Kestra, then transformed by dbt - all characteristic of batch processing. No streaming components like Kafka, Flink, or real-time message brokers are present.",GCP
https://github.com/urbanclimatefr/de-zoomcamp-2025-project-attempt2/tree/main,Hong Kong Weather Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled batch processing (5-minute intervals) to fetch API data, transform with Python pandas, and load to BigQuery. The architecture shows batch ETL jobs rather than continuous streaming.",GCP
https://github.com/dennismunene-24d/citi-bike-data-pipeline-capstone-project,Citi-Bike Data Processing System,Batch,"The repository contains Apache Airflow DAGs (citibike_data_pipeline, merge_citibike_data, process_citibike_data) that orchestrate scheduled ETL jobs to download, merge, and process Citibike trip data. The pipeline uses workflow orchestrators (Airflow) with scheduled tasks and batch processing patterns, not continuous streaming components like Kafka or Flink.",GCP
https://github.com/Alexander-Heinz/Berliner-Luft,Berlin Pollution Health Metrics Dashboard,Batch,"The repository contains a Cloud Function triggered by Cloud Scheduler (cron-based) that pulls data from the Umweltbundesamt API and loads it to BigQuery. This is a scheduled batch ETL pattern, not continuous streaming. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/Assitan-h/Secondhand_LuxurybrandSalesAnalysis,Secondhand Luxury Sales Analytics Pipeline,Batch,"The repository contains Airflow DAGs (Data_ingestion_airflow_GCS.py, dataproc_data_cleaning.py) that orchestrate scheduled ETL jobs using batch processing with Dataproc/Spark. The pipeline processes CSV files from GCS in batches and loads them to BigQuery on a monthly schedule. No streaming components (Kafka, Flink, Kinesis) are present.",GCP
https://github.com/thatlaconic/Airline-On-Time-Performance-Data.git,Flight Delay Analysis Platform,Batch,"The repository contains a Python ETL script for data extraction and loading, Terraform for infrastructure provisioning, and dbt models for transformation. The dbt README shows commands like ""dbt run"" and ""dbt test"", which are typical of batch processing workflows. There are no streaming components like Kafka, Kinesis, or Flink in the codebase.",GCP
https://github.com/GameRuiner/Indian-Traffic-Violation-Insights,Indian Traffic Data Analytics Platform,Batch,"The project uses Apache Airflow with a DAG (s3_to_postgres_pipeline.py) that orchestrates scheduled ETL jobs to process traffic violation data monthly. The pipeline downloads data from S3, transforms it, and loads it to PostgreSQL, which is a classic batch processing pattern.",AWS
https://github.com/Marcoc51/Brentford-Data-Analytics,Brentford FC Analytics Pipeline,Batch,"The repository uses Apache Airflow with DAGs (Bronze to Silver, DLT to Snowflake, Full Pipeline) that run on daily schedules, indicating batch processing. The pipeline follows an ELT pattern with scheduled jobs rather than continuous streaming.",Other
https://github.com/lq27/bluebikes_dashboard,Bluebikes ETL and Dashboard,Batch,"The project uses Kestra as a workflow orchestrator with scheduled ETL jobs that download data from S3, perform data cleaning/standardization, upload to GCP storage, and run BigQuery transformations. The docker-compose.yaml shows Kestra configured for batch processing workflows rather than streaming.",GCP
https://github.com/linhfk/Zoomcamp2025/tree/main/Global%20Product%20Inventory,Global Product Inventory Analytics Pipeline,Batch,"The repository contains a Mage AI project with data pipeline configurations (connections.yml, global_hooks.yml, pipeline.yaml) and a Docker Compose setup for running scheduled ETL jobs. Mage AI is a workflow orchestrator that runs batch data pipelines on a schedule, not a streaming system.",Other
https://github.com/victoria22/Retail-Intelligence-Platform,Online Retail Intelligence Platform,Batch,"The repository uses Apache Airflow with DAGs (Online_Retail_Data_etl.py) to orchestrate scheduled ETL jobs. The workflow includes DataprocSubmitJobOperator tasks for extraction and transformation, with a monthly schedule interval (@monthly). This is a classic batch processing pattern where data is pulled periodically, transformed, and loaded to BigQuery.",GCP
https://github.com/krish-rm/plant-health-dashboard,Plant Health Analytics Pipeline,Batch,"The repository contains Apache Airflow DAGs (dags/plant_health_dag.py) that orchestrate scheduled ETL jobs. The pipeline follows a batch pattern: data is collected from IoT sensors, stored in Cloud Storage, then Airflow DAGs trigger BigQuery SQL transformations and dbt runs on a schedule. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/ayoubchaoui/data-engineering-zoomcamp/tree/main/Projcet%202/meto_analytics_engineering,Unknown,Unknown,No files fetched,Unknown
https://github.com/BlvckOgre/football-stadiums-of-world/tree/main,Wikipedia Football Data Scraper,Batch,"The project uses Apache Airflow with DAGs (wikipedia_flow.py) to orchestrate scheduled ETL jobs that pull data from Wikipedia, transform it, and load it to storage. This is a classic batch processing pattern with workflow orchestrators running periodic jobs.",Azure
https://github.com/Amir380-A/Data-project,International Fashion Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs to orchestrate scheduled ETL jobs, including PySpark batch processing jobs. The pipeline follows a traditional batch-oriented workflow: ingest data periodically, transform via Spark jobs, validate, and load to warehouse. No streaming components like Kafka, Kinesis, or Flink are present.",AWS
https://github.com/carolinelile/DE_Zoomcamp_Final_Project/tree/main,Unknown,Unknown,No files fetched,Unknown
https://github.com/ninja-con-gafas/portfolio-analyzer,Unknown,Unknown,No files fetched,Unknown
https://github.com/Deljimae/Power_Plant_Data_Pipeline,Global Power Plant Analytics Pipeline,Batch,"The repository uses Kestra, a workflow orchestrator, to run scheduled ETL jobs. The code shows a Kestra flow that downloads a CSV file, uploads it to S3, and creates Athena tables - all batch operations. There are no streaming components like Kafka, Kinesis, or Flink present.",AWS
https://github.com/timosii/de_project,Patient Data Quality Analytics,Batch,"The repository contains Airflow DAGs (daily_data_ingestion and daily_dwh_update) that run scheduled ETL jobs, uses dbt for batch transformations, and has no streaming components like Kafka or Flink.",Other
https://github.com/yoyedmundyoy/f1-analytics,Formula 1 Data Transformation Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (cron triggers) to run ETL jobs. The flows are defined in YAML files with cron schedules (e.g., ""@weekly"") and execute Python scripts for data ingestion. This is a classic batch processing pattern where data is pulled periodically from APIs, transformed, and loaded to BigQuery.",GCP
https://github.com/OnurKerimoglu/stocks_analytics,ETF Data Transformation Pipeline,Batch,"The repository contains Airflow DAGs (etf_forecasts_dag.py, etf_transformations_dag.py) that orchestrate scheduled ETL jobs. The code shows scheduled data fetching, transformation, and loading to BigQuery, which is characteristic of batch processing. There are no streaming components like Kafka, Kinesis, or Flink found in the code.",GCP
https://github.com/wojsamjan/DE-Zoomcamp-2025,Toronto Crime Analytics Pipeline,Batch,"The repository contains Terraform infrastructure for GCS and BigQuery, and the README indicates a PySpark pipeline on Dataproc that processes data and loads it to BigQuery. This is a batch-oriented ETL workflow, not a continuous streaming pipeline.",GCP
https://github.com/ochekayjay/europeCattleExplorationProject,UK Farm Data Transformation Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (mainflow.yml, projectOnboard.yml) that define scheduled ETL jobs for batch data processing. The README explicitly states this is a ""batch data pipeline"" with ""batch data ingestion using python script in kestra"" and mentions ""Static dataset (one-time load)"" indicating batch processing rather than streaming.",GCP
https://github.com/larsvasseldonk/ticketmaster-de-pipeline,Ticketmaster Event Data Analytics Pipeline,Batch,"The project uses Google Cloud Scheduler to trigger a Cloud Function daily at midnight, which extracts data from Ticketmaster API and loads it into BigQuery. This is a scheduled batch ETL process, not continuous streaming. The dbt models further confirm batch transformation patterns.",GCP
https://github.com/vedantjangid/Cap-Project-DE/tree/main,India Air Quality Analytics Pipeline,Batch,"The repository contains a Mage workflow orchestrator with scheduled ETL jobs (trigger.py), Docker Compose setup for Mage, and dbt transformations. The pipeline processes air pollution data daily from OpenWeather API, stores it in GCS, then transforms it with dbt for BigQuery. This is a classic batch processing architecture with scheduled data pulls and transformations.",GCP
https://github.com/DFDuke/de-zoomcamp-project,Urban Bike Data Processing Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions for batch ETL jobs. The flows download ZIP archives, decompress them, load to GCS, and merge into BigQuery. There are no streaming components like Kafka, Kinesis, or Flink.",GCP
https://github.com/philvukovic/coinbase-dashboard,Real-Time Cryptocurrency Data Pipeline,"Batch, Streaming","The repository contains both batch and streaming components. The producer script uses APScheduler for periodic data collection (batch), while the infrastructure includes Kafka for real-time message streaming. The consumer processes data from Kafka topics continuously, indicating streaming architecture. The README also explicitly mentions ""batch processing setup"" while the code implements Kafka-based streaming.",GCP
https://github.com/tsk93/DE-zoomcamp-project,English Premier League Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled triggers (cron: ""0 16 * * *"") and script-based ETL tasks. The architecture shows data is downloaded, transformed via Python/dbt, and loaded to BigQuery in periodic batches, not continuous streaming.",GCP
https://github.com/AIZharau/coinbase-realtime-analytics/tree/main,Kafka Cryptocurrency Streaming Analytics,Streaming,"The repository implements a real-time streaming pipeline using Redpanda (Kafka-compatible) for continuous data flow. It includes websocket producers streaming Coinbase market data to Redpanda topics, and consumers that process data in real-time to ClickHouse. The architecture diagram and documentation confirm this is a streaming system with continuous data ingestion and processing.",Other
https://github.com/rvonahn/de_zoomcamp_project,Spotify Top Songs Analytics,Batch,"The project uses Kestra workflow orchestrator with docker-compose.yml defining Kestra services, and includes dbt for batch transformations. The Readme explicitly states ""The pipeline is Batch and would be loaded as new sets become available.""",GCP
https://github.com/Mohamedelrokh/nyc-taxi-project,NYC Taxi Data Streaming Pipeline,"Batch, Streaming","The repository contains both streaming and batch pipeline components. Streaming components include Kafka producers/consumers, Confluent Kafka topics, and real-time data processing with PostgreSQL. Batch components include Google Dataflow for ETL, BigQuery for data warehousing, and scheduled processing workflows.",GCP
https://github.com/saraisab/Amazon_project_DE_saraisab,Product Sales ETL Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (.yml files) that define scheduled ETL jobs. The flows show batch-oriented tasks like downloading datasets, processing CSV files, and loading to BigQuery in discrete steps. There are no streaming components like Kafka, Kinesis, or Flink mentioned.",GCP
https://github.com/Giko20/DataTalks-Data-Engineering-Project,Kafka Web Scraping Pipeline,"Batch, Streaming","The project implements both batch and streaming components. Batch processing is orchestrated by Apache Airflow DAGs that run scheduled ETL jobs to extract data from websites, transform it, and load it to PostgreSQL and BigQuery. Streaming is implemented using Kafka for real-time data flow from Python Dataframe and PostgreSQL to BigQuery, with Kafka producers and consumers handling continuous data processing.",GCP
https://github.com/zashee/de-zoomcamp-hdb-analytics,HDB Resale Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs (Dockerfile, docker-compose.yaml, ingest_hdb_resale_2017_onwards.py) to orchestrate scheduled ETL jobs. The architecture shows data being pulled periodically from APIs, transformed, and loaded to BigQuery. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/omarbadr1/TFL-Data-Pipeline,TFL Journey Patterns ETL,Batch,"The repository uses Kestra workflow orchestrator with YAML-defined flows (key_setup.yml, gcp_scheduled_upload.yml) that run scheduled ETL jobs to extract data from TfL's dataset, upload to GCS, and load into BigQuery. The dbt project further transforms the data in batch mode. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/leb-jones/pdf_indexer_automation,Google Cloud NLP Indexer,Batch,"The project uses Kestra workflow orchestrator (kestra/application.yml, docker-compose.yml) to run scheduled ETL jobs that process PDFs, extract keywords, and load data to BigQuery. The architecture follows a batch processing pattern with workflow definitions rather than continuous streaming.",GCP
https://github.com/y0ussefElbrini/Movie-Data-Pipeline-with-AI-Powered-Mood-Analysis,Movie Mood Analysis Pipeline,Batch,"The project uses Airflow DAGs (json_to_gcs_to_bq.py) to orchestrate scheduled ETL jobs that pull data from TMDb API, transform it, and load it into BigQuery. The pipeline runs periodically rather than processing data in real-time streams.",GCP
https://github.com/leoimewore/airflow_docker,Chicago Traffic Crash Analytics Pipeline,Batch,"The repository contains Airflow DAGs for scheduled ETL jobs (create_bq_crashes.py, create_delete_cluster.py, dbt_dag.py) that process data in batch mode using Dataproc clusters and BigQuery operations. No streaming components like Kafka, Flink, or Kinesis are present.",GCP
https://github.com/venusieong/yelp-data-pipeline/tree/master,Yelp Open Dataset Pipeline,Batch,"The repository contains Apache Airflow DAGs (yelp_pipeline_web_to_gcs.py, yelp_pipeline_gcs_to_bq.py, yelp_business_json_to_bq.py) that orchestrate scheduled ETL jobs for batch processing. The README explicitly states ""It will be a batch processing pipeline that can be scheduled and run end to end."" There are no streaming components like Kafka, Kinesis, or Flink.",GCP
https://github.com/Joyan9/Berlin-Data-Engineering-Jobs-Analytics/tree/main,Berlin Data Engineering Career Analytics,Batch,"The project uses Apache Airflow with DAGs to orchestrate scheduled ETL jobs that extract data periodically, transform it using dbt, and load it to BigQuery. The architecture is batch-oriented with workflow orchestrators and scheduled tasks.",GCP
https://github.com/alexanderlazutkin/DEZC2025proj,Street Crime Data Platform,Batch,"The project uses Kestra as a workflow orchestrator with declarative YAML flows to schedule and run ETL jobs. Data is extracted from Police UK API archives, transformed using dbt, and loaded to DuckDB/Minio. No streaming components like Kafka, Flink, or real-time processing are present.",Other
https://github.com/batxes/Berlin-Weather-Project,Weather Data ETL Platform,Batch,"The project uses Apache Airflow with DAGs (data_ingestion_dag.py) for scheduled ETL jobs, dbt for batch transformations, and workflow orchestrators. The Airflow DAG runs on a daily schedule (@daily) to pull data from Kaggle, process it, and load to BigQuery.",GCP
https://github.com/rodriguesmafalda/de-zoomcamp/tree/main/project,Seattle Public Life Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (gcp_public_life_data_load.yaml) that define sequential tasks for data ingestion, transformation, and loading. This is a batch-oriented ETL pipeline with scheduled or triggered execution, not continuous streaming.",GCP
https://github.com/Hab00119/Dezoomcamp-RealVoting,Kafka Voting Data Stream,"Batch, Streaming","The project implements both streaming and batch processing. Streaming components include Redpanda (Kafka), Flink jobs for real-time vote processing, and Kafka consumers. Batch components include Airflow DAGs for scheduled ETL jobs, dbt transformations, and Spark batch processing jobs. The Makefile shows separate targets for start-streaming and start-batch operations.",GCP
https://github.com/ChenJun-42/AISalaryPipeline,AI/ML Salary Analytics Pipeline,Batch,"The project uses Airflow with DAGs (datazoomcamp_pipeline.py) to orchestrate scheduled ETL jobs that run periodically to download, transform, and load salary data. The README explicitly states ""batch processing"" was implemented since the dataset is updated weekly.",Other
https://github.com/arjunrarjunr/de-zoomcamp-final-2025,Global Weather Data Pipeline,Batch,"The pipeline uses scheduled AWS Lambda (daily 6AM IST via CloudWatch) and scheduled dbt Cloud runs (daily 8AM IST) to process data in batches. No streaming components like Kafka, Kinesis, or Flink are present.",AWS
https://github.com/Haoshka/hao-de-project-sol-price,Solana Price Analytics Pipeline,Batch,"The project uses scheduled ETL jobs orchestrated by Kestra (job_submit.yaml with monthly cron schedule) and Spark jobs submitted via Dataproc to fetch and process Solana price data in batches. The workflow is triggered monthly to retrieve last month's data, indicating batch processing rather than continuous streaming.",GCP
https://github.com/Dkaattae/annual_quarter_report_and_stock_price,Business Filings Word Analysis Platform,Batch,"The repository uses Kestra workflow orchestrator with YAML flow definitions (flow/ folder) that define scheduled ETL jobs for stock price updates, XBRL data loading, and document extraction. The architecture shows batch processing patterns with scheduled monthly runs and periodic data pulls, not continuous streaming.",GCP
https://github.com/polina-fuksman/data-engineering-zoomcamp/tree/main/project_1,Payment Fraud Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (03_gcp_payments.yaml) that define scheduled ETL jobs. The data pipeline downloads CSV files, processes them, and loads to warehouse in batch mode. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/rasj81/MeteoriteAnalytics,Meteorite Landing Data Analytics,Batch,"The project uses Kestra workflow orchestrator with YAML flow files (1-SetupKeystore.yaml, 2-SetupEnvironment.yaml, 3-IngestData.yaml, 4-DeleteEnvironment.yaml) to run scheduled ETL jobs. The README explicitly states ""Kestra is used to orchestrate the environment and data pipeline"" and mentions that ""Streaming is not included due to the way the data is published"". The data is extracted from a static CSV file and loaded into BigQuery in batch mode.",GCP
https://github.com/akEliza/Billboard_dezoomcamp2025/tree/main,Billboard Data Transformation Pipeline,Batch,"The repository contains an Airflow DAG (gcs_to_bigquery.py) that orchestrates scheduled ETL jobs to load data from GCS to BigQuery, which is a batch processing pattern. The dbt project also indicates batch transformation workflows.",GCP
https://github.com/kpivert/dezc_2025_project/tree/main?tab=readme-ov-file,Medicare Provider Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with docker-compose.yml defining scheduled ETL jobs. The pipeline involves downloading CSV files, uploading to Google Cloud Storage, and loading into BigQuery with dbt transformations - all characteristic of batch processing rather than continuous streaming.",GCP
https://github.com/alepanti/ChiTrafficInsights,Chicago Traffic Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled tasks (daily cron) to run ETL jobs. The pipeline follows a batch pattern: API data extraction, CSV loading to GCS, dlt loading to BigQuery, and dbt transformations. The architecture diagram and README both describe it as a ""batch processing data pipeline"" with daily scheduled execution.",GCP
https://github.com/BigDatalex/rewe_products,REWE Grocery Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with a defined flow (rewe_flow.yml) that runs scheduled ETL jobs: downloading JSON files from API, loading to BigQuery via dlt, and transforming with dbt. This is a classic batch processing pipeline with periodic execution.",GCP
https://github.com/shukew2/Video-Game-Sales-Anaylsis,Video Game Sales Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with scheduled DAGs (cron triggers) to run batch ETL jobs. The pipeline downloads yearly parquet files, uploads to GCS, loads into BigQuery raw tables, and inserts into partitioned tables. This is a classic batch processing pattern with scheduled execution and file-based data ingestion.",GCP
https://github.com/victorfxz/Mental-Well-being-Monitor,Global Mental Health Analytics Pipeline,Batch,"The repository implements a batch data pipeline using Mage-AI for workflow orchestration, with scheduled ETL jobs that extract data from CSV, load to Azurite (local Data Lake), transform with DBT, and load to DuckDB. The architecture follows a traditional batch processing pattern with discrete pipeline runs rather than continuous streaming.",Azure
https://github.com/PK109/e-commerce-workflow/tree/main,Order Processing Data Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (e.g., batch_extract.yml, extract_raw_data.yml) that define scheduled ETL tasks. The architecture shows periodic data extraction, transformation with dbt, and loading to BigQuery - all characteristics of batch processing. No streaming components like Kafka, Flink, or real-time message brokers are present.",GCP
https://github.com/AdaProjectNov/DEProject_TFLData,London Bicycle Traffic Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled flows (e.g., `tfl_github_to_gcp_to_bq_schedule`) that run periodically to extract quarterly data from GitHub, load it to GCS, and transform it in BigQuery. It also uses dbt for batch transformations and modeling. No streaming components (Kafka, Flink, etc.) are present.",GCP
https://github.com/victoriaamarah/Payment-Reconciliation-Pipeline,Payment Reconciliation Analytics Pipeline,Batch,"The repository contains a single Python script (run_pipeline.py) that generates synthetic data and uploads it to Backblaze B2, followed by transformation in Databricks. This is a batch-oriented workflow with no streaming components like Kafka, Kinesis, or Flink mentioned in the code.",Other
https://github.com/nikoprabowo/crydlinetest,Cryptocurrency Market Data Pipeline,Batch,"The repository contains a complete batch-oriented data pipeline with workflow orchestration via Apache Airflow, scheduled ETL jobs, and dbt transformations. Key indicators include: 1) Airflow DAGs for orchestrating data ingestion and processing workflows, 2) dbt for batch transformations with commands like dbt run/test, 3) Terraform infrastructure for batch processing components (Dataproc for Spark), 4) Python scripts for ingesting historical data from Alpha Vantage API, and 5) Makefile commands for running batch operations. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/shravank89/divvy-bikeshare-analytics-project-for-DTC.git,Divvy Bikeshare Analytics Pipeline,Batch,"The project uses Airflow DAGs to orchestrate scheduled ETL jobs that run monthly. The pipeline includes: 1) Data extraction DAG that downloads monthly Divvy data from S3, 2) Spark transformation DAG that processes data in batches using Azure Synapse, 3) BigQuery loading DAG that loads transformed data, and 4) dbt transformation DAG that applies business logic. All components are workflow-based with scheduled execution, not continuous streaming.",Azure
https://github.com/Ojekky/optimizing_urban_mobility/tree/main,Bike Fleet Performance ETL,Batch,"The project uses Kestra workflow orchestrator with scheduled tasks (cron-based) to download, process, and load bike trip data in batch mode. The workflows show scheduled monthly data ingestion from static CSV files, not continuous real-time processing.",GCP
https://github.com/yatesmac/sales_data,Sales Data Analytics Pipeline,Batch,"The repository contains an Apache Airflow DAG (pipeline.py) that orchestrates scheduled ETL jobs. The pipeline runs daily, extracting data from CSV, transforming it to Parquet, loading to PostgreSQL, and then running dbt transformations. This is a classic batch processing pattern with scheduled workflow execution.",Other
https://github.com/hammadbinsajjad/Ecommerce-Inventory-Pipeline,Ecommerce Inventory Analytics Pipeline,Batch,"The project uses Kestra as a workflow orchestrator with a Dockerfile for running scheduled ETL jobs. The pipeline follows a batch pattern: data extraction with dlt, transformation with dbt, and loading to BigQuery. The architecture diagram and README describe scheduled data processing rather than continuous streaming.",GCP
https://github.com/sam-haff/air-quality-analysis,Hungary Air Pollution Data Warehouse,Batch,"The repository uses Kestra as a workflow orchestrator with pipeline scripts designed for scheduled ETL jobs. The README explicitly states these scripts are ""part of the Kestra flows"" for batch processing, and there are no streaming components like Kafka, Flink, or real-time processing elements present.",GCP
https://github.com/Jaykold/StockPipeline,Yahoo Finance Data Pipeline,Batch,"The project uses Apache Airflow with DAGs for workflow orchestration, scheduled ETL jobs that extract stock data periodically, transform it with PySpark, and load it to Azure SQL Server. The Docker Compose setup includes Airflow components (webserver, scheduler, worker) and the Makefile contains commands for Airflow initialization, indicating a batch-oriented ETL pipeline rather than continuous streaming.",Azure
https://github.com/gioele-perri/Asset_analysis_dtc_de,Financial Asset Metrics Warehouse,Batch,The project uses Kestra orchestrator to run scheduled ETL jobs that retrieve data every evening from Yahoo Finance API and load it into BigQuery. This is a classic batch processing pattern with periodic data pulls and transformations.,GCP
https://github.com/jimmyAUT/DE_amazon_product_reviews,Amazon Product Review Analytics Pipeline,Batch,"The repository contains a batch-oriented data pipeline using Terraform to provision GCS, BigQuery, and Dataproc for scheduled ETL jobs. The architecture diagram and README describe a workflow where data is extracted, stored in GCS, processed by Spark on Dataproc, and loaded into BigQuery for analysis. There are no streaming components like Kafka, Kinesis, or Flink present.",GCP
https://github.com/krishnavamshithumma/DE-project-Cycling-data-analysis-UK-2023-2024-,UK Cycling Traffic Analytics Pipeline,Batch,"The project uses Airflow DAGs (cycling_data_download_to_gcs) for scheduled ETL jobs, Terraform for infrastructure provisioning, and Spark jobs submitted via Dataproc for batch data transformation. The architecture shows data being pulled periodically, transformed, and loaded to BigQuery rather than continuous streaming.",GCP
https://github.com/ryoko0550/de-zoomcamp-proj-sf-business/tree/main,City Business Registration Data Warehouse,Batch,"The project uses Kestra workflow orchestrator with a defined flow (kestra_data_ingestion_flow.yml) that runs scheduled ETL jobs to extract data from CSV, upload to GCS, and load to BigQuery. The dbt project further confirms batch processing with scheduled transformations. No streaming components (Kafka, Flink, etc.) are present.",GCP
https://github.com/rrust2018/de-zoomcamp-final-project,Book Recommendation Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML-defined tasks (setup.yaml, pipeline.yaml) that run scheduled ETL jobs. The pipeline downloads data from GitHub, uploads to GCS, loads into BigQuery, and runs dbt transformations - all batch-oriented operations with discrete data pulls and scheduled execution.",GCP
https://github.com/sanchis135/DEZoomcamp2025/tree/main/Project,Enterprise Data ETL Pipeline,Batch,"The project uses Kestra workflow orchestrator with defined workflows (gcp_kv.yml, gcp_setup.yml, gcp_stock_pipeline.yml) that run scheduled ETL jobs to process data periodically. The architecture shows a traditional batch pipeline with data ingestion, transformation, and loading to BigQuery warehouse.",GCP
https://github.com/vccat/dezoomcamp_pisa,International Student Assessment ETL Pipeline,Batch,"The project uses Kestra workflow orchestrator to run scheduled ETL jobs for data ingestion, with Terraform for infrastructure provisioning and dbt for data transformation. The code shows workflow definitions (kestra_data_ingestion.yaml) that download, transform, and load data periodically, which is characteristic of batch processing.",GCP
https://github.com/JudsonMorgan/Bike_trip_pipeline,NYC Bike Trip Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs for scheduled ETL jobs, including data extraction, transformation, and loading to BigQuery. The Makefile shows commands to trigger DAGs and run dbt transformations, indicating a batch processing pipeline.",GCP
https://github.com/Dkaattae/pelosi_tracker,Insider Trading Detection Data Platform,Batch,"The repository uses Kestra as a workflow orchestrator with YAML flow definitions (download_index_flow.yml, iterate_docpdf_batch.yml) and scheduled ETL jobs. The architecture follows a batch pattern: downloading index files, scanning PDFs, loading data to Postgres, transforming with dbt, and visualizing in Metabase. No streaming components like Kafka, Flink, or real-time processing are present.",Other
https://github.com/xinchengppy/paris-wifi-analysis-pipeline,Paris Wi-Fi Insights Pipeline,Batch,"The repository uses Kestra workflow orchestrator with YAML flow definitions (kestra/flows/data-ingestion.yml) that define scheduled ETL tasks including data download, splitting, and upload operations. The pipeline follows a traditional batch processing pattern with discrete jobs that run periodically to transform and load data.",GCP
https://github.com/SaschaKay/jobs-research,Data Job ETL Platform,Batch,"The project uses Airflow DAGs with scheduled ETL jobs (cron schedule ""0 6 * * *"") to pull data periodically from an API, transform it using DLT and Pandas, and load it to BigQuery. The code shows workflow orchestrators (Airflow) running scheduled tasks, which is characteristic of batch processing.",GCP
https://github.com/zotyap/Data-Engineer/tree/main/data-engineering-zoomcamp/final_project,Google Cloud Fashion Data Platform,Batch,"The project uses Kestra workflow orchestrator with YAML flow files (gcp_fashion_data_scheduled.yaml, gcp_fashion_filter.yaml) that define scheduled ETL tasks. The flows include backfill operations and manual execution steps, indicating batch processing rather than continuous streaming. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/RxAbdoo/International-Football-Matches-Data-Engineering-Project,International Football Analytics Pipeline,Batch,"The repository contains an Airflow DAG (final_project_dag.py) that orchestrates scheduled ETL jobs using SparkSubmitOperator for data extraction, transformation, and loading to HDFS. The workflow is defined with schedule_interval=None, indicating batch processing triggered manually or on a schedule, not continuous streaming.",GCP
https://github.com/a920604a/data-engineering-zoomcamp-2025,NYC Taxi Data Pipeline,Batch,"The repository contains Docker and Terraform configuration files for setting up batch data processing infrastructure (PostgreSQL, pgAdmin) and Python scripts for ingesting CSV data into a database. The homework shows SQL queries for analyzing historical taxi trip data, indicating batch ETL workflows rather than real-time streaming.",Other
https://github.com/Yaxin12/Real-Time-YouTube-Analytics-Pipeline-Data-Engineering-Zoomcamp-2025-Project-,Real-Time YouTube Analytics Pipeline,Streaming,"The project implements a real-time data pipeline using Apache Kafka for streaming YouTube video analytics. The docker-compose.yml shows a full Kafka ecosystem (Zookeeper, broker, schema registry, ksqlDB), and the README explicitly states ""streaming using Apache Kafka"" and ""real-time processing and analytics"". The code shows Kafka producers, ksqlDB stream processing, and continuous data ingestion.",Other
https://github.com/chelsemet/Mental_Health_in_Tech,Workplace Mental Health Data Warehouse,Batch,"The repository uses Apache Airflow with DAGs for workflow orchestration, indicating a batch processing approach where data is pulled periodically, transformed, and loaded to BigQuery. The docker-compose.yaml shows Airflow webserver and scheduler services, and the terraform configuration creates BigQuery tables for batch data loading.",GCP
https://github.com/sardorboboshov/zoomcamp-project,London Bike Rental Analytics Pipeline,Batch,"The project uses Airflow DAGs with scheduled tasks (@monthly) for ETL workflows, including PythonOperators for data processing and BigQueryInsertJobOperator for loading. This is classic batch processing architecture with periodic data pulls and transformations.",GCP
https://github.com/satiyam/data_engineering_zoomcamp_2025_vksatiyam/tree/main/Project,Cryptocurrency Market Sentiment Pipeline,Batch,"The repository contains a data engineering pipeline orchestrated by Kestra (kestra_flow.yaml) that runs scheduled ETL jobs. The pipeline includes data ingestion via web scrapers, processing on Dataproc clusters, and loading to BigQuery, all managed through workflow orchestrators. While the project mentions ""real-time sentiment integration,"" the actual implementation uses batch processing with scheduled jobs rather than continuous streaming components like Kafka or Flink.",GCP
https://github.com/SebastianSydlik/weather_predictor,Historical Weather Data ETL,Batch,"The repository uses Prefect as a workflow orchestrator to run scheduled ETL jobs. The pipeline involves batch processing steps: API data extraction, local storage, loading to GCS, then BigQuery, followed by dbt transformations. The docker-compose.yaml shows Prefect server and worker services, and the README explicitly states ""Prefect is used to orchestrate batch processing workflows.""",GCP
https://github.com/Sharonsyra/grocery-sales-pipeline,Grocery Sales Analytics Pipeline,Batch,"The repository contains Apache Airflow DAGs (data_ingestion_gcs_dag.py, spark_grocery_transformations_dag.py) that orchestrate scheduled ETL jobs. The pipeline ingests data periodically, transforms it with Spark, and loads it to BigQuery - a classic batch processing pattern. No streaming components (Kafka, Flink, etc.) are present.",GCP
https://github.com/nord94/Human-Energy-Savings-in-Europe,WHO Exercise Energy Metrics Warehouse,Batch,"The repository contains Apache Airflow DAGs (dags/extract_eurostat_data.py, dags/extract_wri_data.py, dags/example_dag.py, dags/dwh_connection_check.py) that orchestrate scheduled ETL jobs. The docker-compose.yml shows Airflow webserver and scheduler services, confirming a batch-oriented workflow system. The tech stack includes dbt for SQL-based transformations, which is typical of batch processing pipelines.",Other
https://github.com/elgrassa/Data-engineering-professional-certificate,Poland Real Estate Analytics Pipeline,Batch,"The repository uses Kestra, a workflow orchestrator, to run scheduled ETL jobs. The flows (build-dbt.yaml and polish_flats_unified_loader.yaml) are parameterized batch jobs that process monthly CSV files, load data to PostgreSQL/BigQuery, and trigger dbt transformations. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/amralaaeldien/final-de-project/tree/main/filesystem_gcs_pipeline,Saudi Coffee Sales Analytics Pipeline,Batch,"The repository contains a dlt (data load tool) pipeline that downloads files, loads them to GCS, and then loads to BigQuery. This is a batch-oriented ETL process orchestrated through Python scripts, not a continuous streaming architecture.",GCP
https://github.com/jennyiskezhen/Data-Engineering/tree/main/project,Supplier Quality Analysis Platform,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (gcp_manufacture.yaml) that define sequential tasks: download dataset, upload to GCS, then query BigQuery. This is classic batch ETL pattern with scheduled/periodic data processing rather than continuous streaming.",GCP
https://github.com/gishusam/nba_pipeline,NBA Performance Analytics Pipeline,Batch,"The repository contains a batch processing data pipeline using dbt for transformations and scheduled ETL jobs. The README explicitly states ""builds a batch processing data pipeline"" and shows tools like dbt run, BigQuery tables, and Python scripts for data ingestion. There are no streaming components like Kafka, Kinesis, or Flink mentioned or present in the code.",GCP
https://github.com/ruben1/ireland-rentals,Unknown,Unknown,No files fetched,Unknown
https://github.com/ArunDharavath/Freight-Data-Analysis-US-2024,US Canada Mexico Freight Dashboard,Batch,"The project uses Kestra as a workflow orchestrator to run scheduled ETL jobs, which is characteristic of batch processing. The README describes building an ETL pipeline with Kestra to automate uploading data to GCS and loading it into BigQuery, followed by SQL transformations - all batch-oriented operations.",GCP
https://github.com/nategetu/us-border-crossing-insights/,US Border Traffic Analytics Pipeline,Batch,"The project uses Apache Airflow with DAGs (DataPipeline DAG) to orchestrate ETL pipelines that run periodically. The architecture shows scheduled data ingestion from CSV files into S3, transformation via SQL scripts in Redshift, and loading to a data warehouse - all characteristic of batch processing rather than continuous streaming.",AWS
https://github.com/AbhishekM2001/UPI-Transactions-Analytics-Platform,RBI Reports ETL Platform,Batch,"The project uses Apache Airflow with a monthly scheduled DAG (schedule_interval=""@monthly"") that orchestrates ETL tasks: scraping RBI data, cleaning it, uploading to GCS, and refreshing BigQuery views. This is a classic batch processing pattern with periodic data pulls and transformations.",GCP
https://github.com/Jzhao1990/zoom_camp_project,COVID-19 Daily Trends ETL Pipeline,Batch,"The repository contains an Airflow DAG (`dags/covid_data_dag.py`) that orchestrates a daily batch ETL pipeline. The pipeline pulls data from DATA.GOV, processes it in batches, and loads it to GCS and BigQuery. The README explicitly states ""An orchestrated data pipeline is built in Airflow with a daily data refresh at 20:00:00 UTC"" and describes batch processing. No streaming components (Kafka, Flink, Kinesis) are present.",GCP
https://github.com/Ojekky/optimizing-divvy-bike-share-system,Chicago Bike Share Historical Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs (YAML workflow files) to process Divvy bike share data. The workflows download zip files, decompress them, convert to CSV, and upload to GCS storage. The trigger is scheduled (cron-based) rather than event-driven streaming.",GCP
https://github.com/NasrinMokhtarian/Flight-Price-Prediction.git,Travel Price Prediction Analytics,Batch,"The repository contains a batch processing pipeline using dlt (data ingestion tool) to extract, transform, and load flight booking data. The README explicitly states this is a ""Batch Processing Project"" with a static dataset that is loaded once, processed using pandas, and stored in DuckDB and BigQuery. There are no streaming components like Kafka, Kinesis, or Flink mentioned or present in the code.",GCP
https://github.com/wiliamquispe/power_comsumption_india.git,India Power Consumption Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML flow definitions (gcp_power.yaml) that define scheduled ETL tasks including data extraction, upload to GCS, BigQuery queries, and dbt transformations. The architecture shows batch processing patterns with periodic data pulls and transformations rather than continuous streaming.",GCP
https://github.com/bisera1/data-engineering-zoomcamp-project,Remote Job Data Pipeline,Batch,"The project uses Apache Airflow with DAGs (my_dag.py, jobicy_dag.py) that run scheduled ETL jobs to fetch data from APIs, transform it using PySpark, and load it to a database. This is a classic batch processing pattern with periodic data pulls and transformations orchestrated by Airflow.",Other
https://github.com/emancsanchez/CMS_Payment_DEZoomcampProject,Healthcare Spending Pattern Analytics,Batch,"The repository contains Airflow DAGs (cms_dag.py, weather_dag.py) that orchestrate scheduled ETL jobs. The workflow downloads CSV files, transforms them with Pandas, and loads to Google Cloud Storage/BigQuery on a schedule (e.g., @once, @hourly). No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/tgy23/Student-Habits-vs-Academic-Performance,Academic Performance Data Warehouse,Batch,"The project uses Azure Data Factory for batch scheduling and orchestration, with data flowing from Azure Data Lake to Databricks SQL Data Warehouse in scheduled batches. The README explicitly states ""Azure Data Factory for every day batch scheduling"" and the IaC configuration provisions Azure Data Factory resources.",Azure
https://github.com/urbanclimatefr/de-zoomcamp-2025-project/tree/main,Hong Kong Weather Data Pipeline,Batch,"The project uses Kestra workflow orchestrator with scheduled ETL jobs (hourly data collection, daily batch processing). The flowchart shows batch-oriented steps: API ingestion, data processing, dbt transformations, and Looker Studio visualization. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/Victory0007/DE_Zoomcamp_Capstone_Project_/tree/master,Weather Data Processing Platform,Batch,"The repository contains a Prefect flow (`flow.py`) that orchestrates ETL jobs with scheduled execution, uses dbt for transformations, and includes a DAG-like structure for data processing. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/frog-with-shoes/zoomcamp-final-project,E-commerce Sales Analytics Pipeline,Batch,"The project uses Airflow with DAGs (upload_dag.py) to orchestrate scheduled ETL jobs. The pipeline includes: 1) uploading ingestion script to GCS, 2) running Python script to extract data, 3) loading parquet files to BigQuery with partitioning/clustering, 4) running dbt transformations. This is a classic batch processing workflow with scheduled tasks, not continuous streaming.",GCP
https://github.com/Kobbyeduah/Binance-Pipeline-.git,Binance Crypto Data Pipeline,Batch,"The repository contains Apache Airflow (version 2.9.0) with scheduled tasks and a schedule.py file, indicating a batch-oriented workflow orchestrator. The README describes a sequential ETL pipeline (scrape → clean → upload to S3 → load to warehouse) rather than continuous streaming. While FastAPI is present, it appears to be for API endpoints rather than real-time data streaming.",AWS
https://github.com/ABDULLAH-ibrahimm/dtc_graduation_project.git,E-commerce Analytics Data Warehouse,Unknown,"The repository contains only documentation and a README file. No actual code, configuration files, or infrastructure definitions are present that would indicate whether the project uses batch processing (e.g., Airflow DAGs, scheduled jobs) or streaming (e.g., Kafka consumers, Flink jobs). The README mentions both batch and streaming concepts but provides no implementation details.",Unknown
https://github.com/minhthanh120/data-zoomcamp2025/tree/main/my_project,Power BI Financial Dashboard,Batch,"The repository uses Kestra workflow orchestrator to run scheduled ETL jobs (kaggle.fiance-data.yml), with dbt for transformation and loading to PostgreSQL warehouse. This is a batch processing pattern with periodic data pulls and transformations.",Other
https://github.com/MAHMOUDMAMDOH8/E2E-ELT-pipeline,NYC Taxi Data Warehouse,Batch,"The repository contains an Airflow DAG (taxi_trip.py) that orchestrates ETL jobs on a schedule (*/15 * * * *). The pipeline processes Parquet files in batches, loads them to PostgreSQL, and uses dbt for transformation. No streaming components like Kafka, Flink, or Kinesis are present.",Other
https://github.com/BinksWANG/ESG_Risk_Project,ESG Company Analysis Pipeline,Batch,"The repository contains Kestra workflow configurations (application.yml, docker-compose.yml) for batch data loading to cloud storage, Docker-based batch ingestion scripts (ingest_data.py), and dbt transformation files for batch warehouse processing. No streaming components like Kafka, Flink, or real-time processing are present.",GCP
https://github.com/Amir380-A/DE_Project,AWS Data Quality Pipeline,Batch,"The project uses Apache Airflow with DAGs (production.py) to orchestrate scheduled ETL jobs, Great Expectations for batch validation, and Spark/PySpark for batch processing. Data is pulled periodically from S3, transformed, and loaded to Redshift - all characteristics of batch processing.",AWS
https://github.com/MAHMOUDMAMDOH8/ELT-Engine,CRM ERP Data Integration,Batch,"The project uses Airflow with a DAG (pipeline.py) that runs scheduled ETL jobs every 15 minutes. The pipeline includes extract, load, and transform tasks orchestrated by Airflow operators. The architecture follows a batch-oriented Medallion Architecture with dbt transformations. Streamlit is used for visualization but is not streaming data processing.",Other
https://github.com/LolloPero/Fraudulent-Financial-Transaction-Prediction,GitHub Activity Analytics Pipeline,Batch,"The repository uses Kestra workflow orchestrator with YAML flow definitions (github_activities_ingestion.yaml) that define scheduled ETL tasks including data extraction, GCS upload, and BigQuery operations. The pipeline processes GitHub archive files in batch mode with scheduled triggers.",GCP
https://github.com/BlvckOgre/football-stadiums-of-world,Football Stadiums Data Pipeline,Batch,"The project uses Apache Airflow with DAGs (wikipedia_flow.py) to orchestrate scheduled ETL jobs that pull data periodically from Wikipedia, transform it, and load it to storage. The docker-compose.yml shows Airflow webserver and scheduler services running batch workflows.",Azure
https://github.com/Rahee93/data-engineering-zoomcamp-2025-project,Accident Data ETL and Visualization,Batch,"The repository contains a batch data pipeline with discrete steps: data ingestion to GCS, cleaning with Python/Pandas, loading to BigQuery, and dbt transformations. No streaming components (Kafka, Flink, Kinesis) are present.",GCP
https://github.com/YukiQiu/DE_Zoomcamp2025/tree/11f2ca2126b0a4bcf180e3612dc3045abe4270d6/MillionSongs_Project,Million Songs Data Pipeline,Batch,"The repository contains a dbt project with models, staging, and marts directories, along with a dbt_project.yml file. This indicates a batch-oriented ELT workflow where dbt transforms data in the warehouse on a scheduled basis.",Unknown
https://github.com/Tomilolaadeyinka/retail-data-pipeline/blob/main/README.md,Online Retail Data Warehouse,Batch,"The repository contains a Makefile with a deploy target that runs Python deploy.py, and the README mentions the pipeline is ""automated using Airflow"" and deployed in GCP. The data transformation uses Python/Pandas and dbt, which are typical batch processing tools. No streaming components like Kafka, Kinesis, or Flink are present.",GCP
https://github.com/Abdou240/Retail-Sales-Analysis,Retail Data Transformation Pipeline,Batch,"The repository contains Terraform infrastructure for provisioning Google Cloud resources including Cloud Storage, BigQuery datasets, and a Dataproc cluster for data cleaning. The infrastructure is designed for batch processing workflows (ETL jobs on the Dataproc cluster), with no streaming components like Kafka, Kinesis, or Flink present in the code.",GCP
https://github.com/Ainapalma/de-zoomcamp-course-project,Netflix Content Analytics Pipeline,Batch,"The project uses Kestra workflow orchestrator with YAML pipeline definitions (gcp_movies_and_shows.yaml, gcp_movies_and_shows_by_year.yaml) that define scheduled ETL tasks including data extraction, GCS upload, BigQuery queries, and dbt transformations. These are batch-oriented workflows that run periodically rather than streaming data processing.",GCP
https://github.com/brem-21/Ecommerce-DataLakehouse,Ecommerce Analytics Data Lakehouse,Batch,"The repository implements a batch ETL pipeline using PySpark for data transformation, with scheduled workflow orchestration via GitHub Actions. The code shows data ingestion from S3, transformation using Spark, and loading to processed tables - all characteristic of batch processing rather than continuous streaming.",AWS
https://github.com/JOHNFFFEE/datacamp_dataEngineering,E-commerce Data Analytics Platform,Batch,"The repository contains workflow orchestrator configurations (Kestra flows, dbt models) and ETL scripts for scheduled data processing. The code shows Kestra flow definitions, dbt transformations, and Docker-based ETL jobs that run periodically rather than streaming data pipelines.",GCP
https://github.com/Yusufislam-id/Toronto-Bikeshare-Pipeline,Urban Mobility Data Processing Pipeline,Batch,"The repository uses Kestra workflow orchestrator with YAML flow definitions (02_gcp_bikeshare.yaml, 01_gcp_setup.yaml, 00_gcp_kv.yaml) that define scheduled ETL jobs. The pipeline downloads data from Toronto Open Data, extracts CSV files, uploads to cloud storage, and loads to BigQuery - all batch operations triggered by workflow runs rather than continuous streaming.",GCP
https://github.com/dhamsrishti/airline-dashboard-project/,Airline Delay Analytics Pipeline,Batch,"The project uses a Python script (main.py) that runs scheduled ETL jobs to process data from GCS to BigQuery, with no streaming components like Kafka or Flink present.",GCP
https://github.com/Jujubalandia/DataGamePipe,Video Game Data Warehouse Platform,Batch,"The repository contains Terraform files for provisioning Google Cloud Composer (Airflow), which is a workflow orchestrator used for batch processing. The infrastructure setup includes BigQuery datasets and GCS buckets, indicating a traditional ETL pipeline with scheduled jobs rather than continuous streaming.",GCP
https://github.com/hnkovr/my-dez-2025,Unknown,Unknown,No files fetched,Unknown
https://github.com/oyetadesegun/DEZoomCamp.git,Learning Outcomes Data Pipeline,Batch,"The repository contains a traditional ETL pipeline with separate extract, transform, and load stages executed via Python scripts. The code shows batch processing patterns: reading CSV files, transforming data, and loading to SQLite database. There are no streaming components like Kafka, Flink, or real-time processing elements.",Other
https://github.com/cssaritama/nyc-analytics,NYC Taxi Analytics Pipeline,Batch,"The repository contains an Airflow DAG (`nyc_taxi_pipeline.py`) that orchestrates scheduled ETL jobs to load and transform data periodically, which is characteristic of batch processing. There are no streaming components like Kafka, Flink, or real-time message brokers present.",GCP
https://github.com/H-Sorkatti/de-bitcoin-dashboard,Kafka Crypto Data Pipeline,"Batch, Streaming",The project has both batch and streaming components. The streaming component uses Kafka producers/consumers for real-time data flow from CoinAPI to PostgreSQL. The batch component uses Kestra workflow orchestrator to run scheduled ETL jobs including DBT transformations.,Other
https://github.com/abrhamadddis/london-cycling-active-travel-insights-data-pipeline,Urban Cycling Insights Dashboard,Batch,"The repository contains a data pipeline orchestrated by Kestra with scheduled ETL jobs (scrape → GCS → BigQuery via DLT → dbt transformations → Looker Studio dashboard). This is a classic batch architecture with periodic data pulls and transformations, not continuous streaming.",GCP
https://github.com/viviayi/data_engineering/tree/main/homework/project,Behavior Order Data Processing,Batch,"The repository contains a batch data processing workflow using PySpark to transform data and load it into BigQuery via Dataproc jobs. The README shows scheduled job submission with gcloud dataproc jobs submit pyspark, which is a batch operation. There are no streaming components like Kafka, Flink, or real-time processing elements present.",GCP
https://github.com/FeloXbit/Electric-School-Bus-Adoption-Analytics,Electric School Bus Analytics Pipeline,Batch,"The repository contains a Kestra workflow file (kestra_workflow.yaml) that defines a batch ETL pipeline with scheduled tasks for fetching data from an API, transforming it via Python scripts, and loading it into PostgreSQL. The Docker Compose setup and supporting scripts are designed for batch processing workflows, not continuous streaming.",Other
https://github.com/ayoquiroga/zoomcamp,Unknown,Unknown,No files fetched,Unknown
https://github.com/SitaraJin/data-engineering-zoomcamp,NYC Taxi Analytics Pipeline,Batch,"The repository contains workflow orchestration projects using Kestra (homework2) and dbt (homework4), both of which are batch-oriented tools. Kestra workflows are configured with scheduled tasks and dbt runs are explicitly mentioned as batch operations. No streaming components like Kafka, Flink, or real-time processing are present.",Unknown
https://github.com/elenset/data-engineering-zoomcamp/tree/main/project,Stock Market ETL Pipeline,Batch,"The repository contains a README.md that explicitly states the pipeline can be ""Stream or Batch"" and instructs users to decide, but the actual project files show only batch-oriented components: Airflow DAGs for workflow orchestration, dbt for data transformation, and scheduled ETL jobs. No streaming components like Kafka, Kinesis, or Flink are present in the files.",Unknown
https://github.com/yemnaing/data-engineering-zoomcamp/tree/main/projects,Healthcare Claims Data Warehouse,Unknown,"The repository contains only documentation and project guidelines. No actual code, configuration files, or infrastructure definitions are present to determine the deployment type.",Unknown
https://github.com/dewaleofficial/FM-Transfers,Unknown,Unknown,No files fetched,Unknown
https://github.com/Aditya-x/Learning-DE/tree/main/dbt-tut/dbt_pg,NYC Taxi Data Warehouse,Batch,"The repository contains a dbt project with dbt configuration files (dbt_project.yml, profiles.yml) and dbt commands (dbt run, dbt test) in the README. dbt is a batch-oriented transformation tool that runs scheduled ETL jobs, not a streaming system. No streaming components like Kafka, Flink, or real-time processing are present.",Other
